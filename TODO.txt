1. knee 설정도 건드려봐야함.
2. 민준 청크 사이즈 변경, 모델 변경 없이도 단순히 파일 하나를 올리는 경우에는 해결이 됐지만
   모든 매뉴얼을 올리니 안 돌아감.


민준 선배는 청크 사이즈를 늘리니까 해결되었다고 하는데
휴리스틱한 방법말고 knee retrieval 방법을 사용하여 동적으로 가져오게 하면 어떨까?

임베딩 모델 교체 없이 갈 수도 있을까?
또한 많은 문서를 임베딩해도 잘 돌아갈까? 




embedding models:
> MTEB 벤치마크에서 경량 임베딩 모델들만 추려서 테스트에 사용
> 모두 ~0.5B 크기, 2025년 8월 30일 기준 순위를 함께 명시함
> 참조: https://huggingface.co/spaces/mteb/leaderboard
- 2위: Qwen/Qwen3-Embedding-8B
- 4위: Qwen/Qwen3-Embedding-0.6B
- 22위: jinaai/jina-embeddings-v3
- 23위: BAAI/bge-m3
- 117위: sentence-transformers/all-MiniLM-L6-v2

reranking models:
- no reranking
-

> reranking 전략은 초기에 임베딩에서 가져온 문서들에서 한 번 재정렬하는거임.
> 예를 들어 100,000개의 문서 중 1,000개의 문서를 검색으로 가져왔다면, 1,000개 사이에서 연관성을 재정렬하는 것을 말함.
> 초기에 가져오는 문서의 수가 중요할 것 같은데, 여기에 kneedle을 적용해보자

knee retrieval vs. top-k retrieval
- top-k retrieval (baseline)
- knee retrieval

=== 연구적 관점에서의 시도 ===

MAS
- DeepResearch를 만들까? 에이전트가 프롬프트를 작성하여 쿼리할 수도 있잖아.

System Prompt Learning
- 별도의 시스템 프롬프트 개선용 멀티-에이전트 시스템은 언젠가 제안되어야하며, 제안될 것이라고 봄.

RAG
- https://g.co/gemini/share/a20f9645bbc7 기반으로 Agentic RAG를 도입해보자.
  https://newsletter.armand.so/p/comprehensive-guide-rag-implementations 를 기반으로
  Naive RAG, Adaptive RAG, Corrective RAG, Self-RAG, Agentic RAG 등을 직접 구현해보고 평가해보자.
- HyDE, Branched RAG, Simple RAG with Memory 등도 하나씩 구현해보자.
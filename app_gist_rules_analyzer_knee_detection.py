"""
GIST Rules Analyzer - Prebuilt Database Version
===============================================

ì‚¬ì „ êµ¬ì¶•ëœ FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•˜ëŠ” ê³ ì† ì‹œì‘ ë²„ì „ì…ë‹ˆë‹¤.

ì‚¬ìš© ì „ ìš”êµ¬ì‚¬í•­:
    python build_rule_database.py  # ë¨¼ì € ì‹¤í–‰

íŠ¹ì§•:
- 3ì´ˆ ë‚´ ì•± ì‹œì‘ ì™„ë£Œ
- ë§¤ ì¿¼ë¦¬ë§ˆë‹¤ FAISS ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¹„êµ
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- ì¶”ê°€ PDF ì—…ë¡œë“œ ì§€ì› (ì„ íƒì )
"""

import gradio as gr
import os
import json
import time
import hashlib
import html
import threading
import fitz
import litellm
import faiss
import numpy as np
from pathlib import Path
from collections import OrderedDict
from datetime import datetime
from dotenv import load_dotenv
from litellm import get_valid_models
from huggingface_hub import InferenceClient
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_core.retrievers import BaseRetriever
from kneed import KneeLocator
from typing import Dict, Generator, List, Optional, Callable, Any

# Environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HF_API_KEY = os.getenv("HUGGING_FACE_API_KEY") or os.getenv("HF_TOKEN")
NOVITA_API_KEY = os.getenv("NOVITA_API_KEY")
HF_ENTERPRISE = os.getenv("HUGGING_FACE_ENTERPRISE")

# Configuration
DB_PATH = Path("faiss_db")
DIMENSION = 384
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)


def _detect_provider(model_id: str) -> str:
    if model_id.startswith("novita/"):
        return "novita"
    if model_id.startswith("fireworks_ai/"):
        return "fireworks"
    return "openai"


def _load_dynamic_models() -> Dict[str, Dict[str, str]]:
    if os.getenv("FIREWORKS_API_KEY") and not os.getenv("FIREWORKS_AI_API_KEY"):
        os.environ["FIREWORKS_AI_API_KEY"] = os.getenv("FIREWORKS_API_KEY") or ""

    try:
        model_ids = get_valid_models(check_provider_endpoint=True)
    except Exception:
        model_ids = []

    dynamic: Dict[str, Dict[str, str]] = {}
    for mid in model_ids:
        dynamic[mid] = {"model_id": mid, "provider": _detect_provider(mid)}

    default_fw = "fireworks_ai/accounts/fireworks/models/gpt-oss-20b"
    if (
        os.getenv("FIREWORKS_AI_API_KEY") or os.getenv("FIREWORKS_API_KEY")
    ) and default_fw not in dynamic:
        dynamic[default_fw] = {"model_id": default_fw, "provider": "fireworks"}

    if OPENAI_API_KEY and not any(
        p.get("provider") == "openai" for p in dynamic.values()
    ):
        dynamic.setdefault(
            "gpt-4o-mini", {"model_id": "gpt-4o-mini", "provider": "openai"}
        )

    return dynamic


MODELS = _load_dynamic_models()

# Rerank options
RERANK_OPTIONS = {
    "ì—†ìŒ": None,
    "ms-marco-MiniLM-L-6-v2": "cross-encoder/ms-marco-MiniLM-L-6-v2",
    "ms-marco-MiniLM-L-12-v2": "cross-encoder/ms-marco-MiniLM-L-12-v2",
    "mmarco-mMiniLMv2-L12-H384-v1": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
}

# Initialize embeddings
EMBED_MODEL = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

# System prompt
system_prompt = """You are a GIST Rules and Regulations Expert Assistant. 
You have comprehensive knowledge of all GIST academic rules, regulations, guidelines, and policies.
Always provide accurate, detailed answers based on the provided context.
When answering questions about GIST rules, cite specific regulation numbers and titles when available."""


# --------- (A) PERFORMANCE LOGGING & FAISS INDEX COMPARISON ---------
class PerformanceLogger:
    """ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ê¸°ë¡í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, log_dir: str = "performance_logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.log_file = (
            self.log_dir
            / f"faiss_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        )

    def log_query_performance(self, query: str, results: Dict):
        """ë‹¨ì¼ ì¿¼ë¦¬ì˜ ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë¡œê¹…"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "query_hash": hashlib.md5(query.encode()).hexdigest()[:8],
            "results": results,
        }

        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        print(f"ğŸ“Š ì„±ëŠ¥ ê²°ê³¼ ê¸°ë¡: {self.log_file}")


class ChatLogger:
    """ì±„íŒ… ì„¸ì…˜ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ê¸°ë¡í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, log_dir: str = "chat_logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.log_file = (
            self.log_dir
            / f"chat_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        )

    def log_chat_interaction(
        self,
        user_query: str,
        bot_response: str,
        rerank_method: str,
        model_info: Dict,
        performance_metrics: Dict,
        retrieved_docs: Optional[List[str]] = None,
        faiss_performance: Optional[Dict] = None,
        knee_detection_info: Optional[Dict] = None,
        session_id: Optional[str] = None,
    ):
        """ì±„íŒ… ìƒí˜¸ì‘ìš©ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ë¡œê·¸ì— ê¸°ë¡"""

        # ë¦¬ë­í‚¹ ìƒì„¸ ì •ë³´
        rerank_config = RERANK_OPTIONS.get(rerank_method)
        if isinstance(rerank_config, dict):
            rerank_info = {
                "method_name": rerank_method,
                "enabled": rerank_config.get("enabled", False),
                "model": rerank_config.get("model"),
                "top_k": rerank_config.get("top_k", 3),
            }
        else:
            # rerank_configê°€ ë¬¸ìì—´ì´ê±°ë‚˜ Noneì¸ ê²½ìš°
            rerank_info = {
                "method_name": rerank_method,
                "enabled": rerank_method != "ì—†ìŒ",
                "model": rerank_config if isinstance(rerank_config, str) else None,
                "top_k": 3,
            }

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "session_id": session_id,
            "query_hash": hashlib.md5(user_query.encode()).hexdigest()[:8],
            "interaction": {
                "user_query": user_query,
                "bot_response": bot_response,
                "query_length": len(user_query),
                "response_length": len(bot_response),
            },
            "model_info": model_info,
            "rerank_info": rerank_info,
            "performance_metrics": performance_metrics,
            "faiss_performance": faiss_performance,
            "knee_detection": knee_detection_info,
            "retrieved_documents": retrieved_docs[:3]
            if retrieved_docs
            else None,  # ì²˜ìŒ 3ê°œë§Œ ì €ì¥
            "metadata": {
                "total_documents": len(shared_state.get("pdfs", [])),
                "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
                "vector_dimension": DIMENSION,
            },
        }

        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False, indent=None) + "\n")

        print(
            f"ğŸ’¬ ì±„íŒ… ë¡œê·¸ ê¸°ë¡: {self.log_file.name} ({len(user_query)} chars query)"
        )

    def get_log_files(self) -> List[Path]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œê·¸ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        return sorted(self.log_dir.glob("chat_session_*.jsonl"), reverse=True)


class FaissIndexComparator:
    """ë‹¤ì–‘í•œ FAISS ì¸ë±ìŠ¤ íƒ€ì…ë³„ ì„±ëŠ¥ ë¹„êµ"""

    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.indexes = {}
        self.performance_logger = PerformanceLogger()

    def load_indexes_from_files(self) -> Dict:
        """íŒŒì¼ì—ì„œ ì‚¬ì „ êµ¬ì¶•ëœ FAISS ì¸ë±ìŠ¤ë“¤ ë¡œë“œ"""
        print("ğŸ”§ ì‚¬ì „ êµ¬ì¶•ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")

        results = {}

        # ê¸°ë³¸ IndexFlatL2 (vectorstoreì—ì„œ)
        if (DB_PATH / "index.faiss").exists():
            flat_index = faiss.read_index(str(DB_PATH / "index.faiss"))
            results["IndexFlatL2"] = {
                "index": flat_index,
                "memory_usage": flat_index.d * flat_index.ntotal * 4,  # float32
            }

        # IndexIVFFlat
        if (DB_PATH / "vectorstore_ivf.faiss").exists():
            ivf_index = faiss.read_index(str(DB_PATH / "vectorstore_ivf.faiss"))
            results["IndexIVFFlat"] = {
                "index": ivf_index,
                "memory_usage": ivf_index.d * ivf_index.ntotal * 4,
            }

        # IndexHNSWFlat
        if (DB_PATH / "vectorstore_hnsw.faiss").exists():
            hnsw_index = faiss.read_index(str(DB_PATH / "vectorstore_hnsw.faiss"))
            results["IndexHNSWFlat"] = {
                "index": hnsw_index,
                "memory_usage": hnsw_index.d * hnsw_index.ntotal * 4,
            }

        # ê²°ê³¼ ì €ì¥
        self.indexes = {name: data["index"] for name, data in results.items()}

        print(f"âœ… {len(results)}ê°œ ì¸ë±ìŠ¤ íƒ€ì… ë¡œë“œ ì™„ë£Œ")
        return results

    def compare_search_performance(self, query_vector: np.ndarray, k: int = 3) -> Dict:
        """ëª¨ë“  ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ"""
        results = {}

        for index_name, index in self.indexes.items():
            start_time = time.time()

            try:
                distances, indices = index.search(query_vector.reshape(1, -1), k)
                search_time = time.time() - start_time
                results[index_name] = {
                    "search_time_ms": search_time * 1000,  # ë°€ë¦¬ì´ˆë¡œ ë³€í™˜
                    "distances": distances.tolist(),
                    "indices": indices.tolist(),
                    "success": True,
                }

            except Exception as e:
                results[index_name] = {
                    "search_time_ms": 0,
                    "error": str(e),
                    "success": False,
                }
                print(f"âš ï¸ {index_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        return results


class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œë¥¼ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        self.start_time: Optional[float] = None
        self.first_token_time: Optional[float] = None
        self.end_time: Optional[float] = None
        self.token_count: int = 0
        self.retrieval_time: float = 0.0
        self.knee_info: Dict = {}

    def start_query(self):
        """ì¿¼ë¦¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡"""
        self.start_time = time.time()
        self.first_token_time = None
        self.end_time = None
        self.token_count = 0
        self.retrieval_time = 0.0
        self.knee_info = {}

    def first_token_received(self):
        """ì²« í† í° ìˆ˜ì‹  ì‹œê°„ ê¸°ë¡"""
        if self.first_token_time is None:
            self.first_token_time = time.time()

    def add_token(self, token_text: str):
        """í† í° ì¶”ê°€"""
        self.token_count += len(token_text.split())

    def query_complete(self):
        """ì¿¼ë¦¬ ì™„ë£Œ ì‹œê°„ ê¸°ë¡"""
        self.end_time = time.time()

    def get_metrics(self) -> Dict:
        """í˜„ì¬ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        if not self.start_time:
            return {"ìƒíƒœ": "ì¸¡ì • ì¤‘..."}

        metrics = {}
        current_time = self.end_time or time.time()

        # ì²« í† í°ê¹Œì§€ ì‹œê°„
        if self.first_token_time:
            time_to_first_token = self.first_token_time - self.start_time
            metrics["ì²« í† í°"] = f"{time_to_first_token:.2f}ì´ˆ"
        else:
            metrics["ì²« í† í°"] = "ëŒ€ê¸° ì¤‘..."

        # ì „ì²´ ì‘ë‹µ ì‹œê°„
        if self.end_time:
            total_time = self.end_time - self.start_time
            metrics["ì´ ì‹œê°„"] = f"{total_time:.2f}ì´ˆ"

            # í† í°/ì´ˆ
            if self.token_count > 0 and total_time > 0:
                tokens_per_second = self.token_count / total_time
                metrics["ì†ë„"] = f"{tokens_per_second:.1f} tokens/s"
        else:
            elapsed = current_time - self.start_time
            metrics["ê²½ê³¼ ì‹œê°„"] = f"{elapsed:.1f}ì´ˆ"

        # ê²€ìƒ‰ ì‹œê°„
        if self.retrieval_time > 0:
            metrics["ê²€ìƒ‰ ì‹œê°„"] = f"{self.retrieval_time:.2f}ì´ˆ"

        # Knee detection ì •ë³´ (ìˆëŠ” ê²½ìš°)
        if hasattr(self, "knee_info") and self.knee_info:
            knee = self.knee_info
            if knee.get("knee_point") is not None:
                metrics["ë¬¸ì„œ ì„ íƒ"] = (
                    f"{knee['selected_docs']}/{knee['total_docs']} (knee:{knee['knee_point']})"
                )
            else:
                metrics["ë¬¸ì„œ ì„ íƒ"] = (
                    f"{knee['selected_docs']}/{knee['total_docs']} ({knee.get('reason', 'auto')})"
                )

            # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if knee.get("category_aware") and knee.get("category_distribution", {}).get(
                "summary"
            ):
                metrics["ì¹´í…Œê³ ë¦¬"] = knee["category_distribution"]["summary"]

        return metrics


# Session variables
MAX_SESSIONS = 100
sessions = OrderedDict()
session_lock = threading.Lock()

# Global shared state
shared_state: Dict = {
    "current_model": None,
    "vectorstore": None,
    "faiss_comparator": None,
    "performance_logger": None,
    "chat_logger": None,
    "database_loaded": False,
    "database_info": {},
    "category_mapping": {},  # ì¹´í…Œê³ ë¦¬ ì •ë³´
    "category_aware": False,  # ì¹´í…Œê³ ë¦¬ ì¸ì‹ ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
}
shared_state_lock = threading.Lock()

# Performance trackers
performance_trackers = {
    method: PerformanceMetrics() for method in RERANK_OPTIONS.keys()
}


def load_existing_database():
    """ê¸°ì¡´ FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ (ì¹´í…Œê³ ë¦¬ ì •ë³´ í¬í•¨)"""
    print("ğŸ” ê¸°ì¡´ FAISS ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...")

    if not DB_PATH.exists():
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DB_PATH}")
        print("ğŸ› ï¸ ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: python build_rule_database.py")
        return False

    if not (DB_PATH / "index.faiss").exists() or not (DB_PATH / "index.pkl").exists():
        print("âŒ FAISS ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("ğŸ› ï¸ ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: python build_rule_database.py")
        return False

    try:
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        info_file = DB_PATH / "database_info.json"
        if info_file.exists():
            with open(info_file, "r", encoding="utf-8") as f:
                database_info = json.load(f)
                shared_state["database_info"] = database_info
                print(
                    f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´: {database_info['total_documents']}ê°œ ë¬¸ì„œ, {database_info['total_chunks']}ê°œ ì²­í¬"
                )

        # ğŸ¯ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ë³´ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        category_file = DB_PATH / "category_mapping.json"
        if category_file.exists():
            with open(category_file, "r", encoding="utf-8") as f:
                category_mapping = json.load(f)
                shared_state["category_mapping"] = category_mapping
                shared_state["category_aware"] = True
                print(
                    f"ğŸ¯ ì¹´í…Œê³ ë¦¬ ì¸ì‹ ê¸°ëŠ¥ í™œì„±í™”: {len(category_mapping)}ê°œ ì¹´í…Œê³ ë¦¬"
                )

                # ì¹´í…Œê³ ë¦¬ë³„ ì •ë³´ ì¶œë ¥
                for category, info in category_mapping.items():
                    priority = info.get("priority", 0)
                    doc_count = info.get("doc_count", 0)
                    print(f"  ğŸ“‹ {category} (ìš°ì„ ìˆœìœ„ {priority}): {doc_count}ê°œ ì²­í¬")
        else:
            print("ğŸ“ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ íŒŒì¼ ì—†ìŒ - ê¸°ë³¸ knee detection ëª¨ë“œ")
            shared_state["category_aware"] = False

        # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        print("ğŸ”„ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        vectorstore = FAISS.load_local(
            str(DB_PATH), EMBED_MODEL, allow_dangerous_deserialization=True
        )

        with shared_state_lock:
            shared_state["vectorstore"] = vectorstore

            # ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            print("ğŸ”§ FAISS ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            shared_state["performance_logger"] = PerformanceLogger()
            shared_state["chat_logger"] = ChatLogger()
            shared_state["faiss_comparator"] = FaissIndexComparator(dimension=DIMENSION)

            # ì‚¬ì „ êµ¬ì¶•ëœ ì¸ë±ìŠ¤ë“¤ ë¡œë“œ
            index_results = shared_state["faiss_comparator"].load_indexes_from_files()
            print(f"âœ… ì„±ëŠ¥ ë¹„êµ ì¤€ë¹„ ì™„ë£Œ: {len(index_results)}ê°œ ì¸ë±ìŠ¤ íƒ€ì…")

            shared_state["database_loaded"] = True

        if shared_state["category_aware"]:
            print("âœ… ì¹´í…Œê³ ë¦¬ ì¸ì‹ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
        else:
            print("âœ… ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

        return True

    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


# --------- (B) SESSION SETUP ---------
def get_session_id(request: gr.Request):
    raw_id = request.client.host + str(request.headers.get("user-agent"))
    return hashlib.md5(raw_id.encode()).hexdigest()


def init_session(session_id: str):
    sessions[session_id] = {
        "client": None,
        "history": {method: [] for method in RERANK_OPTIONS.keys()},
    }


def get_client(model_name: str):
    model_info = MODELS[model_name]

    if model_info["provider"] == "openai":
        return {"type": "litellm", "api_key": OPENAI_API_KEY, "base_url": None}
    elif model_info["provider"] == "novita":
        # NovitaëŠ” LiteLLMì—ì„œ ê³µì‹ ì§€ì›ë¨ - ë¬¸ì„œ ì°¸ì¡°
        # https://docs.litellm.ai/docs/providers/novita
        return {
            "type": "litellm",
            "api_key": NOVITA_API_KEY,
            "base_url": "https://api.novita.ai/v3/openai",
        }
    elif model_info["provider"] == "hf_inference":
        return InferenceClient(api_key=HF_API_KEY)
    else:
        raise ValueError(f"Unknown provider: {model_info['provider']}")


# --------- (C) DYNAMIC KNEE RETRIEVER CLASS ---------
class DynamicKneeRetriever(BaseRetriever):
    """
    Knee Point Detectionì„ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê°œìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” Retriever

    ê³ ì •ëœ kê°œ ëŒ€ì‹  ìœ ì‚¬ë„ ê³¡ì„ ì˜ knee pointë¥¼ ì°¾ì•„ì„œ
    ìì—°ìŠ¤ëŸ¬ìš´ cutoff ì§€ì ê¹Œì§€ì˜ ëª¨ë“  ê´€ë ¨ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        vectorstore: FAISS,
        min_docs: int = 2,
        max_docs: int = 20,
        sensitivity: float = 1.0,
        direction: str = "decreasing",
        curve: str = "convex",
    ):
        """
        Args:
            vectorstore: FAISS ë²¡í„°ìŠ¤í† ì–´
            min_docs: ìµœì†Œ ë°˜í™˜ ë¬¸ì„œ ìˆ˜
            max_docs: ìµœëŒ€ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (knee ì°¾ê¸°ìš©)
            sensitivity: knee detection ë¯¼ê°ë„ (ê¸°ë³¸ 1.0)
            direction: "decreasing" (ê±°ë¦¬ ê¸°ì¤€) ë˜ëŠ” "increasing" (ìœ ì‚¬ë„ ê¸°ì¤€)
            curve: "convex" ë˜ëŠ” "concave"
        """
        super().__init__()
        self.vectorstore = vectorstore
        self.min_docs = min_docs
        self.max_docs = max_docs
        self.sensitivity = sensitivity
        self.direction = direction
        self.curve = curve
        self.last_knee_info = {}  # ë§ˆì§€ë§‰ knee ë¶„ì„ ê²°ê³¼ ì €ì¥

    def _get_relevant_documents(self, query: str, **kwargs) -> List[Document]:
        """ì¿¼ë¦¬ì— ëŒ€í•´ knee point ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ë°˜í™˜"""
        try:
            # 1. ìµœëŒ€ ê°œìˆ˜ë¡œ ë¬¸ì„œì™€ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            docs_and_scores = self.vectorstore.similarity_search_with_score(
                query, k=self.max_docs
            )

            if len(docs_and_scores) <= self.min_docs:
                # ë¬¸ì„œê°€ ë„ˆë¬´ ì ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ ë°˜í™˜
                category_analysis = self._analyze_categories(
                    [doc for doc, _ in docs_and_scores]
                )
                self.last_knee_info = {
                    "total_docs": len(docs_and_scores),
                    "selected_docs": len(docs_and_scores),
                    "knee_point": None,
                    "reason": "Too few documents",
                    "category_distribution": category_analysis,
                    "category_aware": shared_state.get("category_aware", False),
                }
                return [doc for doc, _ in docs_and_scores]

            # ğŸ¯ 2. ì¹´í…Œê³ ë¦¬ ì¸ì‹ ì ìˆ˜ ì¡°ì • (ìˆëŠ” ê²½ìš°)
            if shared_state.get("category_aware", False):
                docs_and_scores = self._apply_category_priority(docs_and_scores, query)

            # 3. ì ìˆ˜ ì¶”ì¶œ ë° ì •ë ¬
            scores = [score for _, score in docs_and_scores]
            documents = [doc for doc, _ in docs_and_scores]

            # 4. Knee point ì°¾ê¸°
            knee_idx = self._find_knee_point(scores)

            # 5. ìµœì¢… ë¬¸ì„œ ì„ íƒ
            if knee_idx is None or knee_idx < self.min_docs:
                selected_docs = documents[: self.min_docs]
                knee_reason = "No clear knee found, using min_docs"
            else:
                selected_docs = documents[: knee_idx + 1]
                knee_reason = f"Knee point detected at index {knee_idx}"

            # 6. ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ê²°ê³¼
            category_analysis = self._analyze_categories(selected_docs)

            # 7. ë¶„ì„ ê²°ê³¼ ì €ì¥
            self.last_knee_info = {
                "total_docs": len(docs_and_scores),
                "selected_docs": len(selected_docs),
                "knee_point": knee_idx,
                "scores": scores[:10],
                "selected_scores": [
                    score for _, score in docs_and_scores[: len(selected_docs)]
                ],
                "reason": knee_reason,
                "sensitivity": self.sensitivity,
                "category_distribution": category_analysis,
                "category_aware": shared_state.get("category_aware", False),
            }

            # ì¶œë ¥ ë©”ì‹œì§€ì— ì¹´í…Œê³ ë¦¬ ì •ë³´ í¬í•¨
            category_info = (
                f" | {category_analysis['summary']}"
                if category_analysis.get("summary")
                else ""
            )
            mode_indicator = (
                "ğŸ¯ Category-Aware"
                if shared_state.get("category_aware", False)
                else "ğŸ”"
            )
            print(
                f"{mode_indicator} Dynamic Retrieval: {len(selected_docs)}/{len(docs_and_scores)} docs selected (knee at {knee_idx}){category_info}"
            )
            return selected_docs

        except Exception as e:
            print(f"âŒ DynamicKneeRetriever error: {e}")
            # ì—ëŸ¬ ì‹œ fallbackìœ¼ë¡œ ê¸°ë³¸ ê²€ìƒ‰
            return self.vectorstore.similarity_search(query, k=self.min_docs)

    def _apply_category_priority(self, docs_and_scores, query):
        """ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ ì ìˆ˜ ì¡°ì •"""
        category_mapping = shared_state.get("category_mapping", {})
        if not category_mapping:
            return docs_and_scores

        # ì¿¼ë¦¬ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ê°ì§€
        query_lower = query.lower()
        relevant_categories = []

        # ì¿¼ë¦¬ì—ì„œ ì¹´í…Œê³ ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
        if "í•™ì‚¬" in query_lower or "í•™ë¶€" in query_lower:
            relevant_categories.extend(["í•™ì‚¬ê·œì •", "í•™ì¹™"])
        if "ëŒ€í•™ì›" in query_lower or "ì„ì‚¬" in query_lower or "ë°•ì‚¬" in query_lower:
            relevant_categories.append("ëŒ€í•™ì›ê·œì •")
        if "ì—°êµ¬" in query_lower:
            relevant_categories.append("ì—°êµ¬ê·œì •")
        if "ë“±ë¡" in query_lower or "í•™ë¹„" in query_lower:
            relevant_categories.append("ë“±ë¡ê·œì •")
        if "ì¥í•™" in query_lower:
            relevant_categories.append("ì¥í•™ê·œì •")
        if "ê¸°ìˆ™ì‚¬" in query_lower or "ìƒí™œê´€" in query_lower:
            relevant_categories.append("ìƒí™œê·œì •")

        adjusted_docs_and_scores = []

        for doc, score in docs_and_scores:
            category = doc.metadata.get("category", "ê¸°íƒ€")
            priority = doc.metadata.get("priority", 1)

            # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜ (ë†’ì€ ìš°ì„ ìˆœìœ„ì¼ìˆ˜ë¡ ì ìˆ˜ í–¥ìƒ)
            priority_weight = 1.0 - (
                priority / 20.0
            )  # ìš°ì„ ìˆœìœ„ 10 â†’ 0.5, ìš°ì„ ìˆœìœ„ 1 â†’ 0.95

            # ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ê°€ì¤‘ì¹˜
            relevance_weight = 0.8 if category in relevant_categories else 1.0

            # ì¡°ì •ëœ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•¨)
            adjusted_score = score * priority_weight * relevance_weight

            adjusted_docs_and_scores.append((doc, adjusted_score))

        # ì¡°ì •ëœ ì ìˆ˜ë¡œ ì¬ì •ë ¬
        adjusted_docs_and_scores.sort(key=lambda x: x[1])

        return adjusted_docs_and_scores

    def _analyze_categories(self, documents):
        """ì„ íƒëœ ë¬¸ì„œë“¤ì˜ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë¶„ì„"""
        if not documents:
            return {"categories": {}, "summary": ""}

        category_count = {}
        total_docs = len(documents)

        for doc in documents:
            category = doc.metadata.get("category", "ê¸°íƒ€")
            priority = doc.metadata.get("priority", 1)

            if category not in category_count:
                category_count[category] = {"count": 0, "priority": priority}
            category_count[category]["count"] += 1

        # ì¹´í…Œê³ ë¦¬ë³„ ë¹„ìœ¨ ê³„ì‚° ë° ìš”ì•½ ìƒì„±
        category_summary = []
        for category, info in sorted(
            category_count.items(), key=lambda x: x[1]["priority"], reverse=True
        ):
            ratio = info["count"] / total_docs * 100
            if ratio >= 10:  # 10% ì´ìƒì¸ ì¹´í…Œê³ ë¦¬ë§Œ ìš”ì•½ì— í¬í•¨
                category_summary.append(f"{category}({info['count']}ê°œ)")

        summary = ", ".join(category_summary) if category_summary else "í˜¼í•© ì¹´í…Œê³ ë¦¬"

        return {
            "categories": category_count,
            "total_docs": total_docs,
            "summary": summary,
        }

    def _find_knee_point(self, scores: List[float]) -> Optional[int]:
        """ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ knee point ì°¾ê¸°"""
        if len(scores) < 3:  # ìµœì†Œ 3ê°œëŠ” ìˆì–´ì•¼ knee ì°¾ê¸° ê°€ëŠ¥
            return None

        try:
            # xì¶•ì€ ë¬¸ì„œ ì¸ë±ìŠ¤, yì¶•ì€ ê±°ë¦¬/ì ìˆ˜
            x = list(range(len(scores)))
            y = scores

            # KneeLocatorë¡œ knee point ì°¾ê¸°
            kl = KneeLocator(
                x=x,
                y=y,
                curve=self.curve,
                direction=self.direction,
                S=self.sensitivity,
                online=True,  # ì˜¨ë¼ì¸ ëª¨ë“œë¡œ ë” ì •í™•í•œ íƒì§€
            )

            return kl.knee

        except Exception as e:
            print(f"âš ï¸ Knee detection failed: {e}")
            return None

    def get_knee_info(self) -> Dict:
        """ë§ˆì§€ë§‰ knee ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        return self.last_knee_info.copy()


class DynamicKneeCompressionRetriever(ContextualCompressionRetriever):
    """Cross-Encoder Rerankerì™€ DynamicKneeRetrieverë¥¼ ê²°í•©í•œ Retriever"""

    def __init__(
        self,
        base_compressor,
        vectorstore: FAISS,
        min_docs: int = 2,
        max_docs: int = 20,
        rerank_top_k: int = 10,
    ):
        # DynamicKneeRetrieverë¥¼ base retrieverë¡œ ì‚¬ìš©
        base_retriever = DynamicKneeRetriever(
            vectorstore=vectorstore, min_docs=min_docs, max_docs=max_docs
        )

        super().__init__(base_compressor=base_compressor, base_retriever=base_retriever)

        # rerankerì˜ top_këŠ” base_retrieverì—ì„œ ì²˜ë¦¬ë¨

    def get_knee_info(self) -> Dict:
        """Base retrieverì˜ knee ì •ë³´ ë°˜í™˜"""
        if isinstance(self.base_retriever, DynamicKneeRetriever):
            return self.base_retriever.get_knee_info()
        return {}


# --------- (D) RETRIEVER CREATION ---------
def create_retriever(vectorstore, rerank_method="ì—†ìŒ", use_dynamic_knee=True):
    """
    Dynamic Knee Point Detectionì„ ì‚¬ìš©í•˜ì—¬ retriever ìƒì„±

    Args:
        vectorstore: FAISS ë²¡í„°ìŠ¤í† ì–´
        rerank_method: ë¦¬ë­í‚¹ ë°©ë²• ("ì—†ìŒ" ë˜ëŠ” cross-encoder ëª¨ë¸ëª…)
        use_dynamic_knee: knee point detection ì‚¬ìš© ì—¬ë¶€
    """

    if rerank_method == "ì—†ìŒ" or not rerank_method:
        if use_dynamic_knee:
            print("ğŸ¯ Creating DynamicKneeRetriever (no reranking)")
            return DynamicKneeRetriever(
                vectorstore=vectorstore,
                min_docs=2,  # ìµœì†Œ 2ê°œ ë¬¸ì„œ
                max_docs=25,  # ìµœëŒ€ 25ê°œê¹Œì§€ ê²€ìƒ‰í•´ì„œ knee ì°¾ê¸°
                sensitivity=1.0,  # ê¸°ë³¸ ë¯¼ê°ë„
            )
        else:
            # ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„±)
            return vectorstore.as_retriever(search_kwargs={"k": 3})

    try:
        print(f"ğŸ¯ Creating DynamicKneeCompressionRetriever with {rerank_method}")
        cross_encoder = HuggingFaceCrossEncoder(model_name=rerank_method)
        compressor = CrossEncoderReranker(model=cross_encoder)  # top_këŠ” ë‚˜ì¤‘ì— ì„¤ì •

        if use_dynamic_knee:
            return DynamicKneeCompressionRetriever(
                base_compressor=compressor,
                vectorstore=vectorstore,
                min_docs=2,
                max_docs=25,
                rerank_top_k=15,
            )
        else:
            # ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„±)
            return ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=vectorstore.as_retriever(search_kwargs={"k": 10}),
            )
    except Exception as e:
        print(f"âŒ Reranker creation failed for {rerank_method}: {e}")
        print("ğŸ”„ Falling back to DynamicKneeRetriever")
        if use_dynamic_knee:
            return DynamicKneeRetriever(
                vectorstore=vectorstore, min_docs=2, max_docs=15, sensitivity=1.0
            )
        else:
            return vectorstore.as_retriever(search_kwargs={"k": 3})


# --------- (D) QUERY HANDLERS ---------
def handle_query_for_rerank(
    user_query: str, rerank_method: str, request: gr.Request
) -> Generator:
    """íŠ¹ì • rerank ë°©ë²•ìœ¼ë¡œ ì¿¼ë¦¬ ì²˜ë¦¬"""
    session_id = get_session_id(request)

    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        session = sessions[session_id]
        sessions.move_to_end(session_id)

    # ì„±ëŠ¥ ì¶”ì  ì‹œì‘
    metrics = performance_trackers[rerank_method]
    metrics.start_query()

    # íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    history = session["history"][rerank_method]
    messages = history.copy()
    client = session["client"]

    # í˜„ì¬ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    with shared_state_lock:
        current_model = str(shared_state["current_model"])
        vectorstore = shared_state["vectorstore"]
        faiss_comparator = shared_state.get("faiss_comparator")
        performance_logger = shared_state.get("performance_logger")

    model_info = MODELS[current_model]

    # Extract relevant text data from PDFs with Dynamic Knee Detection
    context = ""
    faiss_performance_results = {}
    knee_detection_info = {}

    if vectorstore:
        print(
            f"ğŸ” [{rerank_method}] Retrieving relevant GIST rules with Dynamic Knee Detection..."
        )
        retrieval_start = time.time()

        retriever = create_retriever(vectorstore, rerank_method, use_dynamic_knee=True)
        if retriever:
            docs = retriever.invoke(user_query)

            # Knee detection ì •ë³´ ìˆ˜ì§‘
            try:
                if isinstance(retriever, DynamicKneeRetriever):
                    knee_detection_info = retriever.get_knee_info()
                elif isinstance(retriever, DynamicKneeCompressionRetriever):
                    knee_detection_info = retriever.get_knee_info()
                elif hasattr(retriever, "base_retriever") and isinstance(
                    retriever.base_retriever, DynamicKneeRetriever
                ):
                    knee_detection_info = retriever.base_retriever.get_knee_info()
            except Exception as e:
                print(f"âš ï¸ Knee detection ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                knee_detection_info = {}

            # ë¬¸ì„œ ì†ŒìŠ¤ ì •ë³´ í¬í•¨
            context_parts: List[str] = []
            for doc in docs:
                source_info = doc.metadata.get(
                    "filename", doc.metadata.get("source", "")
                )
                category = doc.metadata.get("category", "")
                context_parts.append(f"[{category}] {source_info}:\n{doc.page_content}")
            context = "\n\n".join(context_parts)

        retrieval_end = time.time()
        metrics.retrieval_time = retrieval_end - retrieval_start

        # Enhanced logging with knee detection info
        knee_summary = ""
        if knee_detection_info:
            total_docs = knee_detection_info.get("total_docs", 0)
            selected_docs = knee_detection_info.get("selected_docs", 0)
            knee_point = knee_detection_info.get("knee_point")
            reason = knee_detection_info.get("reason", "Unknown")

            if knee_point is not None:
                knee_summary = (
                    f" (knee at {knee_point}: {selected_docs}/{total_docs} docs)"
                )
            else:
                knee_summary = f" ({reason}: {selected_docs}/{total_docs} docs)"

        print(
            f"ğŸ“Š [{rerank_method}] Retrieved {len(docs) if 'docs' in locals() else 0} documents in {metrics.retrieval_time:.2f}s{knee_summary}"
        )

        # Knee detection ì •ë³´ë¥¼ metricsì— ì €ì¥
        if knee_detection_info:
            metrics.knee_info = knee_detection_info

        # ğŸš€ FAISS ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
        if faiss_comparator and performance_logger:
            print(f"ğŸ”¬ [{rerank_method}] FAISS ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¹„êµ ì‹œì‘...")
            try:
                # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
                query_vector = EMBED_MODEL.embed_query(user_query)
                query_vector_np = np.array(query_vector, dtype=np.float32)

                # ëª¨ë“  ì¸ë±ìŠ¤ì—ì„œ ì„±ëŠ¥ ë¹„êµ
                faiss_performance_results = faiss_comparator.compare_search_performance(
                    query_vector_np, k=3
                )

                # ì„±ëŠ¥ ê²°ê³¼ ë¡œê¹…
                performance_logger.log_query_performance(
                    user_query,
                    {
                        "rerank_method": rerank_method,
                        "faiss_performance": faiss_performance_results,
                        "retrieval_time": metrics.retrieval_time,
                    },
                )

                # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
                print(f"ğŸ“Š [{rerank_method}] FAISS ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
                for index_name, result in faiss_performance_results.items():
                    if result.get("success"):
                        print(f"   â€¢ {index_name}: {result['search_time_ms']:.2f}ms")
                    else:
                        print(
                            f"   â€¢ {index_name}: ì‹¤íŒ¨ - {result.get('error', 'Unknown error')}"
                        )

            except Exception as e:
                print(f"âš ï¸ [{rerank_method}] FAISS ì„±ëŠ¥ ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
                faiss_performance_results = {"error": str(e)}

    messages.append(
        {
            "role": "user",
            "content": f"Context (GIST Rules & Regulations):\n{context}\n\nQuestion: {user_query}",
        }
    )

    # Add user message to history first
    history.append({"role": "user", "content": user_query})

    # Create initial assistant message placeholder
    history.append({"role": "assistant", "content": ""})

    # Yield initial state with user query
    yield history, format_metrics(metrics.get_metrics(), rerank_method)

    # Invoke client with user query using streaming
    print(f"ğŸ’¬ [{rerank_method}] Inquiring LLM with streaming...")

    try:
        if model_info["provider"] in ("openai", "novita"):
            # LiteLLM ìŠ¤íŠ¸ë¦¬ë°
            api_key = (
                client.get("api_key") if isinstance(client, dict) else OPENAI_API_KEY
            )
            base_url = client.get("base_url") if isinstance(client, dict) else None
            completion_fn: Callable[..., Any] = getattr(litellm, "completion")
            completion = completion_fn(
                model=model_info["model_id"],
                messages=messages,
                stream=True,
                api_key=api_key,
                base_url=base_url,
            )

            bot_response = ""
            for chunk in completion:
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and chunk.choices[0].delta
                    and chunk.choices[0].delta.content
                ):
                    if not metrics.first_token_time:
                        metrics.first_token_received()

                    chunk_content = chunk.choices[0].delta.content
                    bot_response += chunk_content
                    metrics.add_token(chunk_content)

                    # Update the last message (assistant's response)
                    history[-1]["content"] = html.escape(bot_response)
                    yield history, format_metrics(metrics.get_metrics(), rerank_method)

        else:
            # HuggingFace Inference Client ìŠ¤íŠ¸ë¦¬ë°
            completion = client.chat.completions.create(
                model=model_info["model_id"], messages=messages, stream=True
            )

            bot_response = ""
            for chunk in completion:
                if hasattr(chunk, "choices") and chunk.choices[0].delta.content:
                    if not metrics.first_token_time:
                        metrics.first_token_received()

                    chunk_content = chunk.choices[0].delta.content
                    bot_response += chunk_content
                    metrics.add_token(chunk_content)

                    # Update the last message (assistant's response)
                    history[-1]["content"] = html.escape(bot_response)
                    yield history, format_metrics(metrics.get_metrics(), rerank_method)

    except Exception as e:
        error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e) if str(e) else 'ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'}"
        print(
            f"âŒ [{rerank_method}] Streaming error: {e if str(e) else 'Unknown streaming error'}"
        )
        history[-1]["content"] = error_msg
        yield history, format_metrics(metrics.get_metrics(), rerank_method)

    # ì™„ë£Œ ì‹œì  ê¸°ë¡
    metrics.query_complete()
    final_metrics = format_metrics(metrics.get_metrics(), rerank_method)

    # ğŸš€ ì±„íŒ… ìƒí˜¸ì‘ìš© ë¡œê¹…
    with shared_state_lock:
        chat_logger = shared_state.get("chat_logger")

    if chat_logger and "bot_response" in locals():
        try:
            # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            retrieved_doc_sources = []
            if "docs" in locals() and docs:
                for doc in docs:
                    source = doc.metadata.get(
                        "filename", doc.metadata.get("source", "Unknown")
                    )
                    category = doc.metadata.get("category", "")
                    retrieved_doc_sources.append(f"[{category}] {source}")

            # ì±„íŒ… ë¡œê·¸ ê¸°ë¡
            chat_logger.log_chat_interaction(
                user_query=user_query,
                bot_response=bot_response,
                rerank_method=rerank_method,
                model_info=model_info,
                performance_metrics=metrics.get_metrics(),
                retrieved_docs=retrieved_doc_sources,
                faiss_performance=faiss_performance_results
                if "faiss_performance_results" in locals()
                else None,
                knee_detection_info=knee_detection_info
                if "knee_detection_info" in locals()
                else None,
                session_id=session_id,
            )
        except Exception as e:
            print(f"âš ï¸ ì±„íŒ… ë¡œê·¸ ê¸°ë¡ ì¤‘ ì˜¤ë¥˜: {e}")

    yield history, final_metrics


def handle_multi_query(user_query, request: gr.Request):
    """ëª¨ë“  rerank ëª¨ë“œì—ì„œ ë™ì‹œì— ì¿¼ë¦¬ ì‹¤í–‰"""
    if not user_query.strip():
        return [[] for _ in RERANK_OPTIONS.keys()] + [
            format_metrics({}, method) for method in RERANK_OPTIONS.keys()
        ]

    print(f"ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {user_query[:50]}...")

    # ëª¨ë“  rerank ëª¨ë“œì— ëŒ€í•´ ì œë„ˆë ˆì´í„° ìƒì„±
    generators = {
        method: handle_query_for_rerank(user_query, method, request)
        for method in RERANK_OPTIONS.keys()
    }

    # í˜„ì¬ ìƒíƒœ ì¶”ì 
    current_states = {
        method: ([], format_metrics({}, method)) for method in RERANK_OPTIONS.keys()
    }
    active_generators = set(RERANK_OPTIONS.keys())

    while active_generators:
        updated_methods = set()

        for method in list(active_generators):
            try:
                history, metrics = next(generators[method])
                current_states[method] = (history, metrics)
                updated_methods.add(method)
            except StopIteration:
                active_generators.remove(method)
                print(f"âœ… {method} completed")

        if updated_methods or len(active_generators) == 0:
            # ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
            results = []
            for method in RERANK_OPTIONS.keys():
                history, metrics = current_states[method]
                results.extend([history, metrics])

            yield results

    print("âœ… ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ì™„ë£Œ!")


def handle_additional_pdf_upload(pdfs, request: gr.Request):
    """ì¶”ê°€ PDF ì—…ë¡œë“œ ì²˜ë¦¬"""
    if not pdfs:
        return "ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

    print("ğŸ“„ Processing additional PDF(s)...")

    try:
        # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        additional_docs = []
        for pdf in pdfs:
            text = ""
            try:
                doc = fitz.open(pdf)
                text = "\n".join([page.get_text("text") for page in doc])
                doc.close()
            except Exception as e:
                print(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨ {pdf}: {e}")
                continue

            if text.strip():
                document = Document(
                    page_content=text,
                    metadata={"source": pdf, "filename": os.path.basename(pdf)},
                )
                docs = TEXT_SPLITTER.split_documents([document])
                additional_docs.extend(docs)

        if not additional_docs:
            return "ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
        with shared_state_lock:
            vectorstore = shared_state["vectorstore"]
            if vectorstore:
                print("ğŸ”„ Merging with existing documents...")
                new_vectorstore = FAISS.from_documents(additional_docs, EMBED_MODEL)
                vectorstore.merge_from(new_vectorstore)
                shared_state["vectorstore"] = vectorstore

        return f"âœ… Added {len(pdfs)} PDFs in {time.time():.2f} seconds"

    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# --------- (D) ë¡œê·¸ ë·°ì–´ ê¸°ëŠ¥ ---------
def get_log_files():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œê·¸ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
    chat_logs_dir = Path("chat_logs")
    performance_logs_dir = Path("performance_logs")

    log_files = []

    # ì±„íŒ… ë¡œê·¸ íŒŒì¼ë“¤
    if chat_logs_dir.exists():
        for log_file in sorted(
            chat_logs_dir.glob("chat_session_*.jsonl"), reverse=True
        ):
            log_files.append(f"ğŸ“ {log_file.name} (ì±„íŒ… ë¡œê·¸)")

    # ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼ë“¤
    if performance_logs_dir.exists():
        for log_file in sorted(
            performance_logs_dir.glob("faiss_performance_*.jsonl"), reverse=True
        ):
            log_files.append(f"âš¡ {log_file.name} (ì„±ëŠ¥ ë¡œê·¸)")

    return log_files if log_files else ["ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."]


def load_log_content(selected_log_file: str):
    """ì„ íƒëœ ë¡œê·¸ íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ì„œ í¬ë§·ëœ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    if not selected_log_file or "ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤" in selected_log_file:
        return "ğŸ“ ë¡œê·¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."

    try:
        # íŒŒì¼ëª… ì¶”ì¶œ (ì´ëª¨ì§€ì™€ ì„¤ëª… ì œê±°)
        file_name = selected_log_file.split(" ")[1]  # ì´ëª¨ì§€ ë‹¤ìŒ ì²« ë²ˆì§¸ ë‹¨ì–´

        # íŒŒì¼ ê²½ë¡œ ê²°ì •
        if "ì±„íŒ… ë¡œê·¸" in selected_log_file:
            log_path = Path("chat_logs") / file_name
        elif "ì„±ëŠ¥ ë¡œê·¸" in selected_log_file:
            log_path = Path("performance_logs") / file_name
        else:
            return "âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë¡œê·¸ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

        if not log_path.exists():
            return f"âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_path}"

        # ë¡œê·¸ ë‚´ìš© ì½ê¸°
        with open(log_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        if not lines:
            return "ğŸ“ ë¡œê·¸ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

        # ë¡œê·¸ í¬ë§·íŒ…
        formatted_content = []
        formatted_content.append(f"# ğŸ“Š ë¡œê·¸ íŒŒì¼: {file_name}")
        formatted_content.append(f"ğŸ“ **ê²½ë¡œ**: `{log_path}`")
        formatted_content.append(f"ğŸ“ˆ **ì´ í•­ëª© ìˆ˜**: {len(lines)}ê°œ")
        formatted_content.append("\n---\n")

        for i, line in enumerate(lines[-20:], 1):  # ìµœê·¼ 20ê°œ í•­ëª©ë§Œ í‘œì‹œ
            try:
                log_entry = json.loads(line.strip())

                if "interaction" in log_entry:
                    # ì±„íŒ… ë¡œê·¸ í¬ë§·íŒ…
                    timestamp = (
                        log_entry.get("timestamp", "").replace("T", " ").split(".")[0]
                    )
                    query = (
                        log_entry["interaction"]["user_query"][:100] + "..."
                        if len(log_entry["interaction"]["user_query"]) > 100
                        else log_entry["interaction"]["user_query"]
                    )
                    response_length = log_entry["interaction"]["response_length"]
                    rerank_method = log_entry["rerank_info"]["method_name"]
                    model = log_entry["model_info"]["model_id"]

                    formatted_content.append(f"## ğŸ’¬ ì±„íŒ… #{len(lines) - 20 + i}")
                    formatted_content.append(f"**ì‹œê°„**: {timestamp}")
                    formatted_content.append(f"**ì§ˆë¬¸**: {query}")
                    formatted_content.append(f"**ë‹µë³€ ê¸¸ì´**: {response_length}ì")
                    formatted_content.append(f"**ë¦¬ë­í‚¹**: {rerank_method}")
                    formatted_content.append(f"**ëª¨ë¸**: {model}")

                    # ì„±ëŠ¥ ì§€í‘œ
                    if (
                        "performance_metrics" in log_entry
                        and log_entry["performance_metrics"]
                    ):
                        metrics = log_entry["performance_metrics"]
                        if "ì´ ì‹œê°„" in metrics:
                            formatted_content.append(
                                f"**ì´ ì‹œê°„**: {metrics['ì´ ì‹œê°„']}"
                            )
                        if "ì†ë„" in metrics:
                            formatted_content.append(f"**ì†ë„**: {metrics['ì†ë„']}")
                        if "ë¬¸ì„œ ì„ íƒ" in metrics:
                            formatted_content.append(
                                f"**ë¬¸ì„œ ì„ íƒ**: {metrics['ë¬¸ì„œ ì„ íƒ']}"
                            )

                    # Knee Detection ì •ë³´
                    if "knee_detection" in log_entry and log_entry["knee_detection"]:
                        knee_info = log_entry["knee_detection"]
                        formatted_content.append("**ğŸ¯ Knee Detection**:")
                        if knee_info.get("knee_point") is not None:
                            formatted_content.append(
                                f"  - Knee Point: ë¬¸ì„œ #{knee_info['knee_point']}"
                            )
                        formatted_content.append(
                            f"  - ì„ íƒëœ ë¬¸ì„œ: {knee_info.get('selected_docs', 0)}/{knee_info.get('total_docs', 0)}"
                        )
                        formatted_content.append(
                            f"  - ì´ìœ : {knee_info.get('reason', 'Unknown')}"
                        )
                        if knee_info.get("selected_scores"):
                            scores_str = ", ".join(
                                [f"{s:.3f}" for s in knee_info["selected_scores"][:5]]
                            )
                            formatted_content.append(
                                f"  - ì ìˆ˜ ë²”ìœ„: [{scores_str}...]"
                            )

                elif "results" in log_entry:
                    # ì„±ëŠ¥ ë¡œê·¸ í¬ë§·íŒ…
                    timestamp = (
                        log_entry.get("timestamp", "").replace("T", " ").split(".")[0]
                    )
                    query_hash = log_entry.get("query_hash", "")
                    rerank_method = log_entry["results"]["rerank_method"]

                    formatted_content.append(f"## âš¡ ì„±ëŠ¥ ì¸¡ì • #{len(lines) - 20 + i}")
                    formatted_content.append(f"**ì‹œê°„**: {timestamp}")
                    formatted_content.append(f"**ì¿¼ë¦¬ í•´ì‹œ**: {query_hash}")
                    formatted_content.append(f"**ë¦¬ë­í‚¹**: {rerank_method}")

                    # FAISS ì„±ëŠ¥ ê²°ê³¼
                    if "faiss_performance" in log_entry["results"]:
                        faiss_results = log_entry["results"]["faiss_performance"]
                        for index_name, result in faiss_results.items():
                            if result.get("success"):
                                formatted_content.append(
                                    f"**{index_name}**: {result['search_time_ms']:.2f}ms"
                                )

                formatted_content.append("\n---\n")

            except (json.JSONDecodeError, KeyError) as e:
                formatted_content.append(f"âŒ ë¡œê·¸ í•­ëª© íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {i}): {e}")
                continue

        return "\n".join(formatted_content)

    except Exception as e:
        return f"âŒ ë¡œê·¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"


# --------- (E) UTILITY FUNCTIONS ---------
def format_metrics(metrics: Dict, rerank_method: str) -> str:
    """ë©”íŠ¸ë¦­ì„ HTML í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
    if not metrics:
        return f"**{rerank_method}** - ì¸¡ì • ì¤‘..."

    # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
    try:
        formatted_lines = [f"**{rerank_method}**"]
        for key, value in metrics.items():
            formatted_lines.append(f"- **{key}**: {value}")

        result = "\n".join(formatted_lines)
        # ë¬¸ìì—´ íƒ€ì… í™•ì¸ ë° ë³´ì¥
        return str(result) if result else f"**{rerank_method}** - ë°ì´í„° ì—†ìŒ"

    except Exception as e:
        print(f"âŒ format_metrics ì˜¤ë¥˜: {e}")
        return f"**{rerank_method}** - í˜•ì‹ ì˜¤ë¥˜"


def copy_as_markdown(history, rerank_method):
    """ëŒ€í™” ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³µì‚¬"""
    if not history:
        return "ë³µì‚¬í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

    markdown_content = f"# GIST Rules Analyzer - {rerank_method} ê²€ìƒ‰ ê²°ê³¼\n\n"
    markdown_content += (
        f"**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    )

    for i, message in enumerate(history):
        if message["role"] == "user":
            markdown_content += f"## ğŸ™‹â€â™‚ï¸ ì§ˆë¬¸ {(i // 2) + 1}\n{message['content']}\n\n"
        elif message["role"] == "assistant":
            markdown_content += (
                f"## ğŸ¤– ë‹µë³€ ({rerank_method})\n{message['content']}\n\n"
            )

    return markdown_content


def reset_all_chats():
    """ëª¨ë“  ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"""
    with session_lock:
        for session in sessions.values():
            for method in RERANK_OPTIONS.keys():
                session["history"][method] = []

    # ì„±ëŠ¥ íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
    for tracker in performance_trackers.values():
        tracker.__init__()

    return [[] for _ in RERANK_OPTIONS.keys()] + [
        format_metrics({}, method) for method in RERANK_OPTIONS.keys()
    ]


def change_model(model_name: str):
    """ëª¨ë¸ ë³€ê²½"""
    print(f"ğŸ”„ Model changed to: {model_name}")

    with shared_state_lock:
        shared_state["current_model"] = model_name

    # ëª¨ë“  ì„¸ì…˜ì˜ í´ë¼ì´ì–¸íŠ¸ ì—…ë°ì´íŠ¸
    with session_lock:
        for session in sessions.values():
            session["client"] = get_client(model_name)

    return f"âœ… {model_name} ì¤€ë¹„ì™„ë£Œ"


# --------- (F) DATABASE STATUS ---------
def get_database_status():
    """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ë°˜í™˜"""
    with shared_state_lock:
        if not shared_state["database_loaded"]:
            return "âŒ ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\në¨¼ì € `python build_rule_database.py`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

        db_info = shared_state["database_info"]
        status_lines = [
            "âœ… **ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ**",
            "",
            "ğŸ“Š **í†µê³„**:",
            f"- ì´ ë¬¸ì„œ: {db_info.get('total_documents', 'N/A')}ê°œ",
            f"- ì´ ì²­í¬: {db_info.get('total_chunks', 'N/A')}ê°œ",
            f"- ì„ë² ë”© ì°¨ì›: {db_info.get('dimension', 'N/A')}",
            f"- ìƒì„± ì¼ì‹œ: {db_info.get('created_at', 'N/A')}",
            "",
            "ğŸ’¾ **íŒŒì¼ í¬ê¸°**:",
            f"- FAISS ì¸ë±ìŠ¤: {db_info.get('file_sizes', {}).get('vectorstore.faiss', 0) / (1024 * 1024):.1f}MB",
            f"- ë©”íƒ€ë°ì´í„°: {db_info.get('file_sizes', {}).get('vectorstore.pkl', 0) / (1024 * 1024):.1f}MB",
        ]

        additional_indexes = db_info.get("additional_indexes", {})
        if additional_indexes:
            status_lines.extend(
                ["", f"âš¡ **ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤**: {len(additional_indexes)}ê°œ ì¤€ë¹„ë¨"]
            )

        return "\n".join(status_lines)


# --------- (G) UI SETUP ---------
css = """
div {
    flex-wrap: nowrap !important;
}
.responsive-height {
    height: 100vh !important;
    padding-bottom: 20px !important;
}
.fill-height {
    height: 100% !important;
    flex-wrap: nowrap !important;
}
.extend-height {
    min-height: 300px !important;
    flex: 1 !important;
    overflow: auto !important;
}
.metrics-box {
    background: linear-gradient(135deg, #e3f2fd 0%, #f8f9fa 100%) !important;
    border: 2px solid #2196f3 !important;
    border-radius: 12px !important;
    padding: 16px !important;
    margin: 12px 0 !important;
    color: #1565c0 !important;
    font-weight: 500 !important;
    box-shadow: 0 2px 8px rgba(33, 150, 243, 0.1) !important;
}
.metrics-box h2 {
    color: #0d47a1 !important;
    font-size: 1.1em !important;
    margin-bottom: 8px !important;
    font-weight: 600 !important;
}
.metrics-box p {
    color: #1565c0 !important;
    margin: 4px 0 !important;
    font-size: 0.95em !important;
    line-height: 1.4 !important;
}
.status-box {
    background: linear-gradient(135deg, #1a237e 0%, #283593 100%) !important;
    border: 2px solid #3f51b5 !important;
    border-radius: 12px !important;
    padding: 20px !important;
    margin: 12px 0 !important;
    color: #ffffff !important;
    font-weight: 500 !important;
    box-shadow: 0 4px 12px rgba(63, 81, 181, 0.3) !important;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif !important;
}
.status-box h2, .status-box h3 {
    color: #e3f2fd !important;
    margin-bottom: 8px !important;
    text-shadow: 0 1px 2px rgba(0,0,0,0.3) !important;
}
.status-box strong {
    color: #e8eaf6 !important;
    font-weight: 600 !important;
}
.status-box code {
    background: rgba(255, 255, 255, 0.15) !important;
    padding: 2px 8px !important;
    border-radius: 4px !important;
    font-family: 'Courier New', monospace !important;
    color: #ffffff !important;
    border: 1px solid rgba(255, 255, 255, 0.2) !important;
}
.status-box p {
    color: #ffffff !important;
    margin: 6px 0 !important;
}
.status-box ul li {
    color: #ffffff !important;
    margin: 4px 0 !important;
}
footer {
    display: none !important;
}
"""

# ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ)
print("ğŸš€ GIST Rules Analyzer (Prebuilt Database Version) ì‹œì‘!")
if not load_existing_database():
    print("âŒ ì•±ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
    exit(1)

with gr.Blocks(
    title="GIST Rules Analyzer - Prebuilt DB", css=css, fill_height=True
) as demo:
    # ì¹´í…Œê³ ë¦¬ ì¸ì‹ ê¸°ëŠ¥ ìƒíƒœì— ë”°ë¥¸ ì œëª© ì„¤ì •
    category_status = (
        "ğŸ¯ ì¹´í…Œê³ ë¦¬ ì¸ì‹"
        if shared_state.get("category_aware", False)
        else "ğŸ” ê¸°ë³¸ ëª¨ë“œ"
    )
    gr.Markdown(
        f"<center><h1>ğŸ“š GIST Rules Analyzer (Dynamic Knee)</h1><p><strong>{category_status}</strong> | Knee Point Detection + ì¹´í…Œê³ ë¦¬ë³„ ìš°ì„ ìˆœìœ„ | ì‹¤ì‹œê°„ ì„±ëŠ¥ ë¹„êµ</p></center>"
    )

    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í‘œì‹œ
    with gr.Row():
        database_status = gr.Markdown(
            value=get_database_status(), elem_classes=["status-box"]
        )

    with gr.Row():
        with gr.Column(scale=2):
            # ê³µí†µ ì»¨íŠ¸ë¡¤
            with gr.Row():
                model_choices = list(MODELS.keys())
                default_model = "fireworks_ai/accounts/fireworks/models/gpt-oss-20b"
                default_value = (
                    default_model
                    if default_model in MODELS
                    else (model_choices[0] if model_choices else "")
                )
                with shared_state_lock:
                    shared_state["current_model"] = default_value

                model_dropdown = gr.Dropdown(
                    model_choices,
                    label="ğŸ§  LLM ëª¨ë¸ ì„ íƒ",
                    value=default_value,
                    scale=2,
                    allow_custom_value=True,
                )
                model_status = gr.Textbox(
                    label="ëª¨ë¸ ìƒíƒœ",
                    value="âœ… GPT-4 ì¤€ë¹„ì™„ë£Œ",
                    interactive=False,
                    scale=1,
                )

            additional_pdf_upload = gr.Files(
                label="ğŸ“„ ì¶”ê°€ ë¬¸ì„œ ì—…ë¡œë“œ (Vector Store í™•ì¥)", file_types=[".pdf"]
            )

            user_input = gr.Textbox(
                label="ğŸ” ì§ˆì˜ë¬¸ ì…ë ¥ (Query Input)",
                placeholder="ì˜ˆ: êµìˆ˜ë‹˜ì´ ë°•ì‚¬ê³¼ì • í•™ìƒì„ ì§€ë„í•  ìˆ˜ ìˆëŠ” ê¸°ê°„ì€ ì–¸ì œê¹Œì§€ì¸ê°€ìš”?",
                info="ğŸ¯ Knee Point Detectionìœ¼ë¡œ ê´€ë ¨ì„± ìˆëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤",
                lines=3,
                interactive=True,
            )

        with gr.Column(scale=1):
            submit_btn = gr.Button("ğŸš€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", variant="primary", size="lg")
            reset_btn = gr.Button("ğŸ”„ ì´ˆê¸°í™”", size="lg")

    # 4ê°œì˜ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ (2x2 ê·¸ë¦¬ë“œ)
    with gr.Row(elem_classes=["fill-height"]):
        with gr.Column(scale=1, elem_classes=["fill-height"]):
            with gr.Group(elem_classes=["extend-height"]):
                gr.Markdown("### ğŸ¯ Dynamic Knee Detection")
                with gr.Row():
                    gr.Dropdown(
                        ["Dynamic Document Selection"],
                        value="Dynamic Document Selection",
                        interactive=False,
                        scale=3,
                        show_label=False,
                    )
                chatbot_none = gr.Chatbot(
                    elem_classes=["extend-height"],
                    show_copy_button=True,
                    type="messages",
                )
                metrics_none = gr.Markdown(
                    "**Dynamic Knee** - ì¸¡ì • ì¤‘...", elem_classes=["metrics-box"]
                )
                copy_btn_none = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")

            with gr.Group(elem_classes=["extend-height"]):
                gr.Markdown("### ğŸ¯ Dynamic + Cross-Encoder (Basic)")
                with gr.Row():
                    gr.Dropdown(
                        ["Knee + ms-marco-MiniLM-L-6-v2"],
                        value="Knee + ms-marco-MiniLM-L-6-v2",
                        interactive=False,
                        scale=3,
                        show_label=False,
                    )
                chatbot_basic = gr.Chatbot(
                    elem_classes=["extend-height"],
                    show_copy_button=True,
                    type="messages",
                )
                metrics_basic = gr.Markdown(
                    "**Cross-Encoder (ê¸°ë³¸)** - ì¸¡ì • ì¤‘...",
                    elem_classes=["metrics-box"],
                )
                copy_btn_basic = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")

        with gr.Column(scale=1, elem_classes=["fill-height"]):
            with gr.Group(elem_classes=["extend-height"]):
                gr.Markdown("### ğŸš€ Dynamic + Cross-Encoder (Advanced)")
                with gr.Row():
                    gr.Dropdown(
                        ["Knee + ms-marco-MiniLM-L-12-v2"],
                        value="Knee + ms-marco-MiniLM-L-12-v2",
                        interactive=False,
                        scale=3,
                        show_label=False,
                    )
                chatbot_advanced = gr.Chatbot(
                    elem_classes=["extend-height"],
                    show_copy_button=True,
                    type="messages",
                )
                metrics_advanced = gr.Markdown(
                    "**Cross-Encoder (ê³ ì„±ëŠ¥)** - ì¸¡ì • ì¤‘...",
                    elem_classes=["metrics-box"],
                )
                copy_btn_advanced = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")

            with gr.Group(elem_classes=["extend-height"]):
                gr.Markdown("### ğŸŒ Dynamic + Multilingual Cross-Encoder")
                with gr.Row():
                    gr.Dropdown(
                        ["Knee + mmarco-mMiniLMv2-L12-H384-v1"],
                        value="Knee + mmarco-mMiniLMv2-L12-H384-v1",
                        interactive=False,
                        scale=3,
                        show_label=False,
                    )
                chatbot_multilingual = gr.Chatbot(
                    elem_classes=["extend-height"],
                    show_copy_button=True,
                    type="messages",
                )
                metrics_multilingual = gr.Markdown(
                    "**ë‹¤êµ­ì–´ Cross-Encoder** - ì¸¡ì • ì¤‘...",
                    elem_classes=["metrics-box"],
                )
                copy_btn_multilingual = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")

    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ (Generatorë¡œ ìˆ˜ì •)
    def init_client_on_first_query(user_query, request: gr.Request):
        session_id = get_session_id(request)
        with session_lock:
            if session_id not in sessions:
                init_session(session_id)
            if sessions[session_id]["client"] is None:
                sessions[session_id]["client"] = get_client(
                    shared_state["current_model"]
                )

        # Generatorë¥¼ ì œëŒ€ë¡œ yield
        for result in handle_multi_query(user_query, request):
            yield result

    # ëª¨ë¸ ë³€ê²½ ì´ë²¤íŠ¸
    model_dropdown.change(
        fn=change_model, inputs=[model_dropdown], outputs=[model_status]
    )

    # ì¶”ê°€ PDF ì—…ë¡œë“œ
    additional_pdf_upload.upload(
        fn=handle_additional_pdf_upload,
        inputs=[additional_pdf_upload],
        outputs=[database_status],
    )

    # ë©€í‹° ì¿¼ë¦¬ ì²˜ë¦¬
    submit_btn.click(
        fn=init_client_on_first_query,
        inputs=[user_input],
        outputs=[
            chatbot_none,
            metrics_none,
            chatbot_basic,
            metrics_basic,
            chatbot_advanced,
            metrics_advanced,
            chatbot_multilingual,
            metrics_multilingual,
        ],
    )

    user_input.submit(
        fn=init_client_on_first_query,
        inputs=[user_input],
        outputs=[
            chatbot_none,
            metrics_none,
            chatbot_basic,
            metrics_basic,
            chatbot_advanced,
            metrics_advanced,
            chatbot_multilingual,
            metrics_multilingual,
        ],
    )

    # ì´ˆê¸°í™”
    reset_btn.click(
        fn=reset_all_chats,
        outputs=[
            chatbot_none,
            metrics_none,
            chatbot_basic,
            metrics_basic,
            chatbot_advanced,
            metrics_advanced,
            chatbot_multilingual,
            metrics_multilingual,
        ],
    )

    # ë³µì‚¬ ê¸°ëŠ¥
    copy_btn_none.click(
        fn=lambda h: copy_as_markdown(h, "Dynamic Knee Detection"),
        inputs=[chatbot_none],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_basic.click(
        fn=lambda h: copy_as_markdown(h, "Dynamic + Cross-Encoder (ê¸°ë³¸)"),
        inputs=[chatbot_basic],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_advanced.click(
        fn=lambda h: copy_as_markdown(h, "Dynamic + Cross-Encoder (ê³ ì„±ëŠ¥)"),
        inputs=[chatbot_advanced],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_multilingual.click(
        fn=lambda h: copy_as_markdown(h, "Dynamic + ë‹¤êµ­ì–´ Cross-Encoder"),
        inputs=[chatbot_multilingual],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

if __name__ == "__main__":
    print("ğŸ‰ GIST Rules Analyzer (Dynamic Knee Detection) ì¤€ë¹„ì™„ë£Œ!")
    print("ğŸ¯ Knee Point Detectionìœ¼ë¡œ ìµœì  ë¬¸ì„œ ê°œìˆ˜ ìë™ ê²°ì •")
    print("ğŸŒ http://localhost:7860 ì—ì„œ ì‹¤í–‰ ì¤‘...")
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

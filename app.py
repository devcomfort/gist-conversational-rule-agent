"""
LiberVance RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ PDF ë¬¸ì„œë“¤ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ìƒ‰ì¸í™”í•˜ê³ ,
ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•´
ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- PDF ë¬¸ì„œ ìë™ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
- FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ í†µí•œ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
- ë‹¤ì–‘í•œ LLM ëª¨ë¸ ì§€ì› (GPT-4, DeepSeek-R1, Gemma-3, LLaMA-3.3, QwQ-32B)
- ì‹¤ì‹œê°„ ëª¨ë¸ ì „í™˜ ê¸°ëŠ¥
- ì„¸ì…˜ë³„ ë…ë¦½ì ì¸ ë¬¸ì„œ ê´€ë¦¬
- ëŒ€í™” ê¸°ë¡ ì €ì¥ ë° ê´€ë¦¬

ê¸°ìˆ  ìŠ¤íƒ:
- ì„ë² ë”©: sentence-transformers/all-MiniLM-L6-v2
- ë²¡í„° ìŠ¤í† ì–´: FAISS
- í…ìŠ¤íŠ¸ ë¶„í• : RecursiveCharacterTextSplitter
- PDF ì²˜ë¦¬: PyMuPDF (fitz)
"""

import gradio as gr
import os
import json
import time
import hashlib
import html
import threading
import fitz
import openai
from collections import OrderedDict
from datetime import datetime
from dotenv import load_dotenv
from huggingface_hub import InferenceClient
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# Environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HF_API_KEY = os.getenv("HUGGING_FACE_API_KEY")
HF_ENTERPRISE = os.getenv("HUGGING_FACE_ENTERPRISE")

# Session variables
MAX_SESSIONS = 100
sessions = OrderedDict()
session_lock = threading.Lock()

# Model setup
MODELS = {
    "GPT-4": {"model_id": "gpt-4", "provider": "openai"},
    "DeepSeek-R1": {"model_id": "deepseek-ai/DeepSeek-R1", "provider": "novita"},
    "Gemma-3-27B": {"model_id": "google/gemma-3-27b-it", "provider": "hf-inference"},
    "Llama-3.3-70B": {
        "model_id": "meta-llama/Llama-3.3-70B-Instruct",
        "provider": "hf-inference",
    },
    "QwQ-32B": {"model_id": "Qwen/QwQ-32B", "provider": "hf-inference"},
}

# Rerank ì„¤ì •
RERANK_OPTIONS = {
    "ì—†ìŒ": {"enabled": False, "model": None, "top_k": 3},
    "Cross-Encoder (ê¸°ë³¸)": {
        "enabled": True,
        "model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "top_k": 3,
    },
    "Cross-Encoder (ê³ ì„±ëŠ¥)": {
        "enabled": True,
        "model": "cross-encoder/ms-marco-MiniLM-L-12-v2",
        "top_k": 3,
    },
    "ë‹¤êµ­ì–´ Cross-Encoder": {
        "enabled": True,
        "model": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
        "top_k": 3,
    },
}

EMBED_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
DIMENSION = 384
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

system_prompt = "You are a helpful assistant that can answer questions based on the context when provided."


# --------- (A) SESSION SETUP ---------
def get_session_id(request: gr.Request):
    raw_id = request.client.host + str(request.headers.get("user-agent"))
    return hashlib.sha256(raw_id.encode()).hexdigest()


def init_session(session_id: str, model_name="GPT-4", rerank_method="ì—†ìŒ"):
    if len(sessions) >= MAX_SESSIONS:
        evicted_id, _ = sessions.popitem(last=False)
        print(f"ğŸ§¹ Removed LRU session: {evicted_id[:8]}...")
    model_info = MODELS[model_name]
    sessions[session_id] = {
        "history": [{"role": "system", "content": system_prompt}],
        "vectorstore": None,
        "retriever": None,
        "pdfs": [],
        "model_id": model_info["model_id"],
        "client": create_client(model_info["provider"]),
        "rerank_method": rerank_method,
    }


# --------- (B) DOCUMENT TEXT EXTRACTION ---------
def extract_text_from_pdf(pdf):
    doc = fitz.open(pdf)
    return "\n".join([page.get_text("text") for page in doc])


def create_vectorstore_from_pdfs(pdfs):
    all_docs = []
    for pdf in pdfs:
        text = extract_text_from_pdf(pdf)
        doc = Document(page_content=text, metadata={"source": pdf})
        docs = TEXT_SPLITTER.split_documents([doc])
        all_docs.extend(docs)
    return FAISS.from_documents(all_docs, EMBED_MODEL)


def create_retriever(vectorstore, rerank_method="ì—†ìŒ"):
    """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ retriever ìƒì„± - rerank ì˜µì…˜ ì§€ì›"""
    if not vectorstore:
        return None

    rerank_config = RERANK_OPTIONS.get(rerank_method, RERANK_OPTIONS["ì—†ìŒ"])

    if not rerank_config["enabled"]:
        # ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©
        print("ğŸ” Using basic vector retrieval (no rerank)")
        return vectorstore.as_retriever(search_kwargs={"k": 3})

    try:
        # Rerank ê¸°ëŠ¥ í™œì„±í™”
        print(f"ğŸ”„ Setting up reranker: {rerank_method}")
        print(f"   Model: {rerank_config['model']}")

        # ë” ë§ì€ ë¬¸ì„œë¥¼ ì´ˆê¸° ê²€ìƒ‰ (rerankë¥¼ ìœ„í•´)
        top_k = rerank_config.get("top_k", 3)
        if not isinstance(top_k, int) or top_k <= 0:
            top_k = 3
        initial_k = max(10, top_k * 3)  # ìµœì¢… ê²°ê³¼ì˜ 3ë°° ì •ë„ ê²€ìƒ‰
        base_retriever = vectorstore.as_retriever(search_kwargs={"k": initial_k})

        # Cross-encoder ëª¨ë¸ ì„¤ì •
        cross_encoder = HuggingFaceCrossEncoder(model_name=rerank_config["model"])
        compressor = CrossEncoderReranker(model=cross_encoder)

        # ContextualCompressionRetrieverë¡œ ë˜í•‘
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor, base_retriever=base_retriever
        )

        # Top-K ì œí•œì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
        class TopKLimitedRetriever:
            def __init__(self, retriever, top_k):
                self.retriever = retriever
                self.top_k = top_k

            def invoke(self, query, **kwargs):
                docs = self.retriever.invoke(query, **kwargs)
                return docs[: self.top_k]

        limited_retriever = TopKLimitedRetriever(compression_retriever, top_k)

        print(f"âœ… Reranker setup complete - will return top {top_k} documents")
        return limited_retriever

    except Exception as e:
        print(f"âŒ Reranker setup failed: {e}")
        print("   Falling back to basic vector retrieval")
        return vectorstore.as_retriever(search_kwargs={"k": 3})


def handle_pdf_upload(pdfs, request: gr.Request):
    start_time = time.time()  # â±ï¸ TIMER
    session_id = get_session_id(request)
    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        # Generate vectorstore from PDFs
        print("Processing PDF(s)...")
        vectorstore = create_vectorstore_from_pdfs(pdfs) if pdfs else None
        sessions[session_id]["vectorstore"] = vectorstore

        # Create retriever with current rerank method
        rerank_method = sessions[session_id].get("rerank_method", "ì—†ìŒ")
        sessions[session_id]["retriever"] = create_retriever(vectorstore, rerank_method)
        sessions[session_id]["pdfs"] = pdfs
        sessions.move_to_end(session_id)
    end_time = time.time()  # â±ï¸ TIMER
    elapsed_time = end_time - start_time  # â±ï¸ TIMER
    print(f"Processed {len(pdfs)} PDFs in {elapsed_time:.2f} seconds")  # â±ï¸ TIMER


# --------- (C) PRIMARY CHAT FUNCTION ---------
def handle_query(user_query, request: gr.Request):
    session_id = get_session_id(request)
    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        session = sessions[session_id]
        sessions.move_to_end(session_id)

    history = session["history"]
    messages = history.copy()
    client = session["client"]
    model_info = None
    for name, info in MODELS.items():
        if info["model_id"] == session["model_id"]:
            model_info = info
            break

    start_time = time.time()  # â±ï¸ TIMER

    # Extract relevant text data from PDFs
    context = ""
    retriever = session["retriever"]
    if retriever:
        rerank_method = session.get("rerank_method", "ì—†ìŒ")
        print(f"Retrieving relevant data using: {rerank_method}")

        retrieval_start = time.time()
        docs = retriever.invoke(user_query)
        retrieval_end = time.time()

        context = "\n".join([doc.page_content for doc in docs])
        print(
            f"ğŸ“Š Retrieved {len(docs)} documents in {retrieval_end - retrieval_start:.2f}s"
        )
    messages.append(
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_query}"}
    )

    # Add user message to history first
    history.append({"role": "user", "content": user_query})

    # Create initial assistant message placeholder
    history.append({"role": "assistant", "content": ""})

    # Yield initial state with user query
    yield history

    # Invoke client with user query using streaming
    print("Inquiring LLM with streaming...")

    try:
        if model_info and model_info["provider"] == "openai":
            # OpenAI ìŠ¤íŠ¸ë¦¬ë°
            completion = client.chat.completions.create(
                model=session["model_id"], messages=messages, stream=True
            )

            bot_response = ""
            for chunk in completion:
                if chunk.choices[0].delta.content:
                    chunk_content = chunk.choices[0].delta.content
                    bot_response += chunk_content
                    # Update the last message (assistant's response)
                    history[-1]["content"] = html.escape(bot_response)
                    yield history

        else:
            # HuggingFace Inference Client ìŠ¤íŠ¸ë¦¬ë°
            completion = client.chat.completions.create(
                model=session["model_id"], messages=messages, stream=True
            )

            bot_response = ""
            for chunk in completion:
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and chunk.choices[0].delta.content
                ):
                    chunk_content = chunk.choices[0].delta.content
                    bot_response += chunk_content
                    # Update the last message (assistant's response)
                    history[-1]["content"] = html.escape(bot_response)
                    yield history
                elif hasattr(chunk, "token"):
                    # HuggingFace ìŠ¤íŠ¸ë¦¬ë°ì˜ ê²½ìš° token í•„ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
                    bot_response += (
                        chunk.token.text
                        if hasattr(chunk.token, "text")
                        else str(chunk.token)
                    )
                    history[-1]["content"] = html.escape(bot_response)
                    yield history

    except Exception as e:
        print(f"âŒ Streaming error: {e}")
        print("Falling back to non-streaming mode...")

        # Fallback to non-streaming
        try:
            completion = client.chat.completions.create(
                model=session["model_id"],
                messages=messages,
            )
            bot_response = completion.choices[0].message.content
            history[-1]["content"] = html.escape(bot_response)
            yield history
        except Exception as fallback_error:
            print(f"âŒ Fallback error: {fallback_error}")
            error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            history[-1]["content"] = error_message
            yield history
            return

    # Save final history
    save_history(history, session_id)

    end_time = time.time()  # â±ï¸ TIMER
    elapsed_time = end_time - start_time  # â±ï¸ TIMER
    print(f"Responded to user query in {elapsed_time:.2f} seconds")  # â±ï¸ TIMER


# --------- (D) ADDITIONAL FUNCTIONS ---------
def create_client(provider):
    """ëª¨ë¸ ì •ë³´ì— ë”°ë¼ InferenceClient ê°ì²´ ìƒì„±"""
    if provider == "openai":
        return openai.Client(api_key=OPENAI_API_KEY)

    headers = {}
    if HF_ENTERPRISE:
        headers["X-HF-Bill-To"] = HF_ENTERPRISE

    return InferenceClient(
        provider=provider,
        api_key=HF_API_KEY,
        headers=headers if headers else None,
    )


def change_model(model_name, request: gr.Request):
    """ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ ëª¨ë¸ ë³€ê²½"""
    session_id = get_session_id(request)
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        model_info = MODELS[model_name]
        sessions[session_id]["model_id"] = model_info["model_id"]
        sessions[session_id]["client"] = create_client(model_info["provider"])
        sessions.move_to_end(session_id)
    # print(f"ğŸ”„ Now using: {model_name}")


def change_rerank_method(rerank_method, request: gr.Request):
    """ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ rerank ë°©ë²• ë³€ê²½"""
    session_id = get_session_id(request)
    with session_lock:
        if session_id not in sessions:
            init_session(session_id, rerank_method=rerank_method)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        else:
            # ê¸°ì¡´ ì„¸ì…˜ì—ì„œ rerank ë°©ë²• ë³€ê²½
            sessions[session_id]["rerank_method"] = rerank_method

            # ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ retriever ì¬ìƒì„±
            vectorstore = sessions[session_id]["vectorstore"]
            if vectorstore:
                print(f"ğŸ”„ Updating retriever with: {rerank_method}")
                sessions[session_id]["retriever"] = create_retriever(
                    vectorstore, rerank_method
                )

            sessions.move_to_end(session_id)


def save_history(history, session_id):
    """ëŒ€í™” ê¸°ë¡(history)ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    folder = "./chat_logs_lvrag"
    os.makedirs(folder, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d")
    filename = os.path.join(folder, f"{timestamp}_{session_id}.json")
    counter = 1
    while os.path.exists(filename):
        filename = os.path.join(folder, f"{timestamp}_{session_id}_{counter}.json")
        counter += 1
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def reset_session(request: gr.Request):
    """ëŒ€í™” ë° íŒŒì¼ ì—…ë¡œë“œ ë‚´ì—­ ì‚­ì œ"""
    session_id = get_session_id(request)
    with session_lock:
        current_rerank_method = "ì—†ìŒ"
        if session_id in sessions:
            current_rerank_method = sessions[session_id].get("rerank_method", "ì—†ìŒ")

        init_session(session_id, rerank_method=current_rerank_method)
        sessions.move_to_end(session_id)
        print(f"â™»ï¸ Session {session_id[:8]}... reset.")
    return "", []


# --------- (E) Gradio UI ---------
css = """
div {
    flex-wrap: nowrap !important;
}
.responsive-height {
    height: 768px !important;
    padding-bottom: 64px !important;
}
.fill-height {
    height: 100% !important;
    flex-wrap: nowrap !important;
}
.extend-height {
    min-height: 260px !important;
    flex: 1 !important;
    overflow: auto !important;
}
footer {
    display: none !important;
}
"""

with gr.Blocks(title="LiberVance RAG", css=css, fill_height=True) as demo:
    gr.Markdown("<center><h1>ğŸ“„ LiberVance RAG</h1></center>")
    with gr.Row(elem_classes=["responsive-height"]):
        # Output column
        with gr.Column(elem_classes=["fill-height"]):
            chatbot = gr.Chatbot(
                label="Chatbot", type="messages", elem_classes=["extend-height"]
            )
        # Input column
        with gr.Column(elem_classes=["fill-height"]):
            model_dropdown = gr.Dropdown(
                list(MODELS.keys()), label="Select Model", value="GPT-4"
            )
            rerank_dropdown = gr.Dropdown(
                list(RERANK_OPTIONS.keys()),
                label="Rerank Method",
                value="ì—†ìŒ",
                info="Cross-Encoder ëª¨ë¸ì„ ì‚¬ìš©í•´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•©ë‹ˆë‹¤",
            )
            pdf_upload = gr.Files(
                label="Upload file(s) (PDF only)",
                file_types=[".pdf"],
                elem_classes=["extend-height"],
            )
            user_input = gr.Textbox(
                label="Enter your query here",
                placeholder="e.g., Summarize the key points from this document.",
                lines=3,
            )
            with gr.Row():
                submit_btn = gr.Button("Submit", variant="primary")
                reset_btn = gr.Button("Reset", variant="secondary")
    # Event listeners
    pdf_upload.change(fn=handle_pdf_upload, inputs=[pdf_upload], outputs=[])
    model_dropdown.input(fn=change_model, inputs=[model_dropdown], outputs=[])
    rerank_dropdown.input(fn=change_rerank_method, inputs=[rerank_dropdown], outputs=[])
    user_input.submit(handle_query, inputs=[user_input], outputs=[chatbot])
    submit_btn.click(handle_query, inputs=[user_input], outputs=[chatbot])
    reset_btn.click(reset_session, inputs=[], outputs=[user_input, chatbot])


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - CLI entry point"""
    demo.launch(share=True, favicon_path="")


def main_dev():
    """ê°œë°œìš© ì‹¤í–‰ í•¨ìˆ˜ - ë¡œì»¬ ì„œë²„ë§Œ"""
    demo.launch(share=False, server_name="localhost", server_port=7860)


def main_prod():
    """í”„ë¡œë•ì…˜ ì‹¤í–‰ í•¨ìˆ˜ - ì™¸ë¶€ ì ‘ê·¼ ê°€ëŠ¥"""
    import os

    port = int(os.getenv("PORT", 7860))
    demo.launch(share=False, server_name="0.0.0.0", server_port=port)


if __name__ == "__main__":
    main()

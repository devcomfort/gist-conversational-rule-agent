"""
LiberVance RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ PDF ë¬¸ì„œë“¤ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ìƒ‰ì¸í™”í•˜ê³ ,
ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•´
ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- PDF ë¬¸ì„œ ìë™ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
- FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ í†µí•œ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
- ë‹¤ì–‘í•œ LLM ëª¨ë¸ ì§€ì› (GPT-4, DeepSeek-R1, Gemma-3, LLaMA-3.3, QwQ-32B)
- ì‹¤ì‹œê°„ ëª¨ë¸ ì „í™˜ ê¸°ëŠ¥
- ì„¸ì…˜ë³„ ë…ë¦½ì ì¸ ë¬¸ì„œ ê´€ë¦¬
- ëŒ€í™” ê¸°ë¡ ì €ì¥ ë° ê´€ë¦¬

ê¸°ìˆ  ìŠ¤íƒ:
- ì„ë² ë”©: sentence-transformers/all-MiniLM-L6-v2
- ë²¡í„° ìŠ¤í† ì–´: FAISS
- í…ìŠ¤íŠ¸ ë¶„í• : RecursiveCharacterTextSplitter
- PDF ì²˜ë¦¬: PyMuPDF (fitz)
"""

import gradio as gr
import os
import json
import time
import hashlib
import html
import threading
import fitz
import litellm
import numpy as np
from numpy.typing import NDArray
from collections import OrderedDict
from datetime import datetime
from dotenv import load_dotenv
from litellm import get_valid_models
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

try:
    from kneed import KneeLocator
except Exception:
    KneeLocator = None

# Environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NOVITA_API_KEY = os.getenv("NOVITA_API_KEY")
FIREWORKS_API_KEY = os.getenv("FIREWORKS_AI_API_KEY") or os.getenv("FIREWORKS_API_KEY")

# Session variables
MAX_SESSIONS = 100
sessions = OrderedDict()
session_lock = threading.Lock()


# Model setup (dynamic using LiteLLM get_valid_models)
def _detect_provider(model_id: str) -> str:
    if model_id.startswith("novita/"):
        return "novita"
    if model_id.startswith("fireworks_ai/"):
        return "fireworks"
    return "openai"


def _load_dynamic_models() -> dict:
    # Map FIREWORKS_API_KEY -> FIREWORKS_AI_API_KEY if only legacy var is set
    if os.getenv("FIREWORKS_API_KEY") and not os.getenv("FIREWORKS_AI_API_KEY"):
        os.environ["FIREWORKS_AI_API_KEY"] = os.getenv("FIREWORKS_API_KEY") or ""

    try:
        model_ids = get_valid_models(check_provider_endpoint=True)
    except Exception:
        model_ids = []

    dynamic: dict = {}
    for mid in model_ids:
        dynamic[mid] = {"model_id": mid, "provider": _detect_provider(mid)}

    # Ensure Fireworks default appears when key present
    default_fw = "fireworks_ai/accounts/fireworks/models/gpt-oss-20b"
    if (
        os.getenv("FIREWORKS_AI_API_KEY") or os.getenv("FIREWORKS_API_KEY")
    ) and default_fw not in dynamic:
        dynamic[default_fw] = {"model_id": default_fw, "provider": "fireworks"}

    # Ensure at least one OpenAI option when OPENAI_API_KEY is set
    if OPENAI_API_KEY and not any(
        p.get("provider") == "openai" for p in dynamic.values()
    ):
        dynamic.setdefault(
            "gpt-4o-mini", {"model_id": "gpt-4o-mini", "provider": "openai"}
        )

    return dynamic


MODELS = _load_dynamic_models()
DEFAULT_MODEL = (
    "fireworks_ai/accounts/fireworks/models/gpt-oss-20b"
    if "fireworks_ai/accounts/fireworks/models/gpt-oss-20b" in MODELS
    else (list(MODELS.keys())[0] if MODELS else "")
)

# Retrieval ëª¨ë“œ ì„¤ì •
RETRIEVAL_MODES = {
    "ì ì‘í˜• (Knee Detection)": {"use_knee": True, "default_k": 30},
    "ê³ ì • Top-K": {"use_knee": False, "default_k": 30},
}

# Rerank ì„¤ì •
RERANK_OPTIONS = {
    "ì—†ìŒ": {"enabled": False, "model": None, "top_k": 30},
    "Cross-Encoder (ê¸°ë³¸)": {
        "enabled": True,
        "model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "top_k": 30,
    },
    "Cross-Encoder (ê³ ì„±ëŠ¥)": {
        "enabled": True,
        "model": "cross-encoder/ms-marco-MiniLM-L-12-v2",
        "top_k": 30,
    },
    "ë‹¤êµ­ì–´ Cross-Encoder": {
        "enabled": True,
        "model": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
        "top_k": 30,
    },
}

EMBED_MODEL_ID = "sentence-transformers/all-MiniLM-L6-v2"
EMBED_MODEL = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_ID,
    encode_kwargs={"normalize_embeddings": True},
)
DIMENSION = 384
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

system_prompt = "You are a helpful assistant that can answer questions based on the context when provided."


def _compute_knee_cutoff(sorted_scores, min_k: int = 1, default_k: int = 30) -> int:
    """ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ëœ ì ìˆ˜ ì‹œí€€ìŠ¤ì—ì„œ ë¬´ë¦ì ì„ ì°¾ì•„ ë°˜í™˜ ê°œìˆ˜ë¥¼ ê²°ì •.

    - sorted_scores: ë†’ì€ ì ìˆ˜ â†’ ë‚®ì€ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸
    - min_k: ìµœì†Œ ë°˜í™˜ ê°œìˆ˜ ë³´ì¥
    - default_k: ë¬´ë¦ì ì„ ì°¾ì§€ ëª»í–ˆì„ ë•Œ ê¸°ë³¸ ë°˜í™˜ ê°œìˆ˜
    """
    try:
        n = len(sorted_scores)
        if n <= 0:
            return 0
        if n <= min_k:
            return n

        if KneeLocator is not None:
            try:
                x = list(range(1, n + 1))
                kl = KneeLocator(
                    x, sorted_scores, curve="convex", direction="decreasing"
                )
                if kl.knee is not None:
                    return max(min_k, min(int(kl.knee), n))
            except Exception:
                pass

        # ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±: ì´ì „ ëŒ€ë¹„ ìƒëŒ€ í•˜ë½í­ì´ ì„ê³„ì¹˜ ì´ìƒì¸ ì²« ì§€ì ì—ì„œ ì ˆë‹¨
        threshold = 0.15
        for i in range(1, n):
            prev_score = sorted_scores[i - 1]
            curr_score = sorted_scores[i]
            if prev_score == 0:
                continue
            drop_ratio = (prev_score - curr_score) / (abs(prev_score) + 1e-9)
            if drop_ratio >= threshold:
                return max(min_k, i)

        return max(min_k, min(default_k, n))
    except Exception:
        return max(min_k, default_k)


def _get_total_docs_in_vectorstore(vectorstore) -> int:
    """Vectorstore ë‚´ ì´ ë¬¸ì„œ ìˆ˜ ì¶”ì • (FAISS ìš°ì„ , ê·¸ ì™¸ ì•ˆì „í•œ í´ë°±).

    - FAISS: index.ntotal ì‚¬ìš©
    - ê¸°íƒ€: index_to_docstore_id ê¸¸ì´, docstore._dict ê¸¸ì´ ìˆœì„œë¡œ ì‹œë„
    """
    try:
        index = getattr(vectorstore, "index", None)
        if index is not None:
            ntotal = getattr(index, "ntotal", None)
            if isinstance(ntotal, (int, float)) and int(ntotal) > 0:
                return int(ntotal)
    except Exception:
        pass
    try:
        ids = getattr(vectorstore, "index_to_docstore_id", None)
        if ids is not None:
            return len(ids)
    except Exception:
        pass
    try:
        docstore = getattr(vectorstore, "docstore", None)
        if docstore is not None:
            d = getattr(docstore, "_dict", None)
            if d is not None:
                return len(d)
    except Exception:
        pass
    return 0


# --------- (A) SESSION SETUP ---------
def get_session_id(request: gr.Request):
    raw_id = request.client.host + str(request.headers.get("user-agent"))
    return hashlib.sha256(raw_id.encode()).hexdigest()


def init_session(
    session_id: str,
    model_name=DEFAULT_MODEL,
    rerank_method="ì—†ìŒ",
    retrieval_mode="ì ì‘í˜• (Knee Detection)",
):
    if len(sessions) >= MAX_SESSIONS:
        evicted_id, _ = sessions.popitem(last=False)
        print(f"ğŸ§¹ Removed LRU session: {evicted_id[:8]}...")
    model_info = MODELS[model_name]
    sessions[session_id] = {
        "history": [{"role": "system", "content": system_prompt}],
        "vectorstore": None,
        "retriever": None,
        "pdfs": [],
        "model_id": model_info["model_id"],
        "client": create_client(model_info["provider"]),
        "rerank_method": rerank_method,
        "retrieval_mode": retrieval_mode,
    }


# --------- (B) DOCUMENT TEXT EXTRACTION ---------
def extract_text_from_pdf(pdf):
    doc = fitz.open(pdf)
    return "\n".join([page.get_text("text") for page in doc])


def create_vectorstore_from_pdfs(pdfs):
    all_docs = []
    for pdf in pdfs:
        text = extract_text_from_pdf(pdf)
        doc = Document(page_content=text, metadata={"source": pdf})
        docs = TEXT_SPLITTER.split_documents([doc])
        all_docs.extend(docs)
    vs = FAISS.from_documents(all_docs, EMBED_MODEL)
    # ë©”íƒ€: ë²¡í„°ìŠ¤í† ì–´ì— í˜„ì¬ ì„ë² ë”© ëª¨ë¸ ì‹ë³„ì ê¸°ë¡
    try:
        setattr(vs, "_embedding_model_id", EMBED_MODEL_ID)
    except Exception:
        pass
    return vs


def create_retriever(
    vectorstore, rerank_method="ì—†ìŒ", retrieval_mode="ì ì‘í˜• (Knee Detection)"
):
    """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ retriever ìƒì„± - rerank ì˜µì…˜ ì§€ì›"""
    if not vectorstore:
        return None

    rerank_config = RERANK_OPTIONS.get(rerank_method, RERANK_OPTIONS["ì—†ìŒ"])
    retrieval_config = RETRIEVAL_MODES.get(
        retrieval_mode, RETRIEVAL_MODES["ì ì‘í˜• (Knee Detection)"]
    )
    use_knee = retrieval_config["use_knee"]
    default_k = retrieval_config["default_k"]

    if not rerank_config["enabled"]:
        if use_knee:
            # ì ì‘í˜• ê²€ìƒ‰: kneedle ê¸°ë°˜ ì ì‘í˜• ì„ íƒ
            print("ğŸ” Using basic vector retrieval (no rerank) with kneedle cutoff")

            class AdaptiveKneeRetriever:
                def __init__(self, vectorstore, default_k: int = 30):
                    self.vectorstore = vectorstore
                    self.default_k = default_k

                def invoke(self, query, **kwargs):
                    try:
                        total = _get_total_docs_in_vectorstore(self.vectorstore)
                        k_all = max(total, 1000) if total <= 0 else total
                        results = self.vectorstore.similarity_search_with_score(
                            query, k=k_all
                        )
                    except Exception:
                        # ì¼ë¶€ ë°±ì—”ë“œê°€ _with_scoreë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ê²½ìš°ì˜ í´ë°±
                        total = _get_total_docs_in_vectorstore(self.vectorstore)
                        k_all = max(total, 1000) if total <= 0 else total
                        docs = self.vectorstore.similarity_search(query, k=k_all)
                        # ì ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ìœ¼ë¡œ ìë¥´ê³  rank ë©”íƒ€ë°ì´í„° ë¶€ì—¬
                        for idx, d in enumerate(docs[: self.default_k]):
                            d.metadata = dict(
                                d.metadata or {}, rank=idx + 1, selected=True
                            )
                        return docs[: self.default_k]

                    # results: List[Tuple[Document, distance]] (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
                    docs = [doc for doc, _ in results]
                    distances: NDArray[np.float64] = np.asarray(
                        [float(distance) for _, distance in results], dtype=np.float64
                    )
                    # ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ë³€í™˜ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
                    scores: NDArray[np.float64] = np.divide(1.0, distances + 1.0)
                    # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì¸ë±ìŠ¤
                    order: NDArray[np.int64] = np.argsort(-scores)
                    scores_sorted: NDArray[np.float64] = scores[order]
                    cutoff_k = _compute_knee_cutoff(
                        scores_sorted.tolist(), min_k=1, default_k=self.default_k
                    )
                    selected_idx: NDArray[np.int64] = order[:cutoff_k]
                    # ì„ íƒëœ ë¬¸ì„œì— rank/score/index ë©”íƒ€ë°ì´í„° ë¶€ì—¬
                    selected_docs = []
                    for rank, idx in enumerate(selected_idx, start=1):
                        d = docs[int(idx)]
                        d.metadata = dict(
                            d.metadata or {},
                            rank=rank,
                            score=float(scores[int(idx)]),
                            original_index=int(idx),
                            selected=True,
                        )
                        selected_docs.append(d)
                    return selected_docs

            return AdaptiveKneeRetriever(vectorstore, default_k=default_k)
        else:
            # ê³ ì • Top-K ê²€ìƒ‰
            print(f"ğŸ”¢ Using fixed top-{default_k} retrieval (no rerank)")
            return vectorstore.as_retriever(search_kwargs={"k": default_k})

    try:
        # Rerank ê¸°ëŠ¥ í™œì„±í™”
        print(f"ğŸ”„ Setting up reranker: {rerank_method}")
        print(f"   Model: {rerank_config['model']}")

        # ë” ë§ì€ ë¬¸ì„œë¥¼ ì´ˆê¸° ê²€ìƒ‰ (rerankë¥¼ ìœ„í•´)
        top_k = rerank_config.get("top_k", 30)
        if not isinstance(top_k, int) or top_k <= 0:
            top_k = 30
        # ì „ì²´ ë¬¸ì„œ ëŒ€ìƒìœ¼ë¡œ 1ì°¨ ê²€ìƒ‰
        total = _get_total_docs_in_vectorstore(vectorstore)
        initial_k = max(total, 1000) if total <= 0 else total
        base_retriever = vectorstore.as_retriever(search_kwargs={"k": initial_k})

        # Cross-encoder ëª¨ë¸ ì„¤ì •
        cross_encoder = HuggingFaceCrossEncoder(model_name=rerank_config["model"])
        compressor = CrossEncoderReranker(model=cross_encoder)

        # ContextualCompressionRetrieverë¡œ ë˜í•‘
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor, base_retriever=base_retriever
        )

        if use_knee:
            # Kneedle ê¸°ë°˜ ì ì‘í˜• ì œí•œì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
            class AdaptiveKneeLimitedRetriever:
                def __init__(self, retriever, cross_encoder, default_k):
                    self.retriever = retriever
                    self.cross_encoder = cross_encoder
                    self.default_k = default_k

                def _predict_scores(self, query, docs):
                    pairs = [(query, d.page_content) for d in docs]

                    def _to_score_list(result, num_docs: int):
                        try:
                            arr = np.asarray(result, dtype=float)
                            if arr.ndim == 0:
                                arr = np.full(num_docs, float(arr))
                            else:
                                arr = arr.reshape(-1)
                            if arr.size < num_docs:
                                pad_value = float(arr[-1]) if arr.size > 0 else 1.0
                                arr = np.pad(
                                    arr,
                                    (0, num_docs - arr.size),
                                    constant_values=pad_value,
                                )
                            elif arr.size > num_docs:
                                arr = arr[:num_docs]
                            return arr.tolist()
                        except Exception:
                            return [1.0 for _ in range(num_docs)]

                    # ë‹¤ì–‘í•œ êµ¬í˜„ ì°¨ì´ë¥¼ ê³ ë ¤í•œ ì•ˆì „í•œ ì˜ˆì¸¡ í˜¸ì¶œ
                    for attr in ("score", "predict"):
                        method = getattr(self.cross_encoder, attr, None)
                        if callable(method):
                            try:
                                return _to_score_list(method(pairs), len(docs))
                            except Exception:
                                pass
                    for attr in ("model", "_model"):
                        underlying = getattr(self.cross_encoder, attr, None)
                        if underlying is not None:
                            method = getattr(underlying, "predict", None)
                            if callable(method):
                                try:
                                    return _to_score_list(method(pairs), len(docs))
                                except Exception:
                                    pass
                    # ì ìˆ˜ ì‚°ì¶œ ë¶ˆê°€ ì‹œ ê· ë“± ì ìˆ˜ ë°˜í™˜ â†’ ê¸°ë³¸ kë¡œ ì ˆë‹¨ë¨
                    return [1.0 for _ in docs]

                def invoke(self, query, **kwargs):
                    docs = self.retriever.invoke(query, **kwargs)
                    if not docs:
                        return []
                    scores: NDArray[np.float64] = np.asarray(
                        self._predict_scores(query, docs), dtype=np.float64
                    )
                    order: NDArray[np.int64] = np.argsort(-scores)
                    scores_sorted: NDArray[np.float64] = scores[order]
                    cutoff_k = _compute_knee_cutoff(
                        scores_sorted.tolist(), min_k=1, default_k=self.default_k
                    )
                    selected_idx: NDArray[np.int64] = order[:cutoff_k]
                    selected_docs = []
                    for rank, idx in enumerate(selected_idx, start=1):
                        d = docs[int(idx)]
                        d.metadata = dict(
                            d.metadata or {},
                            rank=rank,
                            score=float(scores[int(idx)]),
                            original_index=int(idx),
                            selected=True,
                        )
                        selected_docs.append(d)
                    return selected_docs

            adaptive_retriever = AdaptiveKneeLimitedRetriever(
                compression_retriever, cross_encoder, top_k
            )
        else:
            # ê³ ì • Top-Kë¡œ ì œí•œëœ Compression Retriever
            class TopKLimitedRetriever:
                def __init__(self, retriever, default_k):
                    self.retriever = retriever
                    self.default_k = default_k

                def invoke(self, query, **kwargs):
                    docs = self.retriever.invoke(query, **kwargs)
                    return (
                        docs[: self.default_k] if len(docs) > self.default_k else docs
                    )

            adaptive_retriever = TopKLimitedRetriever(compression_retriever, top_k)

        print(f"âœ… Reranker setup complete - adaptive cutoff with default {top_k}")
        return adaptive_retriever

    except Exception as e:
        print(f"âŒ Reranker setup failed: {e}")
        if use_knee:
            print("   Falling back to basic vector retrieval (adaptive kneedle)")

            class AdaptiveKneeRetriever:
                def __init__(self, vectorstore, default_k: int = 30):
                    self.vectorstore = vectorstore
                    self.default_k = default_k

                def invoke(self, query, **kwargs):
                    try:
                        total = _get_total_docs_in_vectorstore(self.vectorstore)
                        k_all = max(total, 1000) if total <= 0 else total
                        results = self.vectorstore.similarity_search_with_score(
                            query, k=k_all
                        )
                    except Exception:
                        total = _get_total_docs_in_vectorstore(self.vectorstore)
                        k_all = max(total, 1000) if total <= 0 else total
                        docs = self.vectorstore.similarity_search(query, k=k_all)
                        return docs[: self.default_k]

                    docs = []
                    scores = []
                    for doc, distance in results:
                        docs.append(doc)
                        sim = 1.0 / (1.0 + float(distance))
                        scores.append(sim)

                    cutoff_k = _compute_knee_cutoff(
                        scores, min_k=1, default_k=self.default_k
                    )
                    return docs[:cutoff_k]

            return AdaptiveKneeRetriever(vectorstore, default_k=default_k)
        else:
            print(f"   Falling back to basic vector retrieval (fixed top-{default_k})")
            return vectorstore.as_retriever(search_kwargs={"k": default_k})


def handle_pdf_upload(pdfs, request: gr.Request):
    start_time = time.time()  # â±ï¸ TIMER
    session_id = get_session_id(request)
    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        # Generate vectorstore from PDFs
        print("Processing PDF(s)...")
        vectorstore = create_vectorstore_from_pdfs(pdfs) if pdfs else None
        sessions[session_id]["vectorstore"] = vectorstore

        # Create retriever with current rerank method and retrieval mode
        rerank_method = sessions[session_id].get("rerank_method", "ì—†ìŒ")
        retrieval_mode = sessions[session_id].get(
            "retrieval_mode", "ì ì‘í˜• (Knee Detection)"
        )
        sessions[session_id]["retriever"] = create_retriever(
            vectorstore, rerank_method, retrieval_mode
        )
        sessions[session_id]["pdfs"] = pdfs
        sessions.move_to_end(session_id)
    end_time = time.time()  # â±ï¸ TIMER
    elapsed_time = end_time - start_time  # â±ï¸ TIMER
    print(f"Processed {len(pdfs)} PDFs in {elapsed_time:.2f} seconds")  # â±ï¸ TIMER


# --------- (C) PRIMARY CHAT FUNCTION ---------
def handle_query(user_query, request: gr.Request):
    session_id = get_session_id(request)
    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        session = sessions[session_id]
        sessions.move_to_end(session_id)

    history = session["history"]
    messages = history.copy()
    # client = session["client"]  # not used
    model_info = None
    for name, info in MODELS.items():
        if info["model_id"] == session["model_id"]:
            model_info = info
            break

    start_time = time.time()  # â±ï¸ TIMER

    # Extract relevant text data from PDFs
    context = ""
    retriever = session["retriever"]
    # ë²¡í„°ìŠ¤í† ì–´-ì„ë² ë”© ëª¨ë¸ ë¶ˆì¼ì¹˜ ê°ì§€ ì‹œ ì¬êµ¬ì¶•
    vectorstore = session.get("vectorstore")
    if vectorstore is not None:
        vs_model_id = getattr(vectorstore, "_embedding_model_id", None)
        if vs_model_id != EMBED_MODEL_ID:
            print("âš ï¸ Rebuilding vectorstore due to embedding model change...")
            pdfs = session.get("pdfs", [])
            vectorstore = create_vectorstore_from_pdfs(pdfs) if pdfs else None
            session["vectorstore"] = vectorstore
            retriever = (
                create_retriever(
                    vectorstore,
                    session.get("rerank_method", "ì—†ìŒ"),
                    session.get("retrieval_mode", "ì ì‘í˜• (Knee Detection)"),
                )
                if vectorstore
                else None
            )
            session["retriever"] = retriever
    if retriever:
        rerank_method = session.get("rerank_method", "ì—†ìŒ")
        print(f"Retrieving relevant data using: {rerank_method}")

        retrieval_start = time.time()
        docs = retriever.invoke(user_query)
        retrieval_end = time.time()

        context = "\n".join([doc.page_content for doc in docs])
        print(
            f"ğŸ“Š Retrieved {len(docs)} documents in {retrieval_end - retrieval_start:.2f}s"
        )
    messages.append(
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_query}"}
    )

    # Add user message to history first
    history.append({"role": "user", "content": user_query})

    # Create initial assistant message placeholder
    history.append({"role": "assistant", "content": ""})

    # Yield initial state with user query
    yield history

    # Invoke client with user query using streaming
    print("Inquiring LLM with streaming...")

    # Resolve LiteLLM endpoint & credentials
    api_key = OPENAI_API_KEY
    base_url = None
    if model_info and model_info["provider"] == "novita":
        api_key = NOVITA_API_KEY
        base_url = "https://api.novita.ai/v3/openai"
    elif model_info and model_info["provider"] == "fireworks":
        api_key = FIREWORKS_API_KEY
        base_url = "https://api.fireworks.ai/inference/v1"

    try:
        if model_info and model_info["provider"] in ("openai", "novita", "fireworks"):
            # LiteLLM ìŠ¤íŠ¸ë¦¬ë°
            completion_fn = getattr(litellm, "completion")
            completion = completion_fn(
                model=session["model_id"],
                messages=messages,
                stream=True,
                api_key=api_key,
                base_url=base_url,
            )

            bot_response = ""
            for chunk in completion:
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and chunk.choices[0].delta
                    and chunk.choices[0].delta.content
                ):
                    chunk_content = chunk.choices[0].delta.content
                    bot_response += chunk_content
                    history[-1]["content"] = html.escape(bot_response)
                    yield history

        else:
            # ê¸°íƒ€ í”„ë¡œë°”ì´ë”: ê¸°ë³¸ LiteLLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (base_url ì—†ìŒ)
            completion_fn = getattr(litellm, "completion")
            completion = completion_fn(
                model=session["model_id"], messages=messages, stream=True
            )

            bot_response = ""
            for chunk in completion:
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and chunk.choices[0].delta
                    and chunk.choices[0].delta.content
                ):
                    chunk_content = chunk.choices[0].delta.content
                    bot_response += chunk_content
                    history[-1]["content"] = html.escape(bot_response)
                    yield history

    except Exception as e:
        print(f"âŒ Streaming error: {e}")
        print("Falling back to non-streaming mode...")

        # Fallback to non-streaming via LiteLLM
        try:
            completion_fn = getattr(litellm, "completion")
            completion = completion_fn(
                model=session["model_id"],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
            )
            if (
                hasattr(completion, "choices")
                and completion.choices
                and hasattr(completion.choices[0], "message")
            ):
                bot_response = completion.choices[0].message.content
            else:
                bot_response = str(getattr(completion, "output_text", "")) or str(
                    completion
                )
            history[-1]["content"] = html.escape(bot_response)
            yield history
        except Exception as fallback_error:
            print(f"âŒ Fallback error: {fallback_error}")
            error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            history[-1]["content"] = error_message
            yield history
            return

    # Save final history
    save_history(history, session_id)

    end_time = time.time()  # â±ï¸ TIMER
    elapsed_time = end_time - start_time  # â±ï¸ TIMER
    print(f"Responded to user query in {elapsed_time:.2f} seconds")  # â±ï¸ TIMER


# --------- (D) ADDITIONAL FUNCTIONS ---------
def create_client(provider):
    """ëª¨ë¸ ì œê³µìì— ë”°ë¼ í´ë¼ì´ì–¸íŠ¸/ì„¤ì • ìƒì„±"""
    if provider in ("openai", "novita", "fireworks"):
        if provider == "novita":
            return {
                "type": "litellm",
                "api_key": NOVITA_API_KEY,
                "base_url": "https://api.novita.ai/v3/openai",
            }
        if provider == "fireworks":
            return {
                "type": "litellm",
                "api_key": FIREWORKS_API_KEY,
                "base_url": "https://api.fireworks.ai/inference/v1",
            }
        return {"type": "litellm", "api_key": OPENAI_API_KEY, "base_url": None}

    raise ValueError(f"Unsupported provider: {provider}")


def change_model(model_name, request: gr.Request):
    """ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ ëª¨ë¸ ë³€ê²½"""
    session_id = get_session_id(request)
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        model_info = MODELS[model_name]
        sessions[session_id]["model_id"] = model_info["model_id"]
        sessions[session_id]["client"] = create_client(model_info["provider"])
        sessions.move_to_end(session_id)
    # print(f"ğŸ”„ Now using: {model_name}")


def change_rerank_method(rerank_method, request: gr.Request):
    """ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ rerank ë°©ë²• ë³€ê²½"""
    session_id = get_session_id(request)
    with session_lock:
        if session_id not in sessions:
            init_session(session_id, rerank_method=rerank_method)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        else:
            # ê¸°ì¡´ ì„¸ì…˜ì—ì„œ rerank ë°©ë²• ë³€ê²½
            sessions[session_id]["rerank_method"] = rerank_method

            # ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ retriever ì¬ìƒì„±
            vectorstore = sessions[session_id]["vectorstore"]
            if vectorstore:
                retrieval_mode = sessions[session_id].get(
                    "retrieval_mode", "ì ì‘í˜• (Knee Detection)"
                )
                print(f"ğŸ”„ Updating retriever with: {rerank_method}")
                sessions[session_id]["retriever"] = create_retriever(
                    vectorstore, rerank_method, retrieval_mode
                )

            sessions.move_to_end(session_id)


def change_retrieval_mode(retrieval_mode, request: gr.Request):
    """ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ retrieval ëª¨ë“œ ë³€ê²½"""
    session_id = get_session_id(request)
    with session_lock:
        if session_id not in sessions:
            init_session(session_id, retrieval_mode=retrieval_mode)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )
        else:
            # ê¸°ì¡´ ì„¸ì…˜ì—ì„œ retrieval ëª¨ë“œ ë³€ê²½
            sessions[session_id]["retrieval_mode"] = retrieval_mode

            # ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ retriever ì¬ìƒì„±
            vectorstore = sessions[session_id]["vectorstore"]
            if vectorstore:
                rerank_method = sessions[session_id].get("rerank_method", "ì—†ìŒ")
                print(f"ğŸ”„ Updating retriever with: {retrieval_mode}")
                sessions[session_id]["retriever"] = create_retriever(
                    vectorstore, rerank_method, retrieval_mode
                )

            sessions.move_to_end(session_id)


def save_history(history, session_id):
    """ëŒ€í™” ê¸°ë¡(history)ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    folder = "./chat_logs_lvrag"
    os.makedirs(folder, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d")
    filename = os.path.join(folder, f"{timestamp}_{session_id}.json")
    counter = 1
    while os.path.exists(filename):
        filename = os.path.join(folder, f"{timestamp}_{session_id}_{counter}.json")
        counter += 1
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def reset_session(request: gr.Request):
    """ëŒ€í™” ë° íŒŒì¼ ì—…ë¡œë“œ ë‚´ì—­ ì‚­ì œ"""
    session_id = get_session_id(request)
    with session_lock:
        current_rerank_method = "ì—†ìŒ"
        current_retrieval_mode = "ì ì‘í˜• (Knee Detection)"
        if session_id in sessions:
            current_rerank_method = sessions[session_id].get("rerank_method", "ì—†ìŒ")
            current_retrieval_mode = sessions[session_id].get(
                "retrieval_mode", "ì ì‘í˜• (Knee Detection)"
            )

        init_session(
            session_id,
            rerank_method=current_rerank_method,
            retrieval_mode=current_retrieval_mode,
        )
        sessions.move_to_end(session_id)
        print(f"â™»ï¸ Session {session_id[:8]}... reset.")
    return "", []


# --------- (E) Gradio UI ---------
css = """
div {
    flex-wrap: nowrap !important;
}
.responsive-height {
    height: 768px !important;
    padding-bottom: 64px !important;
}
.fill-height {
    height: 100% !important;
    flex-wrap: nowrap !important;
}
.extend-height {
    min-height: 260px !important;
    flex: 1 !important;
    overflow: auto !important;
}
footer {
    display: none !important;
}
"""

with gr.Blocks(title="LiberVance RAG", css=css, fill_height=True) as demo:
    gr.Markdown("<center><h1>ğŸ“„ LiberVance RAG</h1></center>")
    with gr.Row(elem_classes=["responsive-height"]):
        # Output column
        with gr.Column(elem_classes=["fill-height"]):
            chatbot = gr.Chatbot(
                label="Chatbot", type="messages", elem_classes=["extend-height"]
            )
        # Input column
        with gr.Column(elem_classes=["fill-height"]):
            model_dropdown = gr.Dropdown(
                list(MODELS.keys()),
                label="Select Model",
                value=DEFAULT_MODEL,
                allow_custom_value=True,
            )
            rerank_dropdown = gr.Dropdown(
                list(RERANK_OPTIONS.keys()),
                label="Rerank Method",
                value="ì—†ìŒ",
                info="Cross-Encoder ëª¨ë¸ì„ ì‚¬ìš©í•´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•©ë‹ˆë‹¤",
            )
            retrieval_mode_dropdown = gr.Dropdown(
                list(RETRIEVAL_MODES.keys()),
                label="Retrieval Mode",
                value="ì ì‘í˜• (Knee Detection)",
                info="ë¬¸ì„œ ì„ íƒ ë°©ì‹: ì ì‘í˜•(ë™ì  ê°œìˆ˜) vs ê³ ì • Top-K",
            )
            pdf_upload = gr.Files(
                label="Upload file(s) (PDF only)",
                file_types=[".pdf"],
                elem_classes=["extend-height"],
            )
            user_input = gr.Textbox(
                label="Enter your query here",
                placeholder="e.g., Summarize the key points from this document.",
                lines=3,
            )
            with gr.Row():
                submit_btn = gr.Button("Submit", variant="primary")
                reset_btn = gr.Button("Reset", variant="secondary")
    # Event listeners
    pdf_upload.change(fn=handle_pdf_upload, inputs=[pdf_upload], outputs=[])
    model_dropdown.input(fn=change_model, inputs=[model_dropdown], outputs=[])
    rerank_dropdown.input(fn=change_rerank_method, inputs=[rerank_dropdown], outputs=[])
    retrieval_mode_dropdown.input(
        fn=change_retrieval_mode, inputs=[retrieval_mode_dropdown], outputs=[]
    )
    user_input.submit(handle_query, inputs=[user_input], outputs=[chatbot])
    submit_btn.click(handle_query, inputs=[user_input], outputs=[chatbot])
    reset_btn.click(reset_session, inputs=[], outputs=[user_input, chatbot])


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - CLI entry point"""
    demo.launch(share=True, favicon_path="")


def main_dev():
    """ê°œë°œìš© ì‹¤í–‰ í•¨ìˆ˜ - ë¡œì»¬ ì„œë²„ë§Œ"""
    demo.launch(share=False, server_name="localhost", server_port=7860)


def main_prod():
    """í”„ë¡œë•ì…˜ ì‹¤í–‰ í•¨ìˆ˜ - ì™¸ë¶€ ì ‘ê·¼ ê°€ëŠ¥"""
    import os

    port = int(os.getenv("PORT", 7860))
    demo.launch(share=False, server_name="0.0.0.0", server_port=port)


if __name__ == "__main__":
    main()

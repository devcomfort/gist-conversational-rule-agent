#!/usr/bin/env python3
"""
Multi-Embedding GIST Rules Database Builder
==========================================

TODO.txtì— ëª…ì‹œëœ ì—¬ëŸ¬ ì„ë² ë”© ëª¨ë¸ë¡œ ê°ê° FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì§€ì›í•˜ëŠ” ì„ë² ë”© ëª¨ë¸:
- Qwen/Qwen3-Embedding-0.6B (3ìœ„)
- jinaai/jina-embeddings-v3 (22ìœ„)
- BAAI/bge-m3 (23ìœ„)
- sentence-transformers/all-MiniLM-L6-v2 (117ìœ„)

ì‚¬ìš©ë²•:
    python build_multi_embedding_databases.py
    python build_multi_embedding_databases.py --model Qwen/Qwen3-Embedding-0.6B

ì¶œë ¥:
    - faiss_qwen3_embedding_0.6b/ (Qwen ëª¨ë¸ìš©)
    - faiss_jina_embeddings_v3/ (Jina ëª¨ë¸ìš©)
    - faiss_bge_m3/ (BGE ëª¨ë¸ìš©)
    - faiss_all_minilm_l6_v2/ (ê¸°ë³¸ ëª¨ë¸ìš©)
"""

import os
import json
import time
import glob
import argparse
import concurrent.futures
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from dotenv import load_dotenv

try:
    import fitz  # PyMuPDF
except ImportError:
    print("âŒ PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("pip install PyMuPDF")
    exit(1)

try:
    import faiss
    import numpy as np
except ImportError:
    print("âŒ FAISSê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("pip install faiss-cpu  # ë˜ëŠ” faiss-gpu")
    exit(1)

# LangChain imports
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load environment variables
load_dotenv()

# Configuration
PDF_PATTERN = "rules/**/*.pdf"

# ì§€ì›í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ ì„¤ì • (TODO.txt ê¸°ë°˜)
EMBEDDING_MODELS = {
    "Qwen/Qwen3-Embedding-0.6B": {
        "model_name": "Qwen/Qwen3-Embedding-0.6B",
        "db_name": "faiss_qwen3_embedding_0.6b",
        "dimension": 1024,
        "mteb_rank": 3,
        "description": "Qwen3 Embedding 0.6B - MTEB 3ìœ„",
    },
    "jinaai/jina-embeddings-v3": {
        "model_name": "jinaai/jina-embeddings-v3",
        "db_name": "faiss_jina_embeddings_v3",
        "dimension": 1024,
        "mteb_rank": 22,
        "description": "Jina Embeddings v3 - MTEB 22ìœ„",
    },
    "BAAI/bge-m3": {
        "model_name": "BAAI/bge-m3",
        "db_name": "faiss_bge_m3",
        "dimension": 1024,
        "mteb_rank": 23,
        "description": "BGE M3 - MTEB 23ìœ„",
    },
    "sentence-transformers/all-MiniLM-L6-v2": {
        "model_name": "sentence-transformers/all-MiniLM-L6-v2",
        "db_name": "faiss_all_minilm_l6_v2",
        "dimension": 384,
        "mteb_rank": 117,
        "description": "All MiniLM L6 v2 - MTEB 117ìœ„ (ê¸°ì¡´ ê¸°ë³¸ ëª¨ë¸)",
    },
}

# Text splitter
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)


def get_document_category(file_path: str) -> tuple[str, int]:
    """íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ì™€ ìš°ì„ ìˆœìœ„ ê²°ì •"""
    path_lower = file_path.lower()

    # ìš°ì„ ìˆœìœ„ê°€ ë†’ì„ìˆ˜ë¡ ì¤‘ìš”í•œ ë¬¸ì„œ (ê²€ìƒ‰ ì‹œ ê°€ì¤‘ì¹˜ ì ìš©)
    if "í•™ì‚¬" in path_lower and "ê·œì •" in path_lower:
        return "í•™ì‚¬ê·œì •", 10
    elif "ëŒ€í•™ì›" in path_lower and "ê·œì •" in path_lower:
        return "ëŒ€í•™ì›ê·œì •", 9
    elif "ì—°êµ¬" in path_lower and ("ê·œì •" in path_lower or "ì§€ì¹¨" in path_lower):
        return "ì—°êµ¬ê·œì •", 8
    elif "í•™ì¹™" in path_lower:
        return "í•™ì¹™", 10
    elif "ë“±ë¡" in path_lower and "ê·œì •" in path_lower:
        return "ë“±ë¡ê·œì •", 7
    elif "ì¥í•™" in path_lower and "ê·œì •" in path_lower:
        return "ì¥í•™ê·œì •", 6
    elif "ê¸°ìˆ™ì‚¬" in path_lower or "ìƒí™œê´€" in path_lower:
        return "ìƒí™œê·œì •", 5
    elif "ë§¤ë‰´ì–¼" in path_lower or "manual" in path_lower:
        return "ì‚¬ìš©ìë§¤ë‰´ì–¼", 3
    elif "ì§€ì¹¨" in path_lower:
        return "ìš´ì˜ì§€ì¹¨", 4
    elif "ê·œì¹™" in path_lower:
        return "ê´€ë¦¬ê·œì¹™", 4
    else:
        return "ê¸°íƒ€", 1


def extract_text_from_pdf(pdf_path: str) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = fitz.open(pdf_path)
        text_pages = []

        for page_num, page in enumerate(doc):
            try:
                page_text = page.get_text("text")
                if page_text.strip():
                    text_pages.append(page_text)
            except Exception as page_error:
                print(f"âš ï¸ í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {page_error}")
                continue

        doc.close()

        if text_pages:
            return "\n".join(text_pages)
        else:
            return ""

    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ ({pdf_path}): {e}")
        return ""


def scan_pdf_files():
    """ëª¨ë“  PDF íŒŒì¼ ìŠ¤ìº”"""
    print("ğŸ” PDF íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
    pdf_files = glob.glob(PDF_PATTERN, recursive=True)
    pdf_files.sort()

    print(f"ğŸ“„ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
    return pdf_files


def process_pdfs(pdf_files):
    """ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬ - ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜"""
    print("\nğŸ”„ PDF íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ì‹œì‘...")

    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¬¸ì„œ ë¶„ë¥˜
    documents_by_category = {}
    category_stats = {}
    successfully_processed = 0
    failed_files = []

    start_time = time.time()

    for i, pdf_file in enumerate(pdf_files):
        print(
            f"\rğŸ“„ ì²˜ë¦¬ ì¤‘ [{i + 1}/{len(pdf_files)}]: {os.path.basename(pdf_file)}",
            end="",
            flush=True,
        )

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(pdf_file):
            print(f"\nâš ï¸ íŒŒì¼ ì—†ìŒ: {pdf_file}")
            failed_files.append(pdf_file)
            continue

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = extract_text_from_pdf(pdf_file)

        if text.strip():
            # íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ì²­í¬ í¬ê¸° ì¡°ì •
            file_size = os.path.getsize(pdf_file)
            is_large_file = file_size > 5 * 1024 * 1024  # 5MB ì´ìƒ

            if is_large_file:
                dynamic_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800, chunk_overlap=100
                )
            else:
                dynamic_splitter = TEXT_SPLITTER

            # ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ë° ìš°ì„ ìˆœìœ„ íšë“
            category, priority = get_document_category(pdf_file)

            # ë¬¸ì„œ ìƒì„± ë° ë¶„í• 
            doc = Document(
                page_content=text,
                metadata={
                    "source": pdf_file,
                    "filename": os.path.basename(pdf_file),
                    "category": category,
                    "priority": priority,
                    "file_size": file_size,
                    "processed_at": datetime.now().isoformat(),
                    "document_id": f"{category}_{os.path.splitext(os.path.basename(pdf_file))[0]}",
                },
            )

            docs = dynamic_splitter.split_documents([doc])

            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¬¸ì„œ ë¶„ë¥˜ ì €ì¥
            if category not in documents_by_category:
                documents_by_category[category] = []
                category_stats[category] = {"files": 0, "chunks": 0, "total_size": 0}

            documents_by_category[category].extend(docs)
            category_stats[category]["files"] += 1
            category_stats[category]["chunks"] += len(docs)
            category_stats[category]["total_size"] += file_size

            successfully_processed += 1

            file_size_mb = file_size / (1024 * 1024)
            print(
                f"\râœ… [{i + 1}/{len(pdf_files)}] {os.path.basename(pdf_file)} â†’ {category} ({len(docs)} chunks, {file_size_mb:.1f}MB)"
            )

        else:
            print(f"\nâš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ: {pdf_file}")
            failed_files.append(pdf_file)

        # ë©”ëª¨ë¦¬ ì •ë¦¬ (20ê°œë§ˆë‹¤)
        if (i + 1) % 20 == 0:
            print(f"\rğŸ§¹ [{i + 1}/{len(pdf_files)}] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ    ")

    processing_time = time.time() - start_time

    # ì „ì²´ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
    total_chunks = sum(len(docs) for docs in documents_by_category.values())

    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¶„ë¥˜ ì™„ë£Œ:")
    print("=" * 50)
    for category, docs in documents_by_category.items():
        stats = category_stats[category]
        priority = docs[0].metadata["priority"] if docs else 0
        size_mb = stats["total_size"] / (1024 * 1024)
        print(
            f"  ğŸ“‹ {category} (ìš°ì„ ìˆœìœ„ {priority}): {stats['files']}ê°œ íŒŒì¼, {stats['chunks']}ê°œ ì²­í¬, {size_mb:.1f}MB"
        )

    print("=" * 50)
    print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
    print(f"   ì„±ê³µ: {successfully_processed}/{len(pdf_files)} íŒŒì¼")
    print(f"   ì‹¤íŒ¨: {len(failed_files)} íŒŒì¼")
    print(f"   ì²­í¬: {total_chunks}ê°œ")
    print(f"   ì†Œìš”ì‹œê°„: {processing_time:.1f}ì´ˆ")

    return documents_by_category, successfully_processed, failed_files


def create_faiss_vectorstore_optimized(all_docs, embed_model):
    """ìµœì í™”ëœ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    print(f"\nğŸš€ ìµœì í™”ëœ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘... ({len(all_docs)} ì²­í¬)")

    if not all_docs:
        raise ValueError("ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")

    start_time = time.time()
    total_chunks = len(all_docs)

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
    estimated_memory_mb = total_chunks * 1.5 / 1024
    print(f"ğŸ“Š ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory_mb:.1f}MB")

    # ì¼ê´„ ì²˜ë¦¬ë¡œ ì‹œë„
    try:
        print("ğŸ“ ëª¨ë“  ë¬¸ì„œë¥¼ í•œ ë²ˆì— ë²¡í„°í™” ì¤‘...")
        vectorstore = FAISS.from_documents(all_docs, embed_model)

        total_time = time.time() - start_time
        avg_speed = len(all_docs) / total_time

        print(f"âœ… ì¼ê´„ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   í‰ê·  ì†ë„: {avg_speed:.1f} chunks/second")

        return vectorstore

    except MemoryError:
        print("âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± - ë°°ì¹˜ ëª¨ë“œë¡œ ì „í™˜...")
        return _create_vectorstore_batch(all_docs, embed_model, start_time)
    except Exception as e:
        print(f"âŒ ì¼ê´„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return _create_vectorstore_batch(all_docs, embed_model, start_time)


def _create_vectorstore_batch(all_docs, embed_model, start_time):
    """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    total_chunks = len(all_docs)
    batch_size = min(1000, max(100, total_chunks // 5))
    num_batches = (total_chunks + batch_size - 1) // batch_size

    print(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬: {num_batches}ê°œ ë°°ì¹˜, ë°°ì¹˜ë‹¹ ~{batch_size}ê°œ ì²­í¬")

    vectorstore = None

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_chunks)
        batch_docs = all_docs[start_idx:end_idx]

        print(
            f"\rğŸš€ ë°°ì¹˜ [{batch_idx + 1}/{num_batches}] ì²˜ë¦¬ ì¤‘... ({len(batch_docs)} chunks)",
            end="",
            flush=True,
        )

        try:
            if vectorstore is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                vectorstore = FAISS.from_documents(batch_docs, embed_model)
                print(f"\râœ… ì´ˆê¸° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {len(batch_docs)} chunks")
            else:
                # í›„ì† ë°°ì¹˜ë“¤ì„ ë³‘í•©
                batch_vectorstore = FAISS.from_documents(batch_docs, embed_model)
                vectorstore.merge_from(batch_vectorstore)
                del batch_vectorstore  # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ

                print(f"\râœ… ë°°ì¹˜ [{batch_idx + 1}/{num_batches}] ë³‘í•© ì™„ë£Œ")

        except Exception as e:
            print(f"\nâŒ ë°°ì¹˜ {batch_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    total_time = time.time() - start_time
    avg_speed = total_chunks / total_time if total_time > 0 else 0

    print(f"\nâœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
    print(f"   í‰ê·  ì†ë„: {avg_speed:.1f} chunks/second")

    return vectorstore


def create_category_aware_vectorstore(documents_by_category, embed_model):
    """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ëœ ë¬¸ì„œë“¤ë¡œ í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    print("\nğŸ¯ ì¹´í…Œê³ ë¦¬ ì¸ì‹ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹œì‘...")

    # ì „ì²´ ë¬¸ì„œë¥¼ ì¹´í…Œê³ ë¦¬ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ê²°í•©
    all_docs = []
    category_info = {}
    start_idx = 0

    # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì •ë ¬
    sorted_categories = sorted(
        documents_by_category.items(),
        key=lambda x: x[1][0].metadata["priority"] if x[1] else 0,
        reverse=True,
    )

    print("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„± ìˆœì„œ:")
    for category, docs in sorted_categories:
        category_info[category] = {
            "start_index": start_idx,
            "end_index": start_idx + len(docs) - 1,
            "doc_count": len(docs),
            "priority": docs[0].metadata["priority"] if docs else 0,
        }

        all_docs.extend(docs)
        start_idx += len(docs)

        priority = docs[0].metadata["priority"] if docs else 0
        print(
            f"  ğŸ“‹ {category} (ìš°ì„ ìˆœìœ„ {priority}): {len(docs)}ê°œ ì²­í¬ [ì¸ë±ìŠ¤ {category_info[category]['start_index']}-{category_info[category]['end_index']}]"
        )

    print(f"ğŸ“ ì´ {len(all_docs)}ê°œ ì²­í¬ë¥¼ í†µí•© ë²¡í„°ìŠ¤í† ì–´ë¡œ ìƒì„± ì¤‘...")

    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = create_faiss_vectorstore_optimized(all_docs, embed_model)

    return vectorstore, category_info


def save_vectorstore(vectorstore, category_info, db_path, model_config, stats):
    """ë²¡í„°ìŠ¤í† ì–´ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
    print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘... ({db_path})")

    # ë””ë ‰í† ë¦¬ ìƒì„±
    db_path.mkdir(exist_ok=True)

    # FAISS ì¸ë±ìŠ¤ ì €ì¥
    vectorstore.save_local(str(db_path))
    print(f"âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {db_path}/")

    # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì €ì¥
    with open(db_path / "category_mapping.json", "w", encoding="utf-8") as f:
        json.dump(category_info, f, ensure_ascii=False, indent=2)

    # í†µê³„ ì •ë³´ ì €ì¥
    database_info = {
        "created_at": datetime.now().isoformat(),
        "embedding_model": model_config["model_name"],
        "embedding_dimension": model_config["dimension"],
        "mteb_rank": model_config["mteb_rank"],
        "description": model_config["description"],
        "total_documents": stats["successfully_processed"],
        "total_chunks": len(vectorstore.docstore._dict),
        "failed_files": len(stats["failed_files"]),
        "processing_time_seconds": stats["processing_time"],
        "vectorization_time_seconds": stats.get("vectorization_time", 0),
        "file_sizes": {
            "index.faiss": os.path.getsize(db_path / "index.faiss"),
            "index.pkl": os.path.getsize(db_path / "index.pkl"),
        },
        "categories": category_info,
    }

    with open(db_path / "database_info.json", "w", encoding="utf-8") as f:
        json.dump(database_info, f, ensure_ascii=False, indent=2)

    print("âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: database_info.json, category_mapping.json")

    return database_info


def build_database_for_model(model_key: str, documents_by_category, stats):
    """íŠ¹ì • ì„ë² ë”© ëª¨ë¸ë¡œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
    model_config = EMBEDDING_MODELS[model_key]
    print(f"\n{'=' * 60}")
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸: {model_config['description']}")
    print(f"ğŸ“Š MTEB ìˆœìœ„: {model_config['mteb_rank']}ìœ„")
    print(f"ğŸ“ ì°¨ì›: {model_config['dimension']}")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {model_config['db_name']}/")
    print(f"{'=' * 60}")

    try:
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print(f"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_config['model_name']}")
        embed_model = HuggingFaceEmbeddings(
            model_name=model_config["model_name"],
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True},
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorization_start = time.time()
        vectorstore, category_info = create_category_aware_vectorstore(
            documents_by_category, embed_model
        )
        vectorization_time = time.time() - vectorization_start

        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        db_path = Path(model_config["db_name"])
        updated_stats = stats.copy()
        updated_stats["vectorization_time"] = vectorization_time

        database_info = save_vectorstore(
            vectorstore, category_info, db_path, model_config, updated_stats
        )

        # ê²°ê³¼ ì¶œë ¥
        total_chunks = len(vectorstore.docstore._dict)
        db_size_mb = database_info["file_sizes"]["index.faiss"] / (1024 * 1024)

        print(f"\nğŸ‰ {model_config['description']} ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        print(f"ğŸ“ ìœ„ì¹˜: {db_path.absolute()}")
        print(f"ğŸ“ ì´ ì²­í¬: {total_chunks}ê°œ")
        print(f"â±ï¸ ë²¡í„°í™” ì‹œê°„: {vectorization_time:.1f}ì´ˆ")
        print(f"ğŸ’¾ DB í¬ê¸°: {db_size_mb:.1f}MB")

        return True

    except Exception as e:
        print(f"âŒ {model_config['description']} ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Multi-Embedding GIST Rules Database Builder"
    )
    parser.add_argument(
        "--model",
        choices=list(EMBEDDING_MODELS.keys()),
        help="íŠ¹ì • ì„ë² ë”© ëª¨ë¸ë§Œ ì²˜ë¦¬ (ë¯¸ì§€ì •ì‹œ ëª¨ë“  ëª¨ë¸ ì²˜ë¦¬)",
    )
    parser.add_argument(
        "--list-models", action="store_true", help="ì§€ì›í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ ëª©ë¡ ì¶œë ¥"
    )

    args = parser.parse_args()

    if args.list_models:
        print("ğŸ¤– ì§€ì›í•˜ëŠ” ì„ë² ë”© ëª¨ë¸:")
        print("=" * 70)
        for key, config in EMBEDDING_MODELS.items():
            print(f"  ğŸ“Š MTEB {config['mteb_rank']:3d}ìœ„ | {config['description']}")
            print(f"  ğŸ·ï¸  ëª¨ë¸ëª…: {config['model_name']}")
            print(f"  ğŸ“ ì°¨ì›: {config['dimension']} | ğŸ’¾ DBëª…: {config['db_name']}")
            print()
        return

    print("ğŸš€ Multi-Embedding GIST Rules Database Builder ì‹œì‘!")
    print("=" * 60)

    overall_start = time.time()

    try:
        # 1. PDF íŒŒì¼ ìŠ¤ìº”
        pdf_files = scan_pdf_files()
        if not pdf_files:
            print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return

        # 2. PDF ì²˜ë¦¬ ë° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        documents_by_category, successfully_processed, failed_files = process_pdfs(
            pdf_files
        )

        if not documents_by_category:
            print("âŒ ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        # 3. í†µê³„ ì •ë³´ ì¤€ë¹„
        stats = {
            "successfully_processed": successfully_processed,
            "failed_files": failed_files,
            "processing_time": time.time() - overall_start,
        }

        # 4. ì„ë² ë”© ëª¨ë¸ë³„ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        if args.model:
            # íŠ¹ì • ëª¨ë¸ë§Œ ì²˜ë¦¬
            models_to_process = [args.model]
            print(f"ğŸ¯ ì„ íƒëœ ëª¨ë¸: {EMBEDDING_MODELS[args.model]['description']}")
        else:
            # ëª¨ë“  ëª¨ë¸ ì²˜ë¦¬
            models_to_process = list(EMBEDDING_MODELS.keys())
            print(f"ğŸ¯ ëª¨ë“  ì„ë² ë”© ëª¨ë¸ ì²˜ë¦¬: {len(models_to_process)}ê°œ")

        successful_builds = 0
        failed_builds = 0

        for model_key in models_to_process:
            try:
                if build_database_for_model(model_key, documents_by_category, stats):
                    successful_builds += 1
                else:
                    failed_builds += 1
            except KeyboardInterrupt:
                print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ {model_key} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                failed_builds += 1

        # 5. ìµœì¢… ê²°ê³¼ ì¶œë ¥
        total_time = time.time() - overall_start
        print("\n" + "=" * 60)
        print("ğŸ‰ Multi-Embedding Database êµ¬ì¶• ì™„ë£Œ!")
        print(f"âœ… ì„±ê³µ: {successful_builds}ê°œ ëª¨ë¸")
        print(f"âŒ ì‹¤íŒ¨: {failed_builds}ê°œ ëª¨ë¸")
        print(f"ğŸ“Š ì´ ë¬¸ì„œ: {successfully_processed}ê°œ")
        total_chunks = sum(len(docs) for docs in documents_by_category.values())
        print(f"ğŸ“ ì´ ì²­í¬: {total_chunks}ê°œ")
        print(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")

        if failed_files:
            print(f"âš ï¸ ì‹¤íŒ¨ íŒŒì¼: {len(failed_files)}ê°œ")

        print(f"\nğŸš€ ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì›í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ë¡œ ì•±ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   python app_gist_rules_analyzer_prebuilt.py")

    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()

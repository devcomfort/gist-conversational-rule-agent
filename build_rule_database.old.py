#!/usr/bin/env python3
"""
GIST Rules Database Builder
===========================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” GIST í•™ì¹™ ë° ê·œì • PDF íŒŒì¼ë“¤ì„ ì „ì²˜ë¦¬í•˜ì—¬
ì™„ì„±ëœ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python build_rule_database.py

ì¶œë ¥:
    - faiss_db/vectorstore.faiss (FAISS ì¸ë±ìŠ¤)
    - faiss_db/vectorstore.pkl (ë©”íƒ€ë°ì´í„°)
    - faiss_db/database_info.json (í†µê³„ ì •ë³´)
"""

import os
import json
import time
import glob
import fitz
import faiss
import numpy as np
import concurrent.futures
from pathlib import Path
from datetime import datetime
from typing import List
from dotenv import load_dotenv

# LangChain imports
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load environment variables
load_dotenv()

# Configuration
OUTPUT_DIR = Path("faiss_db")
DIMENSION = 384
PDF_PATTERN = "rules/**/*.pdf"

# Initialize embeddings model
print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
EMBED_MODEL = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

# Text splitter
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)


def create_output_directory():
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    OUTPUT_DIR.mkdir(exist_ok=True)
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR.absolute()}")


def scan_pdf_files():
    """ëª¨ë“  PDF íŒŒì¼ ìŠ¤ìº”"""
    print("ğŸ” PDF íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
    pdf_files = glob.glob(PDF_PATTERN, recursive=True)
    pdf_files.sort()

    print(f"ğŸ“„ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
    return pdf_files


def get_document_category(file_path: str) -> tuple[str, int]:
    """íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ì™€ ìš°ì„ ìˆœìœ„ ê²°ì •"""
    path_lower = file_path.lower()
    # filename = os.path.basename(file_path).lower()  # í•„ìš”ì‹œ ì‚¬ìš©

    # ìš°ì„ ìˆœìœ„ê°€ ë†’ì„ìˆ˜ë¡ ì¤‘ìš”í•œ ë¬¸ì„œ (ê²€ìƒ‰ ì‹œ ê°€ì¤‘ì¹˜ ì ìš©)
    if "í•™ì‚¬" in path_lower and "ê·œì •" in path_lower:
        return "í•™ì‚¬ê·œì •", 10
    elif "ëŒ€í•™ì›" in path_lower and "ê·œì •" in path_lower:
        return "ëŒ€í•™ì›ê·œì •", 9
    elif "ì—°êµ¬" in path_lower and ("ê·œì •" in path_lower or "ì§€ì¹¨" in path_lower):
        return "ì—°êµ¬ê·œì •", 8
    elif "í•™ì¹™" in path_lower:
        return "í•™ì¹™", 10
    elif "ë“±ë¡" in path_lower and "ê·œì •" in path_lower:
        return "ë“±ë¡ê·œì •", 7
    elif "ì¥í•™" in path_lower and "ê·œì •" in path_lower:
        return "ì¥í•™ê·œì •", 6
    elif "ê¸°ìˆ™ì‚¬" in path_lower or "ìƒí™œê´€" in path_lower:
        return "ìƒí™œê·œì •", 5
    elif "ë§¤ë‰´ì–¼" in path_lower or "manual" in path_lower:
        return "ì‚¬ìš©ìë§¤ë‰´ì–¼", 3
    elif "ì§€ì¹¨" in path_lower:
        return "ìš´ì˜ì§€ì¹¨", 4
    elif "ê·œì¹™" in path_lower:
        return "ê´€ë¦¬ê·œì¹™", 4
    else:
        return "ê¸°íƒ€", 1


def extract_text_from_pdf(pdf_path: str) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = fitz.open(pdf_path)
        text_pages = []

        for page_num, page in enumerate(doc):
            try:
                page_text = page.get_text("text")
                if page_text.strip():
                    text_pages.append(page_text)
            except Exception as page_error:
                print(f"âš ï¸ í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {page_error}")
                continue

        doc.close()

        if text_pages:
            return "\n".join(text_pages)
        else:
            return ""

    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ ({pdf_path}): {e}")
        return ""


def process_pdfs(pdf_files):
    """ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬ - ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜"""
    print("\nğŸ”„ PDF íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ì‹œì‘...")

    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¬¸ì„œ ë¶„ë¥˜
    documents_by_category = {}
    category_stats = {}
    successfully_processed = 0
    failed_files = []

    start_time = time.time()

    for i, pdf_file in enumerate(pdf_files):
        print(
            f"\rğŸ“„ ì²˜ë¦¬ ì¤‘ [{i + 1}/{len(pdf_files)}]: {os.path.basename(pdf_file)}",
            end="",
            flush=True,
        )

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(pdf_file):
            print(f"\nâš ï¸ íŒŒì¼ ì—†ìŒ: {pdf_file}")
            failed_files.append(pdf_file)
            continue

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = extract_text_from_pdf(pdf_file)

        if text.strip():
            # íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ì²­í¬ í¬ê¸° ì¡°ì •
            file_size = os.path.getsize(pdf_file)
            is_large_file = file_size > 5 * 1024 * 1024  # 5MB ì´ìƒ

            if is_large_file:
                dynamic_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800, chunk_overlap=100
                )
            else:
                dynamic_splitter = TEXT_SPLITTER

            # ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ë° ìš°ì„ ìˆœìœ„ íšë“
            category, priority = get_document_category(pdf_file)

            # ë¬¸ì„œ ìƒì„± ë° ë¶„í• 
            doc = Document(
                page_content=text,
                metadata={
                    "source": pdf_file,
                    "filename": os.path.basename(pdf_file),
                    "category": category,
                    "priority": priority,
                    "file_size": file_size,
                    "processed_at": datetime.now().isoformat(),
                    "document_id": f"{category}_{os.path.splitext(os.path.basename(pdf_file))[0]}",
                },
            )

            docs = dynamic_splitter.split_documents([doc])

            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¬¸ì„œ ë¶„ë¥˜ ì €ì¥
            if category not in documents_by_category:
                documents_by_category[category] = []
                category_stats[category] = {"files": 0, "chunks": 0, "total_size": 0}

            documents_by_category[category].extend(docs)
            category_stats[category]["files"] += 1
            category_stats[category]["chunks"] += len(docs)
            category_stats[category]["total_size"] += file_size

            successfully_processed += 1

            file_size_mb = file_size / (1024 * 1024)
            print(
                f"\râœ… [{i + 1}/{len(pdf_files)}] {os.path.basename(pdf_file)} â†’ {category} ({len(docs)} chunks, {file_size_mb:.1f}MB)"
            )

        else:
            print(f"\nâš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ: {pdf_file}")
            failed_files.append(pdf_file)

        # ë©”ëª¨ë¦¬ ì •ë¦¬ (20ê°œë§ˆë‹¤)
        if (i + 1) % 20 == 0:
            print(f"\rğŸ§¹ [{i + 1}/{len(pdf_files)}] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ    ")

    processing_time = time.time() - start_time

    # ì „ì²´ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
    total_chunks = sum(len(docs) for docs in documents_by_category.values())

    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¶„ë¥˜ ì™„ë£Œ:")
    print("=" * 50)
    for category, docs in documents_by_category.items():
        stats = category_stats[category]
        priority = docs[0].metadata["priority"] if docs else 0
        size_mb = stats["total_size"] / (1024 * 1024)
        print(
            f"  ğŸ“‹ {category} (ìš°ì„ ìˆœìœ„ {priority}): {stats['files']}ê°œ íŒŒì¼, {stats['chunks']}ê°œ ì²­í¬, {size_mb:.1f}MB"
        )

    print("=" * 50)
    print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
    print(f"   ì„±ê³µ: {successfully_processed}/{len(pdf_files)} íŒŒì¼")
    print(f"   ì‹¤íŒ¨: {len(failed_files)} íŒŒì¼")
    print(f"   ì²­í¬: {total_chunks}ê°œ")
    print(f"   ì†Œìš”ì‹œê°„: {processing_time:.1f}ì´ˆ")

    return documents_by_category, successfully_processed, failed_files


def create_category_aware_vectorstore(documents_by_category):
    """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ëœ ë¬¸ì„œë“¤ë¡œ í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    print("\nğŸ¯ ì¹´í…Œê³ ë¦¬ ì¸ì‹ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹œì‘...")

    # ì „ì²´ ë¬¸ì„œë¥¼ ì¹´í…Œê³ ë¦¬ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ê²°í•©
    all_docs = []
    category_info = {}
    start_idx = 0

    # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì •ë ¬
    sorted_categories = sorted(
        documents_by_category.items(),
        key=lambda x: x[1][0].metadata["priority"] if x[1] else 0,
        reverse=True,
    )

    print("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„± ìˆœì„œ:")
    for category, docs in sorted_categories:
        category_info[category] = {
            "start_index": start_idx,
            "end_index": start_idx + len(docs) - 1,
            "doc_count": len(docs),
            "priority": docs[0].metadata["priority"] if docs else 0,
        }

        all_docs.extend(docs)
        start_idx += len(docs)

        priority = docs[0].metadata["priority"] if docs else 0
        print(
            f"  ğŸ“‹ {category} (ìš°ì„ ìˆœìœ„ {priority}): {len(docs)}ê°œ ì²­í¬ [ì¸ë±ìŠ¤ {category_info[category]['start_index']}-{category_info[category]['end_index']}]"
        )

    print(f"ğŸ“ ì´ {len(all_docs)}ê°œ ì²­í¬ë¥¼ í†µí•© ë²¡í„°ìŠ¤í† ì–´ë¡œ ìƒì„± ì¤‘...")

    # ê¸°ì¡´ ìµœì í™”ëœ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë¡œì§ ì‚¬ìš©
    vectorstore = create_faiss_vectorstore_optimized(all_docs)

    # ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ (ì•±ì—ì„œ í™œìš©)
    OUTPUT_DIR.mkdir(exist_ok=True)
    with open(OUTPUT_DIR / "category_mapping.json", "w", encoding="utf-8") as f:
        json.dump(category_info, f, ensure_ascii=False, indent=2)
    print("ğŸ“„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ë³´ ì €ì¥: category_mapping.json")

    return vectorstore


def create_faiss_vectorstore_optimized(all_docs):
    """ìµœì í™”ëœ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± - ë³‘ë ¬ ì„ë² ë”© + ì§ì ‘ ë²¡í„° ì¶”ê°€"""
    print(f"\nğŸš€ ìµœì í™”ëœ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘... ({len(all_docs)} ì²­í¬)")

    if not all_docs:
        raise ValueError("ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")

    start_time = time.time()
    total_chunks = len(all_docs)

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (ê° ì„ë² ë”© 384ì°¨ì› * 4bytes = ~1.5KB per chunk)
    estimated_memory_mb = total_chunks * 1.5 / 1024
    available_memory_gb = 8  # ì¶”ì • ê°€ìš© ë©”ëª¨ë¦¬

    print(f"ğŸ“Š ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory_mb:.1f}MB")

    # ğŸ¯ ì „ëµ 1: ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´ í•œ ë²ˆì— ì²˜ë¦¬
    if estimated_memory_mb < available_memory_gb * 1024 * 0.5:  # ê°€ìš© ë©”ëª¨ë¦¬ì˜ 50% ì´ë‚´
        print("ğŸš€ ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ - ì¼ê´„ ì²˜ë¦¬ ëª¨ë“œ")
        return _create_vectorstore_bulk(all_docs, start_time)

    # ğŸ¯ ì „ëµ 2: ë³‘ë ¬ ì„ë² ë”© + ì§ì ‘ ë²¡í„° ì¶”ê°€
    elif total_chunks > 2000:
        print("âš¡ ë³‘ë ¬ ì„ë² ë”© + ì§ì ‘ ë²¡í„° ì¶”ê°€ ëª¨ë“œ")
        return _create_vectorstore_parallel(all_docs, start_time)

    # ğŸ¯ ì „ëµ 3: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ë¡œ ìµœì í™” ì²˜ë¦¬
    else:
        print("âš¡ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ")
        return _create_vectorstore_large_batch(all_docs, start_time)


def _create_vectorstore_bulk(all_docs, start_time):
    """ì „ì²´ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬"""
    print("ğŸ“ ëª¨ë“  ë¬¸ì„œë¥¼ í•œ ë²ˆì— ë²¡í„°í™” ì¤‘...")

    try:
        vectorstore = FAISS.from_documents(all_docs, EMBED_MODEL)

        total_time = time.time() - start_time
        avg_speed = len(all_docs) / total_time

        print(f"âœ… ì¼ê´„ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   í‰ê·  ì†ë„: {avg_speed:.1f} chunks/second")

        return vectorstore

    except MemoryError:
        print("âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± - ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ëª¨ë“œë¡œ ì „í™˜...")
        return _create_vectorstore_large_batch(all_docs, start_time)
    except Exception as e:
        print(f"âŒ ì¼ê´„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return _create_vectorstore_large_batch(all_docs, start_time)


def _create_vectorstore_large_batch(all_docs, start_time):
    """ëŒ€ìš©ëŸ‰ ë°°ì¹˜ë¡œ ìµœì í™” ì²˜ë¦¬"""
    total_chunks = len(all_docs)

    # ğŸš€ ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸° (5-10ë°° ì¦ê°€)
    if total_chunks <= 1000:
        batch_size = 500  # ê¸°ì¡´ 100 â†’ 500
    elif total_chunks <= 5000:
        batch_size = 1000  # ê¸°ì¡´ 200 â†’ 1000
    elif total_chunks <= 15000:
        batch_size = 2000  # ê¸°ì¡´ 300 â†’ 2000
    else:
        batch_size = 3000  # ê¸°ì¡´ 500 â†’ 3000

    num_batches = (total_chunks + batch_size - 1) // batch_size
    print(f"ğŸ“Š ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬: {num_batches}ê°œ ë°°ì¹˜, ë°°ì¹˜ë‹¹ ~{batch_size}ê°œ ì²­í¬")
    print(
        f"ğŸ”§ ì´ì „ ëŒ€ë¹„ ë°°ì¹˜ í¬ê¸°: {3 - 10}ë°° ì¦ê°€ë¡œ merge íšŸìˆ˜ {num_batches}ê°œë¡œ ìµœì†Œí™”"
    )

    vectorstore = None
    merge_times = []
    embed_times = []

    for batch_idx in range(num_batches):
        batch_start_time = time.time()
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_chunks)
        batch_docs = all_docs[start_idx:end_idx]

        print(
            f"\rğŸš€ ë°°ì¹˜ [{batch_idx + 1}/{num_batches}] ì²˜ë¦¬ ì¤‘... ({len(batch_docs)} chunks)",
            end="",
            flush=True,
        )

        try:
            if vectorstore is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                embed_start = time.time()
                vectorstore = FAISS.from_documents(batch_docs, EMBED_MODEL)
                embed_time = time.time() - embed_start
                embed_times.append(embed_time)

                print(f"\râœ… ì´ˆê¸° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {len(batch_docs)} chunks")
            else:
                # ğŸš€ ìµœì í™”: ì„ë² ë”©ê³¼ mergeë¥¼ ë¶„ë¦¬í•˜ì—¬ ì¸¡ì •
                embed_start = time.time()
                batch_vectorstore = FAISS.from_documents(batch_docs, EMBED_MODEL)
                embed_time = time.time() - embed_start
                embed_times.append(embed_time)

                merge_start = time.time()
                vectorstore.merge_from(batch_vectorstore)
                merge_time = time.time() - merge_start
                merge_times.append(merge_time)

                del batch_vectorstore  # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ

            batch_time = time.time() - batch_start_time
            chunks_per_second = len(batch_docs) / batch_time if batch_time > 0 else 0

            # ë” ìƒì„¸í•œ ì„±ëŠ¥ ì •ë³´
            embed_avg = embed_times[-1] if embed_times else 0
            merge_avg = merge_times[-1] if merge_times else 0

            print(
                f"\râœ… ë°°ì¹˜ [{batch_idx + 1}/{num_batches}] ì™„ë£Œ: {len(batch_docs)} chunks | "
                f"ì„ë² ë”©: {embed_avg:.1f}s, ë³‘í•©: {merge_avg:.1f}s | {chunks_per_second:.1f} chunks/s"
            )

        except Exception as e:
            print(f"\nâŒ ë°°ì¹˜ {batch_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    total_time = time.time() - start_time
    avg_speed = total_chunks / total_time if total_time > 0 else 0

    # ğŸ“Š ìƒì„¸ ì„±ëŠ¥ ë¶„ì„
    if embed_times and merge_times:
        total_embed_time = sum(embed_times)
        total_merge_time = sum(merge_times)
        embed_percent = (total_embed_time / total_time) * 100
        merge_percent = (total_merge_time / total_time) * 100

        print("âœ… ìµœì í™”ëœ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   í‰ê·  ì†ë„: {avg_speed:.1f} chunks/second")
        print(
            f"   ì‹œê°„ ë¶„ì„: ì„ë² ë”© {total_embed_time:.1f}s ({embed_percent:.1f}%), "
            f"ë³‘í•© {total_merge_time:.1f}s ({merge_percent:.1f}%)"
        )
        print(f"   ë°°ì¹˜ íš¨ìœ¨ì„±: {num_batches}íšŒ merge (ê¸°ì¡´ ëŒ€ë¹„ ëŒ€í­ ê°ì†Œ)")
    else:
        print("âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   í‰ê·  ì†ë„: {avg_speed:.1f} chunks/second")

    return vectorstore


def _create_vectorstore_parallel(all_docs, start_time):
    """ë³‘ë ¬ ì„ë² ë”© + ì§ì ‘ ë²¡í„° ì¶”ê°€ë¡œ ìµœê³  ì„±ëŠ¥"""
    print("ğŸ”¥ ë³‘ë ¬ ì„ë² ë”© + ì§ì ‘ FAISS ë²¡í„° ì¶”ê°€ ëª¨ë“œ ì‹œì‘")

    total_chunks = len(all_docs)

    # ë³‘ë ¬ ì²˜ë¦¬ìš© ë°°ì¹˜ í¬ê¸° (CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜)
    cpu_count = os.cpu_count() or 4
    max_workers = min(8, max(2, cpu_count // 2))  # ìµœëŒ€ 8ê°œ, ìµœì†Œ 2ê°œ
    batch_size = min(1000, max(100, total_chunks // max_workers))  # ì›Œì»¤ë‹¹ ì ì ˆí•œ í¬ê¸°

    print(
        f"ğŸ”§ ë³‘ë ¬ ì„¤ì •: {max_workers}ê°œ ì›Œì»¤ (CPU: {cpu_count}), ë°°ì¹˜ í¬ê¸°: {batch_size}"
    )

    try:
        # ğŸš€ 1ë‹¨ê³„: ë³‘ë ¬ë¡œ ëª¨ë“  í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        embedding_start = time.time()
        print("ğŸ“ ë³‘ë ¬ ì„ë² ë”© ìƒì„± ì¤‘...")

        all_texts = [doc.page_content for doc in all_docs]
        # all_metadatas = [doc.metadata for doc in all_docs]  # ì‚¬ìš© ì•ˆí•¨

        # ë³‘ë ¬ë¡œ ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ë‹¨ìœ„)
        all_embeddings = []
        batches = [
            all_texts[i : i + batch_size] for i in range(0, len(all_texts), batch_size)
        ]

        def embed_batch_optimized(texts_batch):
            """ìµœì í™”ëœ ë‹¨ì¼ ë°°ì¹˜ ì„ë² ë”©"""
            return EMBED_MODEL.embed_documents(texts_batch)

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            future_to_batch = {
                executor.submit(embed_batch_optimized, batch): i
                for i, batch in enumerate(batches)
            }
            completed_batches = [None] * len(batches)

            for future in concurrent.futures.as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    batch_embeddings = future.result()
                    completed_batches[batch_idx] = batch_embeddings
                    progress = (batch_idx + 1) / len(batches) * 100
                    print(
                        f"\râš¡ ì„ë² ë”© ì§„í–‰: {batch_idx + 1}/{len(batches)} ({progress:.1f}%)",
                        end="",
                        flush=True,
                    )
                except Exception as e:
                    print(f"\nâŒ ë°°ì¹˜ {batch_idx} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                    completed_batches[batch_idx] = []

        # ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ í•©ì¹˜ê¸°
        for batch_embeddings in completed_batches:
            if batch_embeddings:
                all_embeddings.extend(batch_embeddings)

        embedding_time = time.time() - embedding_start
        print(
            f"\nâœ… ë³‘ë ¬ ì„ë² ë”© ì™„ë£Œ: {len(all_embeddings)}ê°œ ë²¡í„° ìƒì„± in {embedding_time:.1f}s"
        )

        if len(all_embeddings) != len(all_docs):
            print(
                f"âš ï¸ ì„ë² ë”© ìˆ˜ ë¶ˆì¼ì¹˜: {len(all_embeddings)} vs {len(all_docs)}, fallback..."
            )
            return _create_vectorstore_large_batch(all_docs, start_time)

        # ğŸš€ 2ë‹¨ê³„: ì§ì ‘ FAISS ì¸ë±ìŠ¤ ìƒì„± (merge ì˜¤ë²„í—¤ë“œ ì œê±°)
        faiss_start = time.time()
        print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ì§ì ‘ ìƒì„± ì¤‘...")

        # numpy ë°°ì—´ë¡œ ë³€í™˜
        embeddings_array = np.array(all_embeddings, dtype=np.float32)
        dimension = embeddings_array.shape[1]

        # FAISS ì¸ë±ìŠ¤ ìƒì„± (L2 ê±°ë¦¬)
        faiss_index = faiss.IndexFlatL2(dimension)
        faiss_index.add(embeddings_array)

        # LangChain FAISS vectorstore ìƒì„±
        from langchain_community.docstore.in_memory import InMemoryDocstore

        # docstore êµ¬ì„± (ì¸ë±ìŠ¤ -> ë¬¸ì„œ ë§¤í•‘)
        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(all_docs)})
        index_to_docstore_id = {i: str(i) for i in range(len(all_docs))}

        vectorstore = FAISS(
            embedding_function=EMBED_MODEL,
            index=faiss_index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
        )

        faiss_time = time.time() - faiss_start
        print(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ in {faiss_time:.1f}s")

        # ğŸ“Š ì„±ëŠ¥ ë¶„ì„
        total_time = time.time() - start_time
        avg_speed = total_chunks / total_time
        embedding_percent = (embedding_time / total_time) * 100
        faiss_percent = (faiss_time / total_time) * 100
        parallelization_speedup = len(batches) / max_workers if max_workers > 1 else 1

        print("ğŸ”¥ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ! (ìµœê³  ì„±ëŠ¥ ëª¨ë“œ)")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   ì²˜ë¦¬ ì†ë„: {avg_speed:.1f} chunks/second")
        print(f"   ë³‘ë ¬ ì„ë² ë”©: {embedding_time:.1f}s ({embedding_percent:.1f}%)")
        print(f"   FAISS ìƒì„±: {faiss_time:.1f}s ({faiss_percent:.1f}%)")
        print(f"   ë³‘ë ¬ íš¨ìœ¨ì„±: {max_workers}ê°œ ì›Œì»¤, {len(batches)}ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬")
        print(f"   ì†ë„ í–¥ìƒ: ~{parallelization_speedup:.1f}x (ì´ë¡ ì )")
        print(f"   ë©”ëª¨ë¦¬ ìµœì í™”: merge ê³¼ì • ì™„ì „ ì œê±°")

        return vectorstore

    except Exception as e:
        print(f"âŒ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ëª¨ë“œë¡œ fallback...")
        return _create_vectorstore_large_batch(all_docs, start_time)


def create_faiss_vectorstore_parallel(all_docs):
    """ìµœê³  ì„±ëŠ¥ ë³‘ë ¬ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    print(f"\nğŸš€ ë³‘ë ¬ ìµœì í™” FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘... ({len(all_docs)} ì²­í¬)")

    if not all_docs:
        raise ValueError("ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")

    start_time = time.time()
    total_chunks = len(all_docs)

    # ğŸ”§ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìµœì í™”
    cpu_count = os.cpu_count() or 4
    max_workers = min(cpu_count, 4)  # ë„ˆë¬´ ë§ì€ ìŠ¤ë ˆë“œëŠ” ì˜¤íˆë ¤ ëŠë¦¼

    print(f"ğŸ”§ ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •: {max_workers} ì›Œì»¤, CPU ì½”ì–´: {cpu_count}")

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
    estimated_memory_mb = total_chunks * 1.5 / 1024
    print(f"ğŸ“Š ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory_mb:.1f}MB")

    # ğŸ¯ ì „ëµ ì„ íƒ
    if total_chunks < 500:
        print("ğŸ“ ì†ŒëŸ‰ ë°ì´í„° - ë‹¨ìˆœ ì²˜ë¦¬ ëª¨ë“œ")
        return _create_vectorstore_bulk(all_docs, start_time)
    elif estimated_memory_mb < 2048:  # 2GB ì´í•˜
        print("âš¡ ì§ì ‘ ë²¡í„° ì¶”ê°€ ëª¨ë“œ (ìµœê³  ì„±ëŠ¥)")
        return _create_vectorstore_direct_add(all_docs, start_time, max_workers)
    else:
        print("ğŸš€ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ")
        return _create_vectorstore_parallel_batch(all_docs, start_time, max_workers)


def _create_vectorstore_direct_add(all_docs, start_time, max_workers):
    """ì§ì ‘ ë²¡í„° ì¶”ê°€ ë°©ì‹ - ìµœê³  ì„±ëŠ¥"""
    print("ğŸ”§ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")

    # 1. ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = [doc.page_content for doc in all_docs]
    # metadatas = [doc.metadata for doc in all_docs]  # ì‚¬ìš© ì•ˆí•¨

    # 2. ë³‘ë ¬ë¡œ ì„ë² ë”© ìƒì„±
    print(f"âš¡ {max_workers}ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬ ì„ë² ë”© ìƒì„± ì¤‘...")
    embed_start = time.time()

    # ë°°ì¹˜ í¬ê¸°ë¥¼ ì›Œì»¤ ìˆ˜ì— ë§ê²Œ ì¡°ì •
    batch_size = max(100, len(texts) // (max_workers * 2))
    text_batches = [texts[i : i + batch_size] for i in range(0, len(texts), batch_size)]

    embeddings = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        future_to_batch = {
            executor.submit(_embed_batch, batch, batch_idx): batch_idx
            for batch_idx, batch in enumerate(text_batches)
        }

        for future in concurrent.futures.as_completed(future_to_batch):
            batch_idx = future_to_batch[future]
            try:
                batch_embeddings = future.result()
                embeddings.extend(batch_embeddings)
                print(
                    f"\râš¡ ë°°ì¹˜ {batch_idx + 1}/{len(text_batches)} ì„ë² ë”© ì™„ë£Œ",
                    end="",
                    flush=True,
                )
            except Exception as e:
                print(f"\nâŒ ë°°ì¹˜ {batch_idx} ì„ë² ë”© ì‹¤íŒ¨: {e}")

    embed_time = time.time() - embed_start
    print(f"\nâœ… ë³‘ë ¬ ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„° in {embed_time:.1f}s")

    # 3. FAISS ì¸ë±ìŠ¤ ì§ì ‘ êµ¬ì„±
    print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ì§ì ‘ êµ¬ì„± ì¤‘...")
    faiss_start = time.time()

    try:
        # ë²¡í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        embeddings_array = np.array(embeddings, dtype=np.float32)
        dimension = embeddings_array.shape[1]

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings_array)

        # FAISS vectorstore ê°ì²´ ìƒì„±
        vectorstore = FAISS(
            embedding_function=EMBED_MODEL.embed_query,
            index=index,
            docstore={i: all_docs[i] for i in range(len(all_docs))},
            index_to_docstore_id={i: i for i in range(len(all_docs))},
        )

        faiss_time = time.time() - faiss_start
        total_time = time.time() - start_time
        avg_speed = len(all_docs) / total_time

        print(f"âœ… ì§ì ‘ ë²¡í„° ì¶”ê°€ ì™„ë£Œ!")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   ì„ë² ë”© ì‹œê°„: {embed_time:.1f}s, FAISS êµ¬ì„±: {faiss_time:.1f}s")
        print(f"   í‰ê·  ì†ë„: {avg_speed:.1f} chunks/second")
        print(f"   ì„±ëŠ¥ í–¥ìƒ: ë³‘ë ¬ ì²˜ë¦¬ë¡œ {max_workers}ë°° ê°€ì†")

        return vectorstore

    except Exception as e:
        print(f"âŒ ì§ì ‘ ë²¡í„° ì¶”ê°€ ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ëª¨ë“œë¡œ í´ë°±...")
        return _create_vectorstore_large_batch(all_docs, start_time)


def _embed_batch(texts: List[str], batch_idx: int) -> List[List[float]]:
    """ë‹¨ì¼ ë°°ì¹˜ ì„ë² ë”© ì²˜ë¦¬"""
    try:
        # ê° ìŠ¤ë ˆë“œì—ì„œ ë…ë¦½ì ì¸ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
        return EMBED_MODEL.embed_documents(texts)
    except Exception as e:
        print(f"ë°°ì¹˜ {batch_idx} ì„ë² ë”© ì˜¤ë¥˜: {e}")
        return []


def _create_vectorstore_parallel_batch(all_docs, start_time, max_workers):
    """ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬"""
    total_chunks = len(all_docs)

    # ì›Œì»¤ ìˆ˜ì— ë§ì¶° ë°°ì¹˜ í¬ê¸° ê²°ì •
    batch_size = max(500, total_chunks // max_workers)
    batches = [all_docs[i : i + batch_size] for i in range(0, total_chunks, batch_size)]

    print(f"ğŸ“Š ë³‘ë ¬ ë°°ì¹˜: {len(batches)}ê°œ ë°°ì¹˜, ë°°ì¹˜ë‹¹ ~{batch_size}ê°œ ì²­í¬")
    print(f"âš¡ {max_workers}ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

    # ë³‘ë ¬ë¡œ ê° ë°°ì¹˜ë¥¼ ë²¡í„°ìŠ¤í† ì–´ë¡œ ë³€í™˜
    vectorstores = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_batch = {
            executor.submit(_create_single_vectorstore, batch, batch_idx): batch_idx
            for batch_idx, batch in enumerate(batches)
        }

        for future in concurrent.futures.as_completed(future_to_batch):
            batch_idx = future_to_batch[future]
            try:
                batch_vectorstore = future.result()
                if batch_vectorstore:
                    vectorstores.append(batch_vectorstore)
                    print(
                        f"âš¡ ë°°ì¹˜ {batch_idx + 1}/{len(batches)} ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ"
                    )
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    # ëª¨ë“  ë°°ì¹˜ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë³‘í•©
    print("ğŸ”— ë³‘ë ¬ ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ë“¤ì„ ë³‘í•© ì¤‘...")
    merge_start = time.time()

    if not vectorstores:
        raise ValueError("ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤!")

    final_vectorstore = vectorstores[0]
    for i, vs in enumerate(vectorstores[1:], 1):
        print(f"\rğŸ”— ë³‘í•© ì¤‘... {i}/{len(vectorstores) - 1}", end="", flush=True)
        final_vectorstore.merge_from(vs)
        del vs  # ë©”ëª¨ë¦¬ í•´ì œ

    merge_time = time.time() - merge_start
    total_time = time.time() - start_time
    avg_speed = total_chunks / total_time

    print(f"\nâœ… ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
    print(f"   ë³‘í•© ì‹œê°„: {merge_time:.1f}ì´ˆ")
    print(f"   í‰ê·  ì†ë„: {avg_speed:.1f} chunks/second")
    print(f"   ë³‘ë ¬ íš¨ìœ¨ì„±: {max_workers}ê°œ ì›Œì»¤ í™œìš©")

    return final_vectorstore


def _create_single_vectorstore(docs, batch_idx):
    """ë‹¨ì¼ ë°°ì¹˜ë¥¼ ë²¡í„°ìŠ¤í† ì–´ë¡œ ë³€í™˜"""
    try:
        return FAISS.from_documents(docs, EMBED_MODEL)
    except Exception as e:
        print(f"ë°°ì¹˜ {batch_idx} ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def create_faiss_vectorstore(documents_input):
    """FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± - ìë™ ìµœì í™” (ì¹´í…Œê³ ë¦¬ ì¸ì‹)"""

    # ì…ë ¥ì´ ì¹´í…Œê³ ë¦¬ë³„ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í‰ë©´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
    if isinstance(documents_input, dict):
        # ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
        print("ğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ê°ì§€ - ì¹´í…Œê³ ë¦¬ ì¸ì‹ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±")
        return create_category_aware_vectorstore(documents_input)
    else:
        # ê¸°ì¡´ í‰ë©´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (í•˜ìœ„ í˜¸í™˜ì„±)
        print("ğŸ“ í‰ë©´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ê°ì§€ - ê¸°ì¡´ ìµœì í™” ë°©ì‹ ì‚¬ìš©")
        all_docs = documents_input
        total_chunks = len(all_docs)

        if total_chunks < 100:
            print("ğŸ“ ì†ŒëŸ‰ ë°ì´í„° - ê¸°ë³¸ ì²˜ë¦¬")
            return FAISS.from_documents(all_docs, EMBED_MODEL)
        elif total_chunks < 2000:
            print("âš¡ ì¤‘ê°„ ê·œëª¨ - ê¸°ë³¸ ìµœì í™”")
            return create_faiss_vectorstore_optimized(all_docs)
        else:
            print("ğŸš€ ëŒ€ìš©ëŸ‰ ë°ì´í„° - ë³‘ë ¬ ìµœì í™”")
            return create_faiss_vectorstore_parallel(all_docs)


def create_additional_indexes(vectorstore):
    """ì¶”ê°€ FAISS ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ ë¹„êµìš©)"""
    print("\nğŸ”§ ì¶”ê°€ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")

    try:
        # ê¸°ì¡´ ë²¡í„° ì¶”ì¶œ
        vectors = vectorstore.index.reconstruct_n(0, vectorstore.index.ntotal)
        results = {}

        # IndexIVFFlat
        if vectorstore.index.ntotal > 100:
            print("  ğŸ”¹ IndexIVFFlat ìƒì„±...")
            nlist = min(100, int(np.sqrt(vectorstore.index.ntotal)))
            quantizer = faiss.IndexFlatL2(DIMENSION)
            ivf_index = faiss.IndexIVFFlat(quantizer, DIMENSION, nlist)
            ivf_index.train(vectors)
            ivf_index.add(vectors)
            ivf_index.nprobe = min(10, nlist)

            faiss.write_index(ivf_index, str(OUTPUT_DIR / "vectorstore_ivf.faiss"))
            results["IndexIVFFlat"] = {"nlist": nlist, "nprobe": ivf_index.nprobe}

        # IndexHNSWFlat
        if vectorstore.index.ntotal > 50:
            print("  ğŸ”¹ IndexHNSWFlat ìƒì„±...")
            hnsw_index = faiss.IndexHNSWFlat(DIMENSION, 32)
            hnsw_index.hnsw.efConstruction = 200
            hnsw_index.hnsw.efSearch = 64
            hnsw_index.add(vectors)

            faiss.write_index(hnsw_index, str(OUTPUT_DIR / "vectorstore_hnsw.faiss"))
            results["IndexHNSWFlat"] = {"M": 32, "efConstruction": 200, "efSearch": 64}

        print(f"âœ… {len(results)}ê°œ ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        return results

    except Exception as e:
        print(f"âš ï¸ ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return {}


def save_vectorstore(vectorstore, additional_indexes, stats):
    """ë²¡í„°ìŠ¤í† ì–´ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
    print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")

    # FAISS ì¸ë±ìŠ¤ ì €ì¥
    vectorstore.save_local(str(OUTPUT_DIR))
    print(f"âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {OUTPUT_DIR}/")

    # í†µê³„ ì •ë³´ ì €ì¥
    database_info = {
        "created_at": datetime.now().isoformat(),
        "total_documents": stats["successfully_processed"],
        "total_chunks": len(vectorstore.docstore._dict),
        "failed_files": len(stats["failed_files"]),
        "processing_time_seconds": stats["processing_time"],
        "vectorization_time_seconds": stats.get("vectorization_time", 0),
        "dimension": DIMENSION,
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "additional_indexes": additional_indexes,
        "file_sizes": {
            "vectorstore.faiss": os.path.getsize(OUTPUT_DIR / "index.faiss"),
            "vectorstore.pkl": os.path.getsize(OUTPUT_DIR / "index.pkl"),
        },
    }

    with open(OUTPUT_DIR / "database_info.json", "w", encoding="utf-8") as f:
        json.dump(database_info, f, ensure_ascii=False, indent=2)

    print("âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: database_info.json")

    return database_info


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ GIST Rules Database Builder ì‹œì‘!")
    print("=" * 50)

    overall_start = time.time()

    try:
        # 1. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        create_output_directory()

        # 2. PDF íŒŒì¼ ìŠ¤ìº”
        pdf_files = scan_pdf_files()
        if not pdf_files:
            print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return

        # 3. PDF ì²˜ë¦¬ ë° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        documents_by_category, successfully_processed, failed_files = process_pdfs(
            pdf_files
        )

        if not documents_by_category:
            print("âŒ ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        # 4. ì¹´í…Œê³ ë¦¬ ì¸ì‹ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorization_start = time.time()
        vectorstore = create_faiss_vectorstore(documents_by_category)
        vectorization_time = time.time() - vectorization_start

        # 5. ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„±
        additional_indexes = create_additional_indexes(vectorstore)

        # 6. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        stats = {
            "successfully_processed": successfully_processed,
            "failed_files": failed_files,
            "processing_time": time.time() - overall_start,
            "vectorization_time": vectorization_time,
        }

        database_info = save_vectorstore(vectorstore, additional_indexes, stats)

        # 7. ìµœì¢… ê²°ê³¼ ì¶œë ¥
        total_time = time.time() - overall_start
        print("\n" + "=" * 50)
        print("ğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        print(f"ğŸ“ ìœ„ì¹˜: {OUTPUT_DIR.absolute()}")
        print(f"ğŸ“Š ì´ ë¬¸ì„œ: {successfully_processed}ê°œ")
        total_chunks = sum(len(docs) for docs in documents_by_category.values())
        print(f"ğŸ“ ì´ ì²­í¬: {total_chunks}ê°œ")
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(
            f"ğŸ’¾ DB í¬ê¸°: {database_info['file_sizes']['vectorstore.faiss'] / (1024 * 1024):.1f}MB"
        )

        if failed_files:
            print(f"âš ï¸ ì‹¤íŒ¨ íŒŒì¼: {len(failed_files)}ê°œ")

        print("\nğŸš€ ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì•±ì„ ë¹ ë¥´ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   python app_gist_rules_analyzer_prebuilt.py")

    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


def benchmark_vectorstore_creation(documents_by_category, sample_ratio=0.1):
    """ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸: ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œì˜ ì„±ëŠ¥ ë¹„êµ"""
    print("\nğŸ ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ: ì¹´í…Œê³ ë¦¬ ì¸ì‹ ë²¡í„°ìŠ¤í† ì–´ ì„±ëŠ¥ ë¹„êµ")

    # ì „ì²´ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
    total_docs = sum(len(docs) for docs in documents_by_category.values())

    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¹„ë¡€í•˜ì—¬ ìƒ˜í”Œë§
    sample_documents_by_category = {}
    total_sample_size = 0

    for category, docs in documents_by_category.items():
        category_sample_size = max(10, int(len(docs) * sample_ratio))
        category_sample_size = min(category_sample_size, 200)  # ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 200ê°œ
        sample_documents_by_category[category] = docs[:category_sample_size]
        total_sample_size += category_sample_size

    print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ:")
    for category, docs in sample_documents_by_category.items():
        orig_count = len(documents_by_category[category])
        print(f"  ğŸ“‹ {category}: {len(docs)}ê°œ ìƒ˜í”Œ (ì›ë³¸ {orig_count}ê°œ ì¤‘)")

    print(
        f"ğŸ“Š ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {total_sample_size}ê°œ ì²­í¬ ({total_docs}ê°œ ì¤‘ {sample_ratio * 100:.1f}%)"
    )
    print("=" * 60)

    results = {}

    # í…ŒìŠ¤íŠ¸ ëª©ë¡ - ì¹´í…Œê³ ë¦¬ ì¸ì‹ ë²„ì „ê³¼ ê¸°ì¡´ ë²„ì „ ë¹„êµ
    tests = [
        (
            "ì¹´í…Œê³ ë¦¬_ì¸ì‹",
            lambda docs_by_cat: create_category_aware_vectorstore(docs_by_cat),
        ),
        (
            "ê¸°ì¡´_ì„ í˜•_ë³‘í•©",
            lambda docs_by_cat: create_faiss_vectorstore_optimized(
                [doc for docs in docs_by_cat.values() for doc in docs]
            ),
        ),
    ]

    for i, (test_name, test_func) in enumerate(tests, 1):
        print(f"\nğŸ” [{i}/{len(tests)}] {test_name} í…ŒìŠ¤íŠ¸ ì¤‘...")

        start_time = time.time()
        try:
            vectorstore = test_func(sample_documents_by_category)
            end_time = time.time()

            duration = end_time - start_time
            speed = total_sample_size / duration if duration > 0 else 0

            results[test_name] = {
                "ì„±ê³µ": True,
                "ì‹œê°„": duration,
                "ì†ë„": speed,
                "vectorstore": vectorstore,
            }

            print(f"âœ… {test_name} ì™„ë£Œ: {duration:.1f}s, {speed:.1f} chunks/s")

        except Exception as e:
            results[test_name] = {"ì„±ê³µ": False, "ì‹œê°„": 0, "ì†ë„": 0, "ì˜¤ë¥˜": str(e)}
            print(f"âŒ {test_name} ì‹¤íŒ¨: {e}")

    # ğŸ“Š ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
    print("\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„")
    print("=" * 60)

    # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë“¤ë§Œ ì •ë ¬
    successful_tests = [(name, data) for name, data in results.items() if data["ì„±ê³µ"]]

    if not successful_tests:
        print("âŒ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None

    # ì†ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    successful_tests.sort(key=lambda x: x[1]["ì†ë„"], reverse=True)

    print("ğŸ† ì„±ëŠ¥ ìˆœìœ„:")
    for rank, (test_name, data) in enumerate(successful_tests, 1):
        medal = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][rank - 1] if rank <= 3 else f"{rank}ìœ„"
        print(
            f"   {medal} {test_name}: {data['ì‹œê°„']:.1f}s ({data['ì†ë„']:.1f} chunks/s)"
        )

    # ìµœê³  ì„±ëŠ¥ ëŒ€ë¹„ ë¹„êµ
    best_speed = successful_tests[0][1]["ì†ë„"]
    print("\nğŸ“ˆ ìµœê³  ì„±ëŠ¥ ëŒ€ë¹„ ë¹„êµ:")
    for test_name, data in successful_tests:
        ratio = data["ì†ë„"] / best_speed if best_speed > 0 else 0
        improvement = (1 - ratio) * 100 if ratio < 1 else (ratio - 1) * 100
        comparison = (
            f"{improvement:.1f}% {'ëŠë¦¼' if ratio < 1 else 'ë¹ ë¦„'}"
            if ratio != 1
            else "ê¸°ì¤€"
        )
        print(f"   {test_name}: {comparison} ({ratio:.2f}x)")

    # ì „ì²´ ë°ì´í„°ì…‹ ì˜ˆìƒ ì‹œê°„
    print(f"\nğŸ”® ì „ì²´ {total_docs}ê°œ ì²˜ë¦¬ ì˜ˆìƒ ì‹œê°„:")
    for test_name, data in successful_tests:
        if data["ì†ë„"] > 0:
            estimated_seconds = total_docs / data["ì†ë„"]
            estimated_minutes = estimated_seconds / 60
            print(
                f"   {test_name}: ~{estimated_seconds:.0f}ì´ˆ ({estimated_minutes:.1f}ë¶„)"
            )

    # ê¶Œì¥ ë°©ì‹ ì„ íƒ
    best_method = successful_tests[0][0]
    print(f"\nğŸ’¡ ê¶Œì¥ ë°©ì‹: {best_method}")
    print(
        f"   ì´ìœ : ê°€ì¥ ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ ({successful_tests[0][1]['ì†ë„']:.1f} chunks/s)"
    )

    return {"ê²°ê³¼": results, "ê¶Œì¥_ë°©ì‹": best_method, "ìµœê³ _ì†ë„": best_speed}


if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ ì„ íƒ ê°€ëŠ¥
    # VECTORSTORE_MODE=benchmark python build_rule_database.py
    if os.getenv("VECTORSTORE_MODE", "").lower() == "benchmark":
        print("ğŸ ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        print("   ìƒ˜í”Œ ë°ì´í„°ë¡œ ì„±ëŠ¥ ë¹„êµë¥¼ ìˆ˜í–‰í•œ í›„ ìµœì  ë°©ì‹ìœ¼ë¡œ ì „ì²´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    main()

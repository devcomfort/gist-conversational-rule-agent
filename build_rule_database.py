#!/usr/bin/env python3
"""
GIST Rules Database Builder
===========================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” GIST í•™ì¹™ ë° ê·œì • PDF íŒŒì¼ë“¤ì„ ì „ì²˜ë¦¬í•˜ì—¬
ì™„ì„±ëœ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python build_rule_database.py

ì¶œë ¥:
    - faiss_db/vectorstore.faiss (FAISS ì¸ë±ìŠ¤)
    - faiss_db/vectorstore.pkl (ë©”íƒ€ë°ì´í„°)
    - faiss_db/database_info.json (í†µê³„ ì •ë³´)
"""

import os
import json
import time
import glob
import fitz
import faiss
import numpy as np
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from dotenv import load_dotenv

# LangChain imports
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load environment variables
load_dotenv()

# Configuration
OUTPUT_DIR = Path("faiss_db")
DIMENSION = 384
PDF_PATTERN = "rules/**/*.pdf"

# Initialize embeddings model
print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
EMBED_MODEL = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

# Text splitter
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)


def create_output_directory():
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    OUTPUT_DIR.mkdir(exist_ok=True)
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR.absolute()}")


def scan_pdf_files():
    """ëª¨ë“  PDF íŒŒì¼ ìŠ¤ìº”"""
    print("ğŸ” PDF íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
    pdf_files = glob.glob(PDF_PATTERN, recursive=True)
    pdf_files.sort()

    print(f"ğŸ“„ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
    return pdf_files


def get_document_category(file_path: str) -> str:
    """íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
    path_lower = file_path.lower()

    if "í•™ì¹™" in path_lower or "ê·œì •" in path_lower:
        return "í•™ì¹™/ê·œì •"
    elif "ë§¤ë‰´ì–¼" in path_lower or "manual" in path_lower:
        return "ì‚¬ìš©ìë§¤ë‰´ì–¼"
    elif "ì§€ì¹¨" in path_lower:
        return "ìš´ì˜ì§€ì¹¨"
    elif "ê·œì¹™" in path_lower:
        return "ê´€ë¦¬ê·œì¹™"
    else:
        return "ê¸°íƒ€"


def extract_text_from_pdf(pdf_path: str) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = fitz.open(pdf_path)
        text_pages = []

        for page_num, page in enumerate(doc):
            try:
                page_text = page.get_text("text")
                if page_text.strip():
                    text_pages.append(page_text)
            except Exception as page_error:
                print(f"âš ï¸ í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {page_error}")
                continue

        doc.close()

        if text_pages:
            return "\n".join(text_pages)
        else:
            return ""

    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ ({pdf_path}): {e}")
        return ""


def process_pdfs(pdf_files):
    """ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬"""
    print("\nğŸ”„ PDF íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...")

    all_docs = []
    successfully_processed = 0
    failed_files = []

    start_time = time.time()

    for i, pdf_file in enumerate(pdf_files):
        print(
            f"\rğŸ“„ ì²˜ë¦¬ ì¤‘ [{i + 1}/{len(pdf_files)}]: {os.path.basename(pdf_file)}",
            end="",
            flush=True,
        )

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(pdf_file):
            print(f"\nâš ï¸ íŒŒì¼ ì—†ìŒ: {pdf_file}")
            failed_files.append(pdf_file)
            continue

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = extract_text_from_pdf(pdf_file)

        if text.strip():
            # íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ì²­í¬ í¬ê¸° ì¡°ì •
            file_size = os.path.getsize(pdf_file)
            is_large_file = file_size > 5 * 1024 * 1024  # 5MB ì´ìƒ

            if is_large_file:
                dynamic_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800, chunk_overlap=100
                )
            else:
                dynamic_splitter = TEXT_SPLITTER

            # ë¬¸ì„œ ìƒì„± ë° ë¶„í• 
            doc = Document(
                page_content=text,
                metadata={
                    "source": pdf_file,
                    "filename": os.path.basename(pdf_file),
                    "category": get_document_category(pdf_file),
                    "file_size": file_size,
                    "processed_at": datetime.now().isoformat(),
                },
            )

            docs = dynamic_splitter.split_documents([doc])
            all_docs.extend(docs)
            successfully_processed += 1

            file_size_mb = file_size / (1024 * 1024)
            print(
                f"\râœ… [{i + 1}/{len(pdf_files)}] {os.path.basename(pdf_file)} ({len(docs)} chunks, {file_size_mb:.1f}MB)"
            )

        else:
            print(f"\nâš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ: {pdf_file}")
            failed_files.append(pdf_file)

        # ë©”ëª¨ë¦¬ ì •ë¦¬ (20ê°œë§ˆë‹¤)
        if (i + 1) % 20 == 0:
            print(f"\rğŸ§¹ [{i + 1}/{len(pdf_files)}] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ    ")

    processing_time = time.time() - start_time

    print(f"\nâœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
    print(f"   ì„±ê³µ: {successfully_processed}/{len(pdf_files)} íŒŒì¼")
    print(f"   ì‹¤íŒ¨: {len(failed_files)} íŒŒì¼")
    print(f"   ì²­í¬: {len(all_docs)}ê°œ")
    print(f"   ì†Œìš”ì‹œê°„: {processing_time:.1f}ì´ˆ")

    return all_docs, successfully_processed, failed_files


def create_faiss_vectorstore(all_docs):
    """FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    print(f"\nğŸ”„ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘... ({len(all_docs)} ì²­í¬)")

    if not all_docs:
        raise ValueError("ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")

    start_time = time.time()

    # ë°°ì¹˜ í¬ê¸° ê²°ì •
    total_chunks = len(all_docs)
    if total_chunks <= 1000:
        batch_size = 100
    elif total_chunks <= 5000:
        batch_size = 200
    elif total_chunks <= 15000:
        batch_size = 300
    else:
        batch_size = 500

    num_batches = (total_chunks + batch_size - 1) // batch_size
    print(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬: {num_batches}ê°œ ë°°ì¹˜, ë°°ì¹˜ë‹¹ ~{batch_size}ê°œ ì²­í¬")

    vectorstore = None

    for batch_idx in range(num_batches):
        batch_start_time = time.time()
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_chunks)
        batch_docs = all_docs[start_idx:end_idx]

        print(
            f"\rğŸš€ ë°°ì¹˜ [{batch_idx + 1}/{num_batches}] ì²˜ë¦¬ ì¤‘... ({len(batch_docs)} chunks)",
            end="",
            flush=True,
        )

        try:
            if vectorstore is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                vectorstore = FAISS.from_documents(batch_docs, EMBED_MODEL)
            else:
                # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ë°°ì¹˜ ì¶”ê°€
                batch_vectorstore = FAISS.from_documents(batch_docs, EMBED_MODEL)
                vectorstore.merge_from(batch_vectorstore)
                del batch_vectorstore

            batch_time = time.time() - batch_start_time
            chunks_per_second = len(batch_docs) / batch_time if batch_time > 0 else 0
            print(
                f"\râœ… ë°°ì¹˜ [{batch_idx + 1}/{num_batches}] ì™„ë£Œ: {len(batch_docs)} chunks in {batch_time:.1f}s ({chunks_per_second:.1f} chunks/s)"
            )

        except Exception as e:
            print(f"\nâŒ ë°°ì¹˜ {batch_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    total_time = time.time() - start_time
    avg_speed = total_chunks / total_time if total_time > 0 else 0

    print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
    print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
    print(f"   í‰ê·  ì†ë„: {avg_speed:.1f} chunks/second")

    return vectorstore


def create_additional_indexes(vectorstore):
    """ì¶”ê°€ FAISS ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ ë¹„êµìš©)"""
    print("\nğŸ”§ ì¶”ê°€ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")

    try:
        # ê¸°ì¡´ ë²¡í„° ì¶”ì¶œ
        vectors = vectorstore.index.reconstruct_n(0, vectorstore.index.ntotal)
        results = {}

        # IndexIVFFlat
        if vectorstore.index.ntotal > 100:
            print("  ğŸ”¹ IndexIVFFlat ìƒì„±...")
            nlist = min(100, int(np.sqrt(vectorstore.index.ntotal)))
            quantizer = faiss.IndexFlatL2(DIMENSION)
            ivf_index = faiss.IndexIVFFlat(quantizer, DIMENSION, nlist)
            ivf_index.train(vectors)
            ivf_index.add(vectors)
            ivf_index.nprobe = min(10, nlist)

            faiss.write_index(ivf_index, str(OUTPUT_DIR / "vectorstore_ivf.faiss"))
            results["IndexIVFFlat"] = {"nlist": nlist, "nprobe": ivf_index.nprobe}

        # IndexHNSWFlat
        if vectorstore.index.ntotal > 50:
            print("  ğŸ”¹ IndexHNSWFlat ìƒì„±...")
            hnsw_index = faiss.IndexHNSWFlat(DIMENSION, 32)
            hnsw_index.hnsw.efConstruction = 200
            hnsw_index.hnsw.efSearch = 64
            hnsw_index.add(vectors)

            faiss.write_index(hnsw_index, str(OUTPUT_DIR / "vectorstore_hnsw.faiss"))
            results["IndexHNSWFlat"] = {"M": 32, "efConstruction": 200, "efSearch": 64}

        print(f"âœ… {len(results)}ê°œ ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        return results

    except Exception as e:
        print(f"âš ï¸ ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return {}


def save_vectorstore(vectorstore, additional_indexes, stats):
    """ë²¡í„°ìŠ¤í† ì–´ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
    print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")

    # FAISS ì¸ë±ìŠ¤ ì €ì¥
    vectorstore.save_local(str(OUTPUT_DIR))
    print(f"âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {OUTPUT_DIR}/")

    # í†µê³„ ì •ë³´ ì €ì¥
    database_info = {
        "created_at": datetime.now().isoformat(),
        "total_documents": stats["successfully_processed"],
        "total_chunks": len(vectorstore.docstore._dict),
        "failed_files": len(stats["failed_files"]),
        "processing_time_seconds": stats["processing_time"],
        "vectorization_time_seconds": stats.get("vectorization_time", 0),
        "dimension": DIMENSION,
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "additional_indexes": additional_indexes,
        "file_sizes": {
            "vectorstore.faiss": os.path.getsize(OUTPUT_DIR / "index.faiss"),
            "vectorstore.pkl": os.path.getsize(OUTPUT_DIR / "index.pkl"),
        },
    }

    with open(OUTPUT_DIR / "database_info.json", "w", encoding="utf-8") as f:
        json.dump(database_info, f, ensure_ascii=False, indent=2)

    print("âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: database_info.json")

    return database_info


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ GIST Rules Database Builder ì‹œì‘!")
    print("=" * 50)

    overall_start = time.time()

    try:
        # 1. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        create_output_directory()

        # 2. PDF íŒŒì¼ ìŠ¤ìº”
        pdf_files = scan_pdf_files()
        if not pdf_files:
            print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return

        # 3. PDF ì²˜ë¦¬
        all_docs, successfully_processed, failed_files = process_pdfs(pdf_files)

        if not all_docs:
            print("âŒ ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        # 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorization_start = time.time()
        vectorstore = create_faiss_vectorstore(all_docs)
        vectorization_time = time.time() - vectorization_start

        # 5. ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„±
        additional_indexes = create_additional_indexes(vectorstore)

        # 6. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        stats = {
            "successfully_processed": successfully_processed,
            "failed_files": failed_files,
            "processing_time": time.time() - overall_start,
            "vectorization_time": vectorization_time,
        }

        database_info = save_vectorstore(vectorstore, additional_indexes, stats)

        # 7. ìµœì¢… ê²°ê³¼ ì¶œë ¥
        total_time = time.time() - overall_start
        print("\n" + "=" * 50)
        print("ğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        print(f"ğŸ“ ìœ„ì¹˜: {OUTPUT_DIR.absolute()}")
        print(f"ğŸ“Š ì´ ë¬¸ì„œ: {successfully_processed}ê°œ")
        print(f"ğŸ“ ì´ ì²­í¬: {len(all_docs)}ê°œ")
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(
            f"ğŸ’¾ DB í¬ê¸°: {database_info['file_sizes']['vectorstore.faiss'] / (1024 * 1024):.1f}MB"
        )

        if failed_files:
            print(f"âš ï¸ ì‹¤íŒ¨ íŒŒì¼: {len(failed_files)}ê°œ")

        print("\nğŸš€ ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì•±ì„ ë¹ ë¥´ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   python app_gist_rules_analyzer_prebuilt.py")

    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()

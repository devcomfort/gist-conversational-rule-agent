"""
GIST Rules Analyzer - Prebuilt Database Version (LiteLLM Integrated)
==================================================================

ÏÇ¨Ï†Ñ Íµ¨Ï∂ïÎêú FAISS Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Î°úÎìúÌïòÎäî Í≥†ÏÜç ÏãúÏûë Î≤ÑÏ†ÑÏûÖÎãàÎã§.
LiteLLMÏùò ÏûêÎèô Î™®Îç∏ Í∞êÏßÄÏôÄ ÌôòÍ≤ΩÎ≥ÄÏàò Í∏∞Î∞ò API ÌÇ§ Í¥ÄÎ¶¨Î•º ÌôúÏö©Ìï©ÎãàÎã§.

üöÄ ÏÇ¨Ïö© Ï†Ñ ÏöîÍµ¨ÏÇ¨Ìï≠:
    python build_rule_database.py  # Î®ºÏ†Ä Ïã§Ìñâ

‚ú® Ï£ºÏöî ÌäπÏßï:
- ‚ö° 3Ï¥à ÎÇ¥ Ïï± ÏãúÏûë ÏôÑÎ£å
- ü§ñ LiteLLM ÏôÑÏ†Ñ ÌÜµÌï©ÏúºÎ°ú 15+ LLM ÌîÑÎ°úÎ∞îÏù¥Îçî ÏûêÎèô ÏßÄÏõê
- üéØ Dynamic Knee DetectionÏúºÎ°ú Ï†ÅÏùëÌòï Î¨∏ÏÑú ÏÑ†ÌÉù
- üì° Ïã§ÏãúÍ∞Ñ Ïä§Ìä∏Î¶¨Î∞ç ÏùëÎãµ
- üìÑ Ï∂îÍ∞Ä PDF ÏóÖÎ°úÎìú ÏßÄÏõê (ÏÑ†ÌÉùÏ†Å)
- üîë ÌôòÍ≤ΩÎ≥ÄÏàò Í∏∞Î∞ò ÏûêÎèô API ÌÇ§ Í¥ÄÎ¶¨

üåê ÏßÄÏõêÌïòÎäî LLM ÌîÑÎ°úÎ∞îÏù¥Îçî (ÌôòÍ≤ΩÎ≥ÄÏàòÎßå ÏÑ§Ï†ïÌïòÎ©¥ ÏûêÎèô Í∞êÏßÄ):
- OpenAI: OPENAI_API_KEY
- Anthropic: ANTHROPIC_API_KEY
- Google: GOOGLE_API_KEY (Vertex AI, Gemini, Palm)
- Azure: AZURE_API_KEY
- Fireworks AI: FIREWORKS_AI_API_KEY ‚≠ê (Í∏∞Î≥∏ Î™®Îç∏)
- Together AI: TOGETHER_AI_API_KEY
- Groq: GROQ_API_KEY
- Cohere: COHERE_API_KEY
- DeepSeek: DEEPSEEK_API_KEY
- Perplexity: PERPLEXITY_API_KEY
- Replicate: REPLICATE_API_TOKEN
- HuggingFace: HUGGINGFACE_API_KEY
- Novita AI: NOVITA_API_KEY
- Í∏∞ÌÉÄ LiteLLM ÏßÄÏõê ÌîÑÎ°úÎ∞îÏù¥Îçî

üìñ ÏÇ¨Ïö©Î≤ï:
1. ÏõêÌïòÎäî ÌîÑÎ°úÎ∞îÏù¥ÎçîÏùò API ÌÇ§Î•º ÌôòÍ≤ΩÎ≥ÄÏàòÏóê ÏÑ§Ï†ï
2. Ïï±ÏùÑ Ïã§ÌñâÌïòÎ©¥ ÏûêÎèôÏúºÎ°ú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Îì§ Í∞êÏßÄ
3. UIÏóêÏÑú Î™®Îç∏ ÏÑ†ÌÉùÌïòÏó¨ ÏÇ¨Ïö©
"""

import gradio as gr
import os
import json
import time
import hashlib
import html
import threading
import fitz
import litellm
import pystache
from pathlib import Path
from collections import OrderedDict
from datetime import datetime
from dotenv import load_dotenv
from litellm import get_valid_models
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Cross-encoder reranking Ï†úÍ±∞ - pure kneed optimizationÎßå ÏÇ¨Ïö©
from langchain_core.retrievers import BaseRetriever
from kneed import KneeLocator
from typing import Dict, Generator, List, Optional
from pydantic import Field
import matplotlib.pyplot as plt
import matplotlib
import base64
from io import BytesIO

# matplotlib Î∞±ÏóîÎìú ÏÑ§Ï†ï (ÏÑúÎ≤Ñ ÌôòÍ≤Ω ÎåÄÏùë)
matplotlib.use("Agg")

# Environment variables - LiteLLMÏù¥ ÏûêÎèôÏúºÎ°ú Í∞êÏßÄÌïòÎØÄÎ°ú ÏµúÏÜåÌôî
load_dotenv()
# LiteLLMÏùÄ Îã§Ïùå ÌôòÍ≤ΩÎ≥ÄÏàòÎì§ÏùÑ ÏûêÎèôÏúºÎ°ú Í∞êÏßÄÌïòÍ≥† ÏÇ¨Ïö©:
# OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, GROQ_API_KEY,
# TOGETHER_AI_API_KEY, DEEPSEEK_API_KEY, PERPLEXITY_API_KEY,
# REPLICATE_API_TOKEN, HUGGINGFACE_API_KEY, NOVITA_API_KEY,
# FIREWORKS_AI_API_KEY Îì±

# Configuration
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

# Template paths
LEGAL_SYSTEM_PROMPT_PATH = Path("system_prompts/legal_agent_system_prompt.mustache")
LEGAL_QUERY_TEMPLATE_PATH = Path("templates/legal_query_template.mustache")

# ÏßÄÏõêÌïòÎäî ÏûÑÎ≤†Îî© Î™®Îç∏ ÏÑ§Ï†ï (TODO.txt Í∏∞Î∞ò - build_multi_embedding_databases.pyÏôÄ ÎèôÏùº)
EMBEDDING_MODELS = {
    "Qwen/Qwen3-Embedding-0.6B": {
        "model_name": "Qwen/Qwen3-Embedding-0.6B",
        "db_name": "faiss_qwen3_embedding_0.6b",
        "dimension": 1024,
        "mteb_rank": 3,
        "description": "Qwen3 Embedding 0.6B - MTEB 3ÏúÑ",
    },
    "jinaai/jina-embeddings-v3": {
        "model_name": "jinaai/jina-embeddings-v3",
        "db_name": "faiss_jina_embeddings_v3",
        "dimension": 1024,
        "mteb_rank": 22,
        "description": "Jina Embeddings v3 - MTEB 22ÏúÑ",
    },
    "BAAI/bge-m3": {
        "model_name": "BAAI/bge-m3",
        "db_name": "faiss_bge_m3",
        "dimension": 1024,
        "mteb_rank": 23,
        "description": "BGE M3 - MTEB 23ÏúÑ",
    },
    "sentence-transformers/all-MiniLM-L6-v2": {
        "model_name": "sentence-transformers/all-MiniLM-L6-v2",
        "db_name": "faiss_all_minilm_l6_v2",
        "dimension": 384,
        "mteb_rank": 117,
        "description": "All MiniLM L6 v2 - MTEB 117ÏúÑ (Í∏∞Ï°¥ Í∏∞Î≥∏ Î™®Îç∏)",
    },
}

# Í∏∞Î≥∏ ÏûÑÎ≤†Îî© Î™®Îç∏
DEFAULT_EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# Î†àÍ±∞Ïãú ÏßÄÏõêÏùÑ ÏúÑÌïú Í∏∞Î≥∏ DB Í≤ΩÎ°ú
LEGACY_DB_PATH = Path("faiss_db")

# Default model configuration
DEFAULT_MODEL_ID = "fireworks_ai/accounts/fireworks/models/gpt-oss-20b"


def _detect_provider(model_id: str) -> str:
    """LiteLLMÏù¥ ÏßÄÏõêÌïòÎäî ÌîÑÎ°úÎ∞îÏù¥ÎçîÎì§ÏùÑ Í∞êÏßÄ"""
    # Anthropic
    if model_id.startswith(("claude", "anthropic/")):
        return "anthropic"

    # Google providers
    if model_id.startswith(("vertex_ai/", "gemini")):
        return "vertex_ai"
    if model_id.startswith("palm/"):
        return "palm"

    # Azure
    if model_id.startswith("azure/"):
        return "azure"

    # AWS
    if model_id.startswith("bedrock/"):
        return "bedrock"
    if model_id.startswith("sagemaker/"):
        return "sagemaker"

    # Specialized providers
    if model_id.startswith("novita/"):
        return "novita"
    if model_id.startswith("fireworks_ai/"):
        return "fireworks_ai"
    if model_id.startswith("together_ai/"):
        return "together_ai"
    if model_id.startswith("groq/"):
        return "groq"
    if model_id.startswith("cohere/"):
        return "cohere"
    if model_id.startswith("deepseek/"):
        return "deepseek"
    if model_id.startswith("perplexity/"):
        return "perplexity"
    if model_id.startswith("ollama/"):
        return "ollama"
    if model_id.startswith("replicate/"):
        return "replicate"
    if model_id.startswith("huggingface/"):
        return "huggingface"

    # Default to OpenAI for unrecognized patterns
    return "openai"


def _load_dynamic_models() -> Dict[str, Dict[str, str]]:
    """LiteLLM get_valid_models()Î•º ÌôúÏö©Ìïú ÎèôÏ†Å Î™®Îç∏ Î°úÎî©"""

    print("üîç LiteLLMÏùÑ ÌÜµÌï¥ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Îì§ÏùÑ Í≤ÄÏÉâ Ï§ë...")

    try:
        # LiteLLMÏù¥ ÌôòÍ≤ΩÎ≥ÄÏàòÎ•º ÏûêÎèôÏúºÎ°ú ÌôïÏù∏ÌïòÏó¨ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Îì§ Î∞òÌôò
        model_ids = get_valid_models(check_provider_endpoint=True)
        print(f"‚úÖ {len(model_ids)}Í∞úÏùò ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ÏùÑ Î∞úÍ≤¨ÌñàÏäµÎãàÎã§")
    except Exception as e:
        print(f"‚ö†Ô∏è Î™®Îç∏ Í≤ÄÏÉâ Ï§ë Ïò§Î•ò: {e}")
        print("üìã Í∏∞Î≥∏ Î™®Îç∏ Î™©Î°ùÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§")
        model_ids = []

    dynamic: Dict[str, Dict[str, str]] = {}
    for mid in model_ids:
        dynamic[mid] = {"model_id": mid, "provider": _detect_provider(mid)}

    # Í∏∞Î≥∏ Î™®Îç∏Ïù¥ Î™©Î°ùÏóê ÏóÜÎäî Í≤ΩÏö∞ Ï∂îÍ∞Ä (ÌôòÍ≤ΩÎ≥ÄÏàòÍ∞Ä ÏûàÎã§Í≥† Í∞ÄÏ†ï)
    if DEFAULT_MODEL_ID not in dynamic:
        dynamic[DEFAULT_MODEL_ID] = {
            "model_id": DEFAULT_MODEL_ID,
            "provider": "fireworks_ai",
        }
        print(f"‚ûï Í∏∞Î≥∏ Î™®Îç∏ Ï∂îÍ∞Ä: {DEFAULT_MODEL_ID}")

    # ÏµúÏÜåÌïú ÌïòÎÇòÏùò Î™®Îç∏Ïù¥ ÏûàÏñ¥Ïïº Ìï®
    if not dynamic:
        # Ìè¥Î∞±ÏúºÎ°ú ÏùºÎ∞òÏ†ÅÏù∏ Î™®Îç∏Îì§ Ï∂îÍ∞Ä
        fallback_models = [
            {"model_id": "gpt-4o-mini", "provider": "openai"},
            {"model_id": "claude-3-haiku-20240307", "provider": "anthropic"},
            {"model_id": DEFAULT_MODEL_ID, "provider": "fireworks_ai"},
        ]
        for model in fallback_models:
            dynamic[model["model_id"]] = model
        print("üìã Ìè¥Î∞± Î™®Îç∏ Î™©Î°ùÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§")

    print(f"üéØ Ï¥ù {len(dynamic)}Í∞ú Î™®Îç∏ Ï§ÄÎπÑ ÏôÑÎ£å")
    return dynamic


# Models setup (ÎèôÏ†Å Î°úÎî©)
MODELS = _load_dynamic_models()

# Kneed Sensitivity Testing Options (Í≥µÏãù Î¨∏ÏÑú Í∏∞Î∞ò Ïò¨Î∞îÎ•∏ Ï†ïÏùò)
# ÏûëÏùÄ S = Îπ†Î•∏ knee Í∞êÏßÄ = Ï†ÅÏùÄ Î¨∏ÏÑú ÏÑ†ÌÉù = Ï†ÅÍ∑πÏ†Å
# ÌÅ∞ S = Î≥¥ÏàòÏ†Å Í∞êÏßÄ = ÎßéÏùÄ Î¨∏ÏÑú ÏÑ†ÌÉù = Î≥¥ÏàòÏ†Å
SENSITIVITY_OPTIONS = {
    "Ï†ÅÍ∑πÏ†Å ÏÑ†ÌÉù (S=1)": {
        "sensitivity": 1,
        "description": "Îπ†Î•∏ knee Í∞êÏßÄ, Ï†ÅÏùÄ Î¨∏ÏÑú ÏÑ†ÌÉù",
    },
    "Í∑†Ìòï ÏÑ†ÌÉù (S=3)": {"sensitivity": 3, "description": "Ï§ëÍ∞Ñ Ï†ïÎèÑ Î¨∏ÏÑú ÏÑ†ÌÉù"},
    "ÌëúÏ§Ä ÏÑ†ÌÉù (S=5)": {"sensitivity": 5, "description": "ÌëúÏ§ÄÏ†Å Î¨∏ÏÑú ÏÑ†ÌÉù"},
    "Î≥¥ÏàòÏ†Å ÏÑ†ÌÉù (S=10)": {
        "sensitivity": 10,
        "description": "Ïã†Ï§ëÌïú Î¨∏ÏÑú ÏÑ†ÌÉù, ÎßéÏùÄ Î¨∏ÏÑú Ìè¨Ìï®",
    },
}

# Ï∂îÍ∞Ä Ïã§ÌóòÏùÑ ÏúÑÌïú Ï†ÑÏ≤¥ sensitivity Î≤îÏúÑ (Î¨∏ÏÑú ÏòàÏãú Í∏∞Î∞ò)
FULL_SENSITIVITY_RANGE = [1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 100]

# Initialize embeddings
EMBED_MODEL = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)


# Template loading functions
def load_legal_system_prompt() -> str:
    """Load legal agent system prompt from mustache template"""
    try:
        if LEGAL_SYSTEM_PROMPT_PATH.exists():
            with open(LEGAL_SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as f:
                template_content = f.read()

            # Legal system promptÎäî static templateÏù¥ÎØÄÎ°ú Î∞îÎ°ú Î∞òÌôò
            return template_content
        else:
            print(f"‚ö†Ô∏è Legal system prompt not found: {LEGAL_SYSTEM_PROMPT_PATH}")
            # Fallback to basic system prompt
            return """You are a GIST Rules and Regulations Expert Assistant. 
You have comprehensive knowledge of all GIST academic rules, regulations, guidelines, and policies.
Always provide accurate, detailed answers based on the provided context.
When answering questions about GIST rules, cite specific regulation numbers and titles when available."""
    except Exception as e:
        print(f"‚ùå Error loading legal system prompt: {e}")
        return "You are a GIST legal expert assistant."


def load_legal_query_template() -> str:
    """Load legal query template from mustache file"""
    try:
        if LEGAL_QUERY_TEMPLATE_PATH.exists():
            with open(LEGAL_QUERY_TEMPLATE_PATH, "r", encoding="utf-8") as f:
                return f.read()
        else:
            print(f"‚ö†Ô∏è Legal query template not found: {LEGAL_QUERY_TEMPLATE_PATH}")
            return "{{user_query}}\n\nContext:\n{{context}}"
    except Exception as e:
        print(f"‚ùå Error loading legal query template: {e}")
        return "{{user_query}}\n\nContext:\n{{context}}"


# Load templates
LEGAL_SYSTEM_PROMPT = load_legal_system_prompt()
LEGAL_QUERY_TEMPLATE = load_legal_query_template()

print("üìã Legal templates loaded successfully")


def render_legal_query(user_query: str, context_documents: List[Document]) -> str:
    """
    Legal query templateÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏÇ¨Ïö©Ïûê ÏøºÎ¶¨Î•º Î†åÎçîÎßÅ

    Args:
        user_query: ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏
        context_documents: Í≤ÄÏÉâÎêú Í¥ÄÎ†® Î¨∏ÏÑúÎì§

    Returns:
        Î†åÎçîÎßÅÎêú legal query Î¨∏ÏûêÏó¥
    """
    try:
        # ÌòÑÏû¨ ÏãúÍ∞Ñ
        current_time = datetime.now()

        # Î¨∏ÏÑúÎì§ÏùÑ ÌÖúÌîåÎ¶ø ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
        template_docs = []
        for idx, doc in enumerate(context_documents):
            doc_data = {
                "index": idx + 1,
                "source": doc.metadata.get(
                    "source", doc.metadata.get("filename", "Unknown")
                ),
                "category": doc.metadata.get("category", ""),
                "page_content": doc.page_content,
                "metadata": doc.metadata,
            }

            # priority ÏÑ§Ï†ï (Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Ïö∞ÏÑ†ÏàúÏúÑ)
            if doc.metadata.get("category") == "ÌïôÏπô":
                doc_data["priority"] = "High (ÌïôÏπô)"
            elif doc.metadata.get("category") in ["Í∑úÏ†ï", "ÏãúÌñâÏÑ∏Ïπô"]:
                doc_data["priority"] = "Medium (Í∑úÏ†ï/ÏãúÌñâÏÑ∏Ïπô)"
            else:
                doc_data["priority"] = "Normal"

            template_docs.append(doc_data)

        # Î≥µÏàò Î≤ïÎ†πÏù¥ Í¥ÄÎ†®Îêú Í≤ΩÏö∞ Ï≤¥ÌÅ¨ (2Í∞ú Ïù¥ÏÉÅÏùò ÏÑúÎ°ú Îã§Î•∏ source)
        unique_sources = set(doc.get("source", "") for doc in template_docs)
        has_multiple_regulations = len(unique_sources) > 1

        # ÌÖúÌîåÎ¶ø Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
        template_data = {
            "user_query": user_query,
            "query_timestamp": current_time.strftime("%Y-%m-%d %H:%M:%S"),
            "current_datetime": current_time.strftime("%Y-%m-%d %H:%M:%S"),
            "document_count": len(context_documents),
            "context_documents": template_docs,
            "multiple_regulations": has_multiple_regulations,
        }

        # Mustache template Î†åÎçîÎßÅ
        renderer = pystache.Renderer()
        rendered_query = renderer.render(LEGAL_QUERY_TEMPLATE, template_data)

        return rendered_query

    except Exception as e:
        print(f"‚ùå Legal query rendering failed: {e}")
        # Fallback: Í∞ÑÎã®Ìïú ÌòïÏãù
        context_text = "\n\n".join(
            [
                f"Document {i + 1}: {doc.page_content}"
                for i, doc in enumerate(context_documents)
            ]
        )
        return f"User Query: {user_query}\n\nContext Documents:\n{context_text}"


# --------- (A) GLOBAL STATE ---------


# Session variables
MAX_SESSIONS = 100
sessions = OrderedDict()
session_lock = threading.Lock()

# Global shared state
shared_state: Dict = {
    "current_model": None,
    "vectorstore": None,
    "database_loaded": False,
    "database_info": {},
    "last_retrievers": {},  # rerank_methodÎ≥Ñ ÎßàÏßÄÎßâ ÏÇ¨Ïö©Îêú retriever Ï†ÄÏû•
}
shared_state_lock = threading.Lock()


def get_embedding_model(model_key: str = None):
    """ÏûÑÎ≤†Îî© Î™®Îç∏ÏùÑ ÎèôÏ†ÅÏúºÎ°ú Î°úÎìúÌïòÍ≥† Î∞òÌôò"""
    if model_key is None:
        model_key = shared_state["current_embedding_model"]

    if model_key not in EMBEDDING_MODELS:
        print(f"‚ö†Ô∏è ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÏûÑÎ≤†Îî© Î™®Îç∏: {model_key}")
        model_key = DEFAULT_EMBEDDING_MODEL

    model_config = EMBEDDING_MODELS[model_key]

    try:
        print(f"üîß ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎî© Ï§ë: {model_config['description']}")
        embed_model = HuggingFaceEmbeddings(
            model_name=model_config["model_name"],
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True},
        )
        print(f"‚úÖ ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {model_config['description']}")
        return embed_model
    except Exception as e:
        print(f"‚ùå ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
        # Í∏∞Î≥∏ Î™®Îç∏Î°ú Ìè¥Î∞±
        if model_key != DEFAULT_EMBEDDING_MODEL:
            print(
                f"üîÑ Í∏∞Î≥∏ Î™®Îç∏Î°ú Ìè¥Î∞±: {EMBEDDING_MODELS[DEFAULT_EMBEDDING_MODEL]['description']}"
            )
            return get_embedding_model(DEFAULT_EMBEDDING_MODEL)
        else:
            raise e


def get_database_path(embedding_model_key: str = None) -> Path:
    """ÏûÑÎ≤†Îî© Î™®Îç∏Ïóê Îî∞Î•∏ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ΩÎ°ú Î∞òÌôò"""
    if embedding_model_key is None:
        embedding_model_key = shared_state["current_embedding_model"]

    if embedding_model_key in EMBEDDING_MODELS:
        db_name = EMBEDDING_MODELS[embedding_model_key]["db_name"]
        db_path = Path(db_name)

        # Î©ÄÌã∞ ÏûÑÎ≤†Îî© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Í∞Ä Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏
        if db_path.exists() and (db_path / "index.faiss").exists():
            return db_path

    # Ìè¥Î∞±: Î†àÍ±∞Ïãú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÇ¨Ïö©
    if LEGACY_DB_PATH.exists() and (LEGACY_DB_PATH / "index.faiss").exists():
        print(f"‚ö†Ô∏è Î†àÍ±∞Ïãú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÇ¨Ïö©: {LEGACY_DB_PATH}")
        return LEGACY_DB_PATH

    # Î©ÄÌã∞ ÏûÑÎ≤†Îî© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ΩÎ°ú Î∞òÌôò (Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏïÑÎèÑ)
    return Path(EMBEDDING_MODELS[embedding_model_key]["db_name"])


def load_database_for_embedding_model(embedding_model_key: str = None):
    """ÌäπÏ†ï ÏûÑÎ≤†Îî© Î™®Îç∏Ïóê ÎåÄÌïú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú"""
    if embedding_model_key is None:
        embedding_model_key = shared_state["current_embedding_model"]

    db_path = get_database_path(embedding_model_key)
    model_config = EMBEDDING_MODELS[embedding_model_key]

    print(f"üîç Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ÄÏÉâ Ï§ë: {model_config['description']}")
    print(f"üìÅ Í≤ΩÎ°ú: {db_path}")

    if not db_path.exists():
        print(f"‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ΩÎ°úÍ∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {db_path}")
        print("üõ†Ô∏è Îã§Ïùå Î™ÖÎ†πÏñ¥Î°ú Î©ÄÌã∞ ÏûÑÎ≤†Îî© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Íµ¨Ï∂ïÌïòÏÑ∏Ïöî:")
        print(
            f"   python build_multi_embedding_databases.py --model '{embedding_model_key}'"
        )
        return False

    if not (db_path / "index.faiss").exists() or not (db_path / "index.pkl").exists():
        print("‚ùå FAISS Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§!")
        print("üõ†Ô∏è Îã§Ïùå Î™ÖÎ†πÏñ¥Î°ú Î©ÄÌã∞ ÏûÑÎ≤†Îî© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Íµ¨Ï∂ïÌïòÏÑ∏Ïöî:")
        print(
            f"   python build_multi_embedding_databases.py --model '{embedding_model_key}'"
        )
        return False

    try:
        # ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎìú
        embed_model = get_embedding_model(embedding_model_key)

        # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú
        info_file = db_path / "database_info.json"
        if info_file.exists():
            with open(info_file, "r", encoding="utf-8") as f:
                database_info = json.load(f)
                shared_state["database_info"] = database_info
                print(
                    f"üìä Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ïÎ≥¥: {database_info['total_documents']}Í∞ú Î¨∏ÏÑú, {database_info['total_chunks']}Í∞ú Ï≤≠ÌÅ¨"
                )

        # FAISS Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥ Î°úÎìú
        print("üîÑ FAISS Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥ Î°úÎìú Ï§ë...")
        vectorstore = FAISS.load_local(
            str(db_path), embed_model, allow_dangerous_deserialization=True
        )

        with shared_state_lock:
            shared_state["vectorstore"] = vectorstore
            shared_state["embed_model"] = embed_model
            shared_state["current_embedding_model"] = embedding_model_key
            shared_state["database_loaded"] = True

        print(f"‚úÖ {model_config['description']} Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú ÏôÑÎ£å!")
        return True

    except Exception as e:
        print(f"‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú Ïã§Ìå®: {e}")
        return False


def load_existing_database():
    """Í∏∞Ï°¥ FAISS Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú (Î©ÄÌã∞ ÏûÑÎ≤†Îî© Î™®Îç∏ ÏßÄÏõê)"""
    return load_database_for_embedding_model(DEFAULT_EMBEDDING_MODEL)


# --------- (B) SESSION SETUP ---------
def get_session_id(request: gr.Request):
    raw_id = request.client.host + str(request.headers.get("user-agent"))
    return hashlib.md5(raw_id.encode()).hexdigest()


def init_session(session_id: str):
    sessions[session_id] = {
        "client": None,
        "history": {method: [] for method in SENSITIVITY_OPTIONS.keys()},
    }


def get_client(model_name: str):
    """LiteLLM ÏûêÎèô ÏÑ§Ï†ï Î∞òÌôò - ÌôòÍ≤ΩÎ≥ÄÏàò Í∏∞Î∞ò ÏûêÎèô Í∞êÏßÄ"""
    model_info = MODELS[model_name]

    # LiteLLMÏù¥ ÌôòÍ≤ΩÎ≥ÄÏàòÏóêÏÑú ÏûêÎèôÏúºÎ°ú API ÌÇ§Î•º Ï∞æÏïÑÏÑú ÏÇ¨Ïö©
    # ÏàòÎèô ÏÑ§Ï†ï ÏµúÏÜåÌôîÌïòÍ≥† LiteLLMÏùò ÏûêÎèô Í∞êÏßÄ Í∏∞Îä• ÌôúÏö©
    config = {"type": "litellm", "model_id": model_info["model_id"]}

    return config


# --------- (C) DYNAMIC KNEE RETRIEVER CLASS ---------
class DynamicKneeRetriever(BaseRetriever):
    """
    Knee Point DetectionÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎèôÏ†ÅÏúºÎ°ú Í¥ÄÎ†® Î¨∏ÏÑú Í∞úÏàòÎ•º Í≤∞Ï†ïÌïòÎäî Retriever

    Í≥†Ï†ïÎêú kÍ∞ú ÎåÄÏã† Ïú†ÏÇ¨ÎèÑ Í≥°ÏÑ†Ïùò knee pointÎ•º Ï∞æÏïÑÏÑú
    ÏûêÏó∞Ïä§Îü¨Ïö¥ cutoff ÏßÄÏ†êÍπåÏßÄÏùò Î™®Îì† Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Î∞òÌôòÌï©ÎãàÎã§.
    """

    # Pydantic v2 Î™®Îç∏ ÌïÑÎìú Ï†ïÏùò
    vectorstore: FAISS
    min_docs: int = 2
    max_docs: int = 30  # Î≥¥ÏàòÏ†Å ÏÑ§Ï†ïÏùÑ ÏúÑÌï¥ Ï¶ùÍ∞Ä
    sensitivity: float = 5.0  # ÌëúÏ§Ä ÏÑ†ÌÉù (Î¨∏ÏÑú Í∏∞Î∞ò)
    direction: str = "decreasing"
    curve: str = "convex"
    last_knee_info: Dict = Field(default_factory=dict)

    # Pydantic Î™®Îç∏ Íµ¨ÏÑ±ÏùÑ ÏúÑÌïú ÏÑ§Ï†ï
    class Config:
        arbitrary_types_allowed = True

    def __init__(
        self,
        vectorstore: FAISS,
        min_docs: int = 2,
        max_docs: int = 30,
        sensitivity: float = 5.0,
        direction: str = "decreasing",
        curve: str = "convex",
        **data,
    ):
        """
        Args:
            vectorstore: FAISS Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥
            min_docs: ÏµúÏÜå Î∞òÌôò Î¨∏ÏÑú Ïàò
            max_docs: ÏµúÎåÄ Í≤ÄÏÉâ Î¨∏ÏÑú Ïàò (knee Ï∞æÍ∏∞Ïö©)
            sensitivity: knee detection ÎØºÍ∞êÎèÑ (Í∏∞Î≥∏ 5.0, ÏûëÏùÑÏàòÎ°ù Ï†ÅÍ∑πÏ†Å, ÌÅ¥ÏàòÎ°ù Î≥¥ÏàòÏ†Å)
            direction: "decreasing" (Í±∞Î¶¨ Í∏∞Ï§Ä) ÎòêÎäî "increasing" (Ïú†ÏÇ¨ÎèÑ Í∏∞Ï§Ä)
            curve: "convex" ÎòêÎäî "concave"
        """
        # Pydantic v2 Ï¥àÍ∏∞Ìôî
        super().__init__(
            vectorstore=vectorstore,
            min_docs=min_docs,
            max_docs=max_docs,
            sensitivity=sensitivity,
            direction=direction,
            curve=curve,
            **data,
        )

    def _get_relevant_documents(self, query: str, **kwargs) -> List[Document]:
        """ÏøºÎ¶¨Ïóê ÎåÄÌï¥ knee point Í∏∞Î∞òÏúºÎ°ú Í¥ÄÎ†® Î¨∏ÏÑúÎì§ÏùÑ Î∞òÌôò"""
        try:
            # 1. ÏµúÎåÄ Í∞úÏàòÎ°ú Î¨∏ÏÑúÏôÄ Ï†êÏàò Í∞ÄÏ†∏Ïò§Í∏∞
            docs_and_scores = self.vectorstore.similarity_search_with_score(
                query, k=self.max_docs
            )

            if len(docs_and_scores) <= self.min_docs:
                # Î¨∏ÏÑúÍ∞Ä ÎÑàÎ¨¥ Ï†ÅÏúºÎ©¥ Î™®Îì† Î¨∏ÏÑú Î∞òÌôò
                self.last_knee_info = {
                    "total_docs": len(docs_and_scores),
                    "selected_docs": len(docs_and_scores),
                    "knee_point": None,
                    "reason": "Too few documents",
                }
                return [doc for doc, _ in docs_and_scores]

            # 2. Ï†êÏàò Ï∂îÏ∂ú Î∞è Ï†ïÎ†¨ (FAISSÎäî Í±∞Î¶¨Î•º Î∞òÌôòÌïòÎØÄÎ°ú ÏûëÏùÑÏàòÎ°ù Ïú†ÏÇ¨)
            scores = [score for _, score in docs_and_scores]
            documents = [doc for doc, _ in docs_and_scores]

            # 3. Knee point Ï∞æÍ∏∞
            knee_idx = self._find_knee_point(scores)

            # 4. ÏµúÏ¢Ö Î¨∏ÏÑú ÏÑ†ÌÉù
            if knee_idx is None or knee_idx < self.min_docs:
                # KneeÎ•º Ï∞æÏßÄ Î™ªÌñàÍ±∞ÎÇò ÎÑàÎ¨¥ Ï†ÅÏúºÎ©¥ ÏµúÏÜå Í∞úÏàò Î∞òÌôò
                selected_docs = documents[: self.min_docs]
                knee_reason = "No clear knee found, using min_docs"
            else:
                # Knee pointÍπåÏßÄ ÏÑ†ÌÉù (inclusive)
                selected_docs = documents[: knee_idx + 1]
                knee_reason = f"Knee point detected at index {knee_idx}"

            # 5. Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû• (ÏãúÍ∞ÅÌôîÎ•º ÏúÑÌï¥ Ï†ÑÏ≤¥ Ï†êÏàò Ï†ÄÏû•)
            self.last_knee_info = {
                "total_docs": len(docs_and_scores),
                "selected_docs": len(selected_docs),
                "knee_point": knee_idx,
                "scores": scores,  # ÏãúÍ∞ÅÌôîÎ•º ÏúÑÌï¥ Î™®Îì† Ï†êÏàò Ï†ÄÏû•
                "scores_preview": scores[:10],  # Î°úÍ∑∏Ïö© Ï≤òÏùå 10Í∞ú
                "selected_scores": [
                    score for _, score in docs_and_scores[: len(selected_docs)]
                ],
                "reason": knee_reason,
                "sensitivity": self.sensitivity,
            }

            print(
                f"üîç Dynamic Retrieval: {len(selected_docs)}/{len(docs_and_scores)} docs selected (knee at {knee_idx})"
            )
            return selected_docs

        except Exception as e:
            print(f"‚ùå DynamicKneeRetriever error: {e}")
            # ÏóêÎü¨ Ïãú fallbackÏúºÎ°ú Í∏∞Î≥∏ Í≤ÄÏÉâ
            return self.vectorstore.similarity_search(query, k=self.min_docs)

    def _find_knee_point(self, scores: List[float]) -> Optional[int]:
        """Ï†êÏàò Î¶¨Ïä§Ìä∏ÏóêÏÑú knee point Ï∞æÍ∏∞"""
        if len(scores) < 3:  # ÏµúÏÜå 3Í∞úÎäî ÏûàÏñ¥Ïïº knee Ï∞æÍ∏∞ Í∞ÄÎä•
            return None

        try:
            # xÏ∂ïÏùÄ Î¨∏ÏÑú Ïù∏Îç±Ïä§, yÏ∂ïÏùÄ Í±∞Î¶¨/Ï†êÏàò
            x = list(range(len(scores)))
            y = scores

            # KneeLocatorÎ°ú knee point Ï∞æÍ∏∞
            kl = KneeLocator(
                x=x,
                y=y,
                curve=self.curve,
                direction=self.direction,
                S=self.sensitivity,
                online=True,  # Ïò®ÎùºÏù∏ Î™®ÎìúÎ°ú Îçî Ï†ïÌôïÌïú ÌÉêÏßÄ
            )

            return kl.knee

        except Exception as e:
            print(f"‚ö†Ô∏è Knee detection failed: {e}")
            return None

    def get_knee_info(self) -> Dict:
        """ÎßàÏßÄÎßâ knee Î∂ÑÏÑù Í≤∞Í≥º Î∞òÌôò"""
        return self.last_knee_info.copy()

    def visualize_knee_detection(self, save_path: Optional[str] = None) -> str:
        """
        Knee Point Detection Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌïòÍ≥† base64 Ïù∏ÏΩîÎî©Îêú Ïù¥ÎØ∏ÏßÄ Î∞òÌôò

        Returns:
            base64Î°ú Ïù∏ÏΩîÎî©Îêú PNG Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ ÎòêÎäî ÏóêÎü¨ Î©îÏãúÏßÄ
        """
        if not self.last_knee_info:
            return "‚ö†Ô∏è No knee detection data available. Please run a query first."

        try:
            scores = self.last_knee_info.get("scores", [])
            knee_point = self.last_knee_info.get("knee_point")
            selected_docs = self.last_knee_info.get("selected_docs", 0)
            total_docs = self.last_knee_info.get("total_docs", len(scores))
            reason = self.last_knee_info.get("reason", "Unknown")
            sensitivity = self.last_knee_info.get("sensitivity", 1.0)

            if not scores:
                return "‚ö†Ô∏è No similarity scores available for visualization."

            # ÏòÅÏñ¥ Ìè∞Ìä∏Îßå ÏÇ¨Ïö© (ÌïúÍ∏Ä Ìè∞Ìä∏ Ïò§Î•ò Î∞©ÏßÄ)
            plt.rcParams["font.family"] = ["DejaVu Sans", "Arial", "Liberation Sans"]
            plt.rcParams["axes.unicode_minus"] = False

            # ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
            fig.suptitle(
                f"Dynamic Knee Point Detection Analysis\nSensitivity: {sensitivity} | Selected: {selected_docs}/{total_docs} docs",
                fontsize=14,
                fontweight="bold",
            )

            # Î¨∏ÏÑú Ïù∏Îç±Ïä§ (xÏ∂ï)
            x = list(range(len(scores)))

            # ÏÉÅÏúÑ ÌîåÎ°Ø: Ï†ÑÏ≤¥ Ïú†ÏÇ¨ÎèÑ Í≥°ÏÑ†
            ax1.plot(
                x,
                scores,
                "b-o",
                linewidth=2,
                markersize=5,
                label="Document Similarity Distance",
                alpha=0.7,
            )

            # Knee point ÌëúÏãú
            if knee_point is not None and knee_point < len(scores):
                ax1.axvline(
                    x=knee_point,
                    color="red",
                    linestyle="--",
                    linewidth=2,
                    label=f"Knee Point (idx={knee_point})",
                )
                ax1.plot(
                    knee_point,
                    scores[knee_point],
                    "ro",
                    markersize=10,
                    label=f"Knee: {scores[knee_point]:.4f}",
                )

            # ÏÑ†ÌÉùÎêú Î¨∏ÏÑú ÏòÅÏó≠ ÌëúÏãú
            if selected_docs > 0:
                selected_x = x[:selected_docs]
                selected_scores = scores[:selected_docs]
                ax1.fill_between(
                    selected_x,
                    0,
                    selected_scores,
                    alpha=0.3,
                    color="green",
                    label=f"Selected Documents ({selected_docs})",
                )

            ax1.set_xlabel("Document Index (ranked by similarity)")
            ax1.set_ylabel("Similarity Distance (lower = more similar)")
            ax1.set_title(f"Document Similarity Curve\nReason: {reason}")
            ax1.legend(loc="upper right")
            ax1.grid(True, alpha=0.3)

            # ÌïòÏúÑ ÌîåÎ°Ø: Knee Detection ÏÑ∏Î∂ÄÏÇ¨Ìï≠ (ÌôïÎåÄ)
            if knee_point is not None and len(scores) > 3:
                # Knee Ï£ºÎ≥Ä Îç∞Ïù¥ÌÑ∞ ÌôïÎåÄ ÌëúÏãú
                start_idx = max(0, knee_point - 3)
                end_idx = min(len(scores), knee_point + 4)
                zoom_x = x[start_idx:end_idx]
                zoom_scores = scores[start_idx:end_idx]

                ax2.plot(
                    zoom_x, zoom_scores, "b-o", linewidth=3, markersize=8, alpha=0.8
                )
                ax2.axvline(x=knee_point, color="red", linestyle="--", linewidth=2)
                ax2.plot(knee_point, scores[knee_point], "ro", markersize=12)

                # Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏ ÎùºÎ≤®ÎßÅ
                for i, (xi, yi) in enumerate(zip(zoom_x, zoom_scores)):
                    ax2.annotate(
                        f"{yi:.4f}",
                        (xi, yi),
                        textcoords="offset points",
                        xytext=(0, 10),
                        ha="center",
                        fontsize=9,
                    )

                ax2.set_xlabel("Document Index (zoomed around knee)")
                ax2.set_ylabel("Similarity Distance")
                ax2.set_title("Knee Point Detail View")
                ax2.grid(True, alpha=0.3)
            else:
                # Knee pointÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞
                ax2.plot(x, scores, "b-", linewidth=2, alpha=0.5)
                ax2.set_xlabel("Document Index")
                ax2.set_ylabel("Similarity Distance")
                ax2.set_title("No Clear Knee Point Detected")
                ax2.text(
                    0.5,
                    0.5,
                    reason,
                    transform=ax2.transAxes,
                    ha="center",
                    va="center",
                    fontsize=12,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7),
                )
                ax2.grid(True, alpha=0.3)

            plt.tight_layout()

            # Ïù¥ÎØ∏ÏßÄÎ•º base64Î°ú Ïù∏ÏΩîÎî©
            buffer = BytesIO()
            plt.savefig(buffer, format="png", dpi=150, bbox_inches="tight")
            buffer.seek(0)

            if save_path:
                plt.savefig(save_path, dpi=150, bbox_inches="tight")
                print(f"Knee detection graph saved to: {save_path}")

            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            plt.close(fig)  # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨

            return f"data:image/png;base64,{image_base64}"

        except Exception as e:
            print(f"Visualization error: {e}")
            return f"Visualization failed: {str(e)}"


# DynamicKneeCompressionRetriever Ï†úÍ±∞ - pure kneed detectionÎßå ÏÇ¨Ïö©


# --------- (D) RETRIEVER CREATION ---------
def create_retriever_with_sensitivity(
    vectorstore,
    sensitivity_config: dict,
    min_docs: int = 2,
    max_docs: int = 30,  # Îçî ÎßéÏùÄ Î≤îÏúÑ Í≤ÄÏÉâÏúºÎ°ú Ï¶ùÍ∞Ä
    direction: str = "decreasing",
    curve: str = "convex",
):
    """
    Sensitivity Í∏∞Î∞ò Dynamic Knee Point Detection Retriever ÏÉùÏÑ±

    Args:
        vectorstore: FAISS Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥
        sensitivity_config: sensitivity ÏÑ§Ï†ï (SENSITIVITY_OPTIONSÏóêÏÑú Í∞ÄÏ†∏Ïò® dict)
        min_docs: ÏµúÏÜå Î∞òÌôò Î¨∏ÏÑú Ïàò
        max_docs: ÏµúÎåÄ Í≤ÄÏÉâ Î¨∏ÏÑú Ïàò (knee Ï∞æÍ∏∞Ïö©, Î≥¥ÏàòÏ†Å ÏÑ§Ï†ïÏùÑ ÏúÑÌï¥ Ï¶ùÍ∞Ä)
        direction: knee detection Î∞©Ìñ• ("decreasing" ÎòêÎäî "increasing")
        curve: knee detection Í≥°ÏÑ† ÌÉÄÏûÖ ("convex" ÎòêÎäî "concave")

    Note:
        - ÏûëÏùÄ sensitivity (S=1) = Îπ†Î•∏ knee Í∞êÏßÄ = Ï†ÅÏùÄ Î¨∏ÏÑú ÏÑ†ÌÉù = Ï†ÅÍ∑πÏ†Å
        - ÌÅ∞ sensitivity (S=10) = Î≥¥ÏàòÏ†Å Í∞êÏßÄ = ÎßéÏùÄ Î¨∏ÏÑú ÏÑ†ÌÉù = Î≥¥ÏàòÏ†Å
    """
    sensitivity = sensitivity_config["sensitivity"]
    description = sensitivity_config["description"]

    print(
        f"üéØ Creating DynamicKneeRetriever with sensitivity={sensitivity} ({description})"
    )

    return DynamicKneeRetriever(
        vectorstore=vectorstore,
        min_docs=min_docs,
        max_docs=max_docs,
        sensitivity=sensitivity,
        direction=direction,
        curve=curve,
    )


# --------- (D) QUERY HANDLERS ---------
def handle_query_for_sensitivity(
    user_query: str, sensitivity_key: str, request: gr.Request
) -> Generator:
    """ÌäπÏ†ï sensitivity ÏÑ§Ï†ïÏúºÎ°ú ÏøºÎ¶¨ Ï≤òÎ¶¨"""
    session_id = get_session_id(request)

    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        session = sessions[session_id]
        sessions.move_to_end(session_id)

    # ÌûàÏä§ÌÜ†Î¶¨ Í∞ÄÏ†∏Ïò§Í∏∞
    history = session["history"][sensitivity_key]
    messages = history.copy()
    client = session["client"]

    # ÌòÑÏû¨ Î™®Îç∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
    with shared_state_lock:
        current_model = str(shared_state["current_model"])
        vectorstore = shared_state["vectorstore"]

    model_info = MODELS[current_model]
    sensitivity_config = SENSITIVITY_OPTIONS[sensitivity_key]

    # Extract relevant text data from PDFs with Dynamic Knee Detection
    docs = []
    context_documents = []

    if vectorstore:
        print(f"üîç [{sensitivity_key}] Retrieving relevant GIST rules...")

        retriever = create_retriever_with_sensitivity(vectorstore, sensitivity_config)
        if retriever:
            # Global stateÏóê retriever Ï†ÄÏû• (ÏãúÍ∞ÅÌôîÏö©)
            with shared_state_lock:
                shared_state["last_retrievers"][sensitivity_key] = retriever
            docs = retriever.invoke(user_query)
            context_documents = docs

        print(f"üìä [{sensitivity_key}] Retrieved {len(docs)} documents")

        # Knee detection Í≤∞Í≥º Ï∂úÎ†•
        if hasattr(retriever, "get_knee_info"):
            knee_info = retriever.get_knee_info()
            if knee_info:
                print(
                    f"üéØ [{sensitivity_key}] Knee Detection: {knee_info.get('selected_docs', 0)}/{knee_info.get('total_docs', 0)} docs, reason: {knee_info.get('reason', 'N/A')}"
                )

    # Legal query templateÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
    if context_documents:
        rendered_query = render_legal_query(user_query, context_documents)
    else:
        rendered_query = (
            f"User Query: {user_query}\n\nNo relevant documents found in the database."
        )

    # Legal system promptÏôÄ rendered query ÏÇ¨Ïö©
    messages = [
        {"role": "system", "content": LEGAL_SYSTEM_PROMPT},
        {"role": "user", "content": rendered_query},
    ]

    # Add user message to history first
    history.append({"role": "user", "content": user_query})

    # Create initial assistant message placeholder
    history.append({"role": "assistant", "content": ""})

    # Yield initial state with user query
    yield history

    # Invoke client with user query using streaming
    print(f"üí¨ [{sensitivity_key}] Inquiring LLM with streaming...")

    try:
        # LiteLLM ÏûêÎèô Í∞êÏßÄÎ•º ÏÇ¨Ïö©Ìïú ÌÜµÌï© Ïä§Ìä∏Î¶¨Î∞ç
        # ÌôòÍ≤ΩÎ≥ÄÏàòÏóê ÏÑ§Ï†ïÎêú API ÌÇ§Î•º ÏûêÎèôÏúºÎ°ú ÏÇ¨Ïö©
        completion = litellm.completion(
            model=model_info["model_id"],
            messages=messages,
            stream=True,
        )

        bot_response = ""
        for chunk in completion:
            if (
                hasattr(chunk, "choices")
                and chunk.choices
                and chunk.choices[0].delta
                and chunk.choices[0].delta.content
            ):
                chunk_content = chunk.choices[0].delta.content
                bot_response += chunk_content

                # Update the last message (assistant's response)
                history[-1]["content"] = html.escape(bot_response)
                yield history

    except Exception as e:
        error_msg = f"Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e) if str(e) else 'Ïä§Ìä∏Î¶¨Î∞ç Ï§ë Ïïå Ïàò ÏóÜÎäî Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'}"
        print(
            f"‚ùå [{sensitivity_key}] Streaming error: {e if str(e) else 'Unknown streaming error'}"
        )
        history[-1]["content"] = error_msg
        yield history

    yield history


def handle_multi_query(user_query, request: gr.Request):
    """Î™®Îì† sensitivity Î™®ÎìúÏóêÏÑú ÎèôÏãúÏóê ÏøºÎ¶¨ Ïã§Ìñâ"""
    if not user_query.strip():
        return [[] for _ in SENSITIVITY_OPTIONS.keys()]

    print(f"üí¨ ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ Ï≤òÎ¶¨ ÏãúÏûë: {user_query[:50]}...")

    # Î™®Îì† sensitivity Î™®ÎìúÏóê ÎåÄÌï¥ Ï†úÎÑàÎ†àÏù¥ÌÑ∞ ÏÉùÏÑ±
    generators = {
        method: handle_query_for_sensitivity(user_query, method, request)
        for method in SENSITIVITY_OPTIONS.keys()
    }

    # ÌòÑÏû¨ ÏÉÅÌÉú Ï∂îÏ†Å
    current_states = {method: [] for method in SENSITIVITY_OPTIONS.keys()}
    active_generators = set(SENSITIVITY_OPTIONS.keys())

    while active_generators:
        updated_methods = set()

        for method in list(active_generators):
            try:
                history = next(generators[method])
                current_states[method] = history
                updated_methods.add(method)
            except StopIteration:
                active_generators.remove(method)
                print(f"‚úÖ {method} completed")

        if updated_methods or len(active_generators) == 0:
            # Í≤∞Í≥ºÎ•º Ïò¨Î∞îÎ•∏ ÏàúÏÑúÎ°ú Ï†ïÎ†¨ÌïòÏó¨ Î∞òÌôò
            results = []
            for method in SENSITIVITY_OPTIONS.keys():
                history = current_states[method]
                results.append(history)

            yield results

    print("‚úÖ Î™®Îì† sensitivity ÏÑ§Ï†ïÏúºÎ°ú ÎãµÎ≥Ä ÏôÑÎ£å!")


def handle_additional_pdf_upload(pdfs, request: gr.Request):
    """Ï∂îÍ∞Ä PDF ÏóÖÎ°úÎìú Ï≤òÎ¶¨"""
    if not pdfs:
        return "ÏóÖÎ°úÎìúÎêú ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§."

    print("üìÑ Processing additional PDF(s)...")

    try:
        # PDFÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
        additional_docs = []
        for pdf in pdfs:
            text = ""
            try:
                doc = fitz.open(pdf)
                text = "\n".join([page.get_text("text") for page in doc])
                doc.close()
            except Exception as e:
                print(f"PDF Ï≤òÎ¶¨ Ïã§Ìå® {pdf}: {e}")
                continue

            if text.strip():
                document = Document(
                    page_content=text,
                    metadata={"source": pdf, "filename": os.path.basename(pdf)},
                )
                docs = TEXT_SPLITTER.split_documents([document])
                additional_docs.extend(docs)

        if not additional_docs:
            return "Ï≤òÎ¶¨Ìï† Ïàò ÏûàÎäî ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§."

        # Í∏∞Ï°¥ Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥Ïóê Ï∂îÍ∞Ä
        with shared_state_lock:
            vectorstore = shared_state["vectorstore"]
            embed_model = shared_state["embed_model"]
            if vectorstore and embed_model:
                print("üîÑ Merging with existing documents...")
                new_vectorstore = FAISS.from_documents(additional_docs, embed_model)
                vectorstore.merge_from(new_vectorstore)
                shared_state["vectorstore"] = vectorstore

        return f"‚úÖ Added {len(pdfs)} PDFs in {time.time():.2f} seconds"

    except Exception as e:
        return f"‚ùå Ïò§Î•ò Î∞úÏÉù: {str(e)}"


# --------- (E) VISUALIZATION FUNCTIONS ---------
def generate_knee_visualization(sensitivity_key: str):
    """ÌäπÏ†ï sensitivity ÏÑ§Ï†ïÏùò knee detection Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôî"""
    try:
        with shared_state_lock:
            retrievers = shared_state.get("last_retrievers", {})

        if sensitivity_key not in retrievers:
            return "No data available for this sensitivity setting. Please run a query first."

        retriever = retrievers[sensitivity_key]

        # DynamicKneeRetrieverÏùò ÏãúÍ∞ÅÌôî Ìò∏Ï∂ú
        if hasattr(retriever, "visualize_knee_detection"):
            return retriever.visualize_knee_detection()
        else:
            return f"'{sensitivity_key}' setting does not use knee detection."

    except Exception as e:
        return f"Visualization generation failed: {str(e)}"


def get_all_knee_visualizations():
    """Î™®Îì† sensitivity ÏÑ§Ï†ïÏùò knee detection Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôî"""
    results = {}
    try:
        with shared_state_lock:
            retrievers = shared_state.get("last_retrievers", {})

        for method in SENSITIVITY_OPTIONS.keys():
            if method in retrievers:
                results[method] = generate_knee_visualization(method)
            else:
                results[method] = "No data available"

        return results
    except Exception as e:
        return {method: f"Error: {str(e)}" for method in SENSITIVITY_OPTIONS.keys()}


# --------- (F) UTILITY FUNCTIONS ---------
def copy_as_markdown(history, rerank_method):
    """ÎåÄÌôî ÎÇ¥Ïö©ÏùÑ ÎßàÌÅ¨Îã§Ïö¥ÏúºÎ°ú Î≥µÏÇ¨"""
    if not history:
        return "Î≥µÏÇ¨Ìï† ÎÇ¥Ïö©Ïù¥ ÏóÜÏäµÎãàÎã§."

    markdown_content = f"# GIST Rules Analyzer - {rerank_method} Í≤ÄÏÉâ Í≤∞Í≥º\n\n"
    markdown_content += (
        f"**ÏÉùÏÑ± ÏãúÍ∞Ñ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    )

    for i, message in enumerate(history):
        if message["role"] == "user":
            markdown_content += f"## üôã‚Äç‚ôÇÔ∏è ÏßàÎ¨∏ {(i // 2) + 1}\n{message['content']}\n\n"
        elif message["role"] == "assistant":
            markdown_content += (
                f"## ü§ñ ÎãµÎ≥Ä ({rerank_method})\n{message['content']}\n\n"
            )

    return markdown_content


def reset_all_chats():
    """Î™®Îì† Ï±ÑÌåÖ Í∏∞Î°ù Ï¥àÍ∏∞Ìôî"""
    with session_lock:
        for session in sessions.values():
            for method in SENSITIVITY_OPTIONS.keys():
                session["history"][method] = []

    return [[] for _ in SENSITIVITY_OPTIONS.keys()]


def change_model(model_name: str):
    """Î™®Îç∏ Î≥ÄÍ≤Ω"""
    print(f"üîÑ Model changed to: {model_name}")

    with shared_state_lock:
        shared_state["current_model"] = model_name

    # Î™®Îì† ÏÑ∏ÏÖòÏùò ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏóÖÎç∞Ïù¥Ìä∏
    with session_lock:
        for session in sessions.values():
            session["client"] = get_client(model_name)

    return f"‚úÖ {model_name} Ï§ÄÎπÑÏôÑÎ£å"


def change_embedding_model(embedding_model_key: str):
    """ÏûÑÎ≤†Îî© Î™®Îç∏ Î≥ÄÍ≤Ω Î∞è Ìï¥Îãπ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú"""
    print(f"üîÑ ÏûÑÎ≤†Îî© Î™®Îç∏ Î≥ÄÍ≤Ω: {embedding_model_key}")

    if embedding_model_key not in EMBEDDING_MODELS:
        status_msg = f"‚ùå ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÏûÑÎ≤†Îî© Î™®Îç∏: {embedding_model_key}"
        return status_msg, get_database_status()

    model_config = EMBEDDING_MODELS[embedding_model_key]

    # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú ÏãúÎèÑ
    if load_database_for_embedding_model(embedding_model_key):
        status_msg = f"‚úÖ {model_config['description']} Î°úÎî© ÏôÑÎ£å"
        db_status = get_database_status()
        return status_msg, db_status
    else:
        status_msg = f"‚ùå {model_config['description']} Î°úÎî© Ïã§Ìå®"
        db_status = f"‚ùå ÏûÑÎ≤†Îî© Î™®Îç∏ '{model_config['description']}' Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\nüõ†Ô∏è Îã§Ïùå Î™ÖÎ†πÏñ¥Î°ú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Íµ¨Ï∂ïÌïòÏÑ∏Ïöî:\n   python build_multi_embedding_databases.py --model \"{embedding_model_key}\""
        return status_msg, db_status


# --------- (F) DATABASE STATUS ---------
def get_database_status():
    """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉÅÌÉú Î∞òÌôò"""
    with shared_state_lock:
        if not shared_state["database_loaded"]:
            return "‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Í∞Ä Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.\nüõ†Ô∏è Îã§Ïùå Î™ÖÎ†πÏñ¥Î°ú Î©ÄÌã∞ ÏûÑÎ≤†Îî© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Íµ¨Ï∂ïÌïòÏÑ∏Ïöî:\n   `python build_multi_embedding_databases.py`"

        db_info = shared_state["database_info"]
        current_embedding = shared_state.get("current_embedding_model", "Unknown")

        # ÌòÑÏû¨ ÏûÑÎ≤†Îî© Î™®Îç∏ Ï†ïÎ≥¥
        embedding_info = EMBEDDING_MODELS.get(current_embedding, {})
        embedding_desc = embedding_info.get("description", current_embedding)
        mteb_rank = embedding_info.get("mteb_rank", "N/A")
        dimension = embedding_info.get("dimension", "N/A")

        status_lines = [
            "‚úÖ **Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú ÏôÑÎ£å**",
            "",
            "ü§ñ **ÌòÑÏû¨ ÏûÑÎ≤†Îî© Î™®Îç∏**:",
            f"- Î™®Îç∏: {embedding_desc}",
            f"- MTEB ÏàúÏúÑ: {mteb_rank}ÏúÑ",
            f"- Ï∞®Ïõê: {dimension}",
            "",
            "üìä **ÌÜµÍ≥Ñ**:",
            f"- Ï¥ù Î¨∏ÏÑú: {db_info.get('total_documents', 'N/A')}Í∞ú",
            f"- Ï¥ù Ï≤≠ÌÅ¨: {db_info.get('total_chunks', 'N/A')}Í∞ú",
            f"- ÏÉùÏÑ± ÏùºÏãú: {db_info.get('created_at', 'N/A').split('T')[0] if db_info.get('created_at') else 'N/A'}",
        ]

        return "\n".join(status_lines)


# --------- (G) UI SETUP ---------
css = """
div {
    flex-wrap: nowrap !important;
}
.responsive-height {
    height: 100vh !important;
    padding-bottom: 20px !important;
}
.fill-height {
    height: 100% !important;
    flex-wrap: nowrap !important;
}
.extend-height {
    min-height: 300px !important;
    flex: 1 !important;
    overflow: auto !important;
}
.status-box {
    background: linear-gradient(135deg, #1a237e 0%, #283593 100%) !important;
    border: 2px solid #3f51b5 !important;
    border-radius: 12px !important;
    padding: 20px !important;
    margin: 12px 0 !important;
    color: #ffffff !important;
    font-weight: 500 !important;
    box-shadow: 0 4px 12px rgba(63, 81, 181, 0.3) !important;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif !important;
}
.status-box h2, .status-box h3 {
    color: #e3f2fd !important;
    margin-bottom: 8px !important;
    text-shadow: 0 1px 2px rgba(0,0,0,0.3) !important;
}
.status-box strong {
    color: #e8eaf6 !important;
    font-weight: 600 !important;
}
.status-box p {
    color: #ffffff !important;
    margin: 6px 0 !important;
}
footer {
    display: none !important;
}
"""

# Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú (Ïï± ÏãúÏûë Ïãú)
print("üöÄ GIST Rules Analyzer (Prebuilt Database Version) ÏãúÏûë!")
if not load_existing_database():
    print("‚ùå Ïï±ÏùÑ ÏãúÏûëÌï† Ïàò ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Íµ¨Ï∂ïÌï¥Ï£ºÏÑ∏Ïöî.")
    exit(1)

with gr.Blocks(
    title="GIST Rules Analyzer - Prebuilt DB", css=css, fill_height=True
) as demo:
    gr.Markdown(
        "<center><h1>‚öñÔ∏è GIST Legal Rules Analyzer (Professional Legal Assistant)</h1><p><strong>üéØ Ï†ÑÎ¨∏ Î≤ïÎ•† Î∂ÑÏÑù</strong> | Ï≤¥Í≥ÑÏ†Å Î≤ïÎ†πÌï¥ÏÑùÍ≥º Knee Point Detection | <strong>üìö Î≤ïÌïôÏ†Å Ìï¥ÏÑùÎ∞©Î≤ïÎ°† Ï†ÅÏö©</strong> | <strong>‚ö° LiteLLM ÌÜµÌï©</strong></p></center>"
    )

    # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉÅÌÉú ÌëúÏãú
    with gr.Row():
        database_status = gr.Markdown(
            value=get_database_status(), elem_classes=["status-box"]
        )

    # Í≥µÌÜµ Ïª®Ìä∏Î°§
    with gr.Row():
        with gr.Column(scale=2):
            with gr.Row():
                # ÎèôÏ†Å Î™®Îç∏ Î™©Î°ù ÏÉùÏÑ± Î∞è Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
                model_choices = list(MODELS.keys())
                default_value = (
                    DEFAULT_MODEL_ID
                    if DEFAULT_MODEL_ID in MODELS
                    else (model_choices[0] if model_choices else "")
                )
                with shared_state_lock:
                    shared_state["current_model"] = default_value

                model_dropdown = gr.Dropdown(
                    model_choices,
                    label="üß† LLM Î™®Îç∏ ÏÑ†ÌÉù",
                    value=default_value,
                    scale=2,
                    allow_custom_value=True,
                )
                model_status = gr.Textbox(
                    label="Î™®Îç∏ ÏÉÅÌÉú",
                    value=f"‚úÖ {DEFAULT_MODEL_ID.split('/')[-1]} Ï§ÄÎπÑÏôÑÎ£å",
                    interactive=False,
                    scale=1,
                )

            # ÏûÑÎ≤†Îî© Î™®Îç∏ ÏÑ†ÌÉù
            with gr.Row():
                embedding_choices = list(EMBEDDING_MODELS.keys())
                embedding_dropdown = gr.Dropdown(
                    embedding_choices,
                    label="üìä ÏûÑÎ≤†Îî© Î™®Îç∏ ÏÑ†ÌÉù (MTEB ÏàúÏúÑ Í∏∞Ï§Ä)",
                    value=DEFAULT_EMBEDDING_MODEL,
                    scale=2,
                    info="ÏûÑÎ≤†Îî© Î™®Îç∏ Î≥ÄÍ≤Ω Ïãú Ìï¥Îãπ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Í∞Ä ÏûêÎèô Î°úÎìúÎê©ÎãàÎã§",
                )
                embedding_status = gr.Textbox(
                    label="ÏûÑÎ≤†Îî© ÏÉÅÌÉú",
                    value=f"‚úÖ {EMBEDDING_MODELS[DEFAULT_EMBEDDING_MODEL]['description']} Ï§ÄÎπÑÏôÑÎ£å",
                    interactive=False,
                    scale=1,
                )

            additional_pdf_upload = gr.Files(
                label="üìÑ Ï∂îÍ∞Ä Î¨∏ÏÑú ÏóÖÎ°úÎìú (Vector Store ÌôïÏû•)", file_types=[".pdf"]
            )

            user_input = gr.Textbox(
                label="‚öñÔ∏è Î≤ïÎ•† ÏßàÏùòÎ¨∏ ÏûÖÎ†• (Legal Query Input)",
                placeholder="Ïòà: ÍµêÏàòÎãòÏù¥ Î∞ïÏÇ¨Í≥ºÏ†ï ÌïôÏÉùÏùÑ ÏßÄÎèÑÌï† Ïàò ÏûàÎäî Í∏∞Í∞ÑÏùÄ Ïñ∏Ï†úÍπåÏßÄÏù∏Í∞ÄÏöî? (ÌïôÏπô Ï†ú‚óãÏ°∞ Í¥ÄÎ†®)",
                info="üèõÔ∏è Î≤ïÌïôÏ†Å Ìï¥ÏÑùÎ∞©Î≤ïÎ°† Í∏∞Î∞ò Ï†ÑÎ¨∏ Î∂ÑÏÑù | üìö Ï≤¥Í≥ÑÏ†Å Î≤ïÎ†π Ìï¥ÏÑù Î∞è Ï°∞Î¨∏ Ïù∏Ïö© | üéØ Dynamic Knee Detection Î¨∏ÏÑú ÏÑ†ÌÉù",
                lines=3,
                interactive=True,
            )

        with gr.Column(scale=1):
            submit_btn = gr.Button("üöÄ ÌÖåÏä§Ìä∏ Ïã§Ìñâ", variant="primary", size="lg")
            reset_btn = gr.Button("üîÑ Ï¥àÍ∏∞Ìôî", size="lg")

    # ÌÉ≠ÏúºÎ°ú Ï±ÑÌåÖÍ≥º ÏãúÍ∞ÅÌôî Î∂ÑÎ¶¨
    with gr.Tabs() as main_tabs:
        with gr.TabItem("‚öñÔ∏è Legal Analysis Results", id="chat_tab"):
            gr.Markdown("### üèõÔ∏è Ï†ÑÎ¨∏ Î≤ïÎ•† Î∂ÑÏÑù Î∞è Sensitivity ÎπÑÍµê")
            gr.Markdown(
                "**Î≤ïÌïôÏ†Å Ìï¥ÏÑùÎ∞©Î≤ïÎ°†** Í∏∞Î∞ò Ï≤¥Í≥ÑÏ†Å Î∂ÑÏÑù | **Ï°∞Î¨∏ Ïù∏Ïö© Î∞è Î≤ïÎ¶¨Ï†Å Ìï¥ÏÑù** | **ÏûëÏùÄ SÍ∞í = Ï†ÅÍ∑πÏ†Å Î¨∏ÏÑúÏÑ†ÌÉù**, **ÌÅ∞ SÍ∞í = Î≥¥ÏàòÏ†Å Î¨∏ÏÑúÏÑ†ÌÉù**"
            )

            # 4Í∞úÏùò sensitivity ÏÑ§Ï†ïÎ≥Ñ Ï±ÑÌåÖ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (2x2 Í∑∏Î¶¨Îìú)
            sensitivity_keys = list(SENSITIVITY_OPTIONS.keys())
            with gr.Row(elem_classes=["fill-height"]):
                with gr.Column(scale=1, elem_classes=["fill-height"]):
                    # Ï≤´ Î≤àÏß∏ sensitivity ÏÑ§Ï†ï
                    with gr.Group(elem_classes=["extend-height"]):
                        config1 = SENSITIVITY_OPTIONS[sensitivity_keys[0]]
                        gr.Markdown(f"### üîπ {sensitivity_keys[0]}")
                        with gr.Row():
                            gr.Dropdown(
                                [
                                    f"Sensitivity: {config1['sensitivity']} | {config1['description']}"
                                ],
                                value=f"Sensitivity: {config1['sensitivity']} | {config1['description']}",
                                interactive=False,
                                scale=3,
                                show_label=False,
                            )
                        chatbot_sens1 = gr.Chatbot(
                            elem_classes=["extend-height"],
                            show_copy_button=True,
                            type="messages",
                        )
                        copy_btn_sens1 = gr.Button("üìã Í≤∞Í≥º Î≥µÏÇ¨", size="sm")

                    # Îëê Î≤àÏß∏ sensitivity ÏÑ§Ï†ï
                    with gr.Group(elem_classes=["extend-height"]):
                        config2 = SENSITIVITY_OPTIONS[sensitivity_keys[1]]
                        gr.Markdown(f"### üî∏ {sensitivity_keys[1]}")
                        with gr.Row():
                            gr.Dropdown(
                                [
                                    f"Sensitivity: {config2['sensitivity']} | {config2['description']}"
                                ],
                                value=f"Sensitivity: {config2['sensitivity']} | {config2['description']}",
                                interactive=False,
                                scale=3,
                                show_label=False,
                            )
                        chatbot_sens2 = gr.Chatbot(
                            elem_classes=["extend-height"],
                            show_copy_button=True,
                            type="messages",
                        )
                        copy_btn_sens2 = gr.Button("üìã Í≤∞Í≥º Î≥µÏÇ¨", size="sm")

                with gr.Column(scale=1, elem_classes=["fill-height"]):
                    # ÏÑ∏ Î≤àÏß∏ sensitivity ÏÑ§Ï†ï
                    with gr.Group(elem_classes=["extend-height"]):
                        config3 = SENSITIVITY_OPTIONS[sensitivity_keys[2]]
                        gr.Markdown(f"### üî∂ {sensitivity_keys[2]}")
                        with gr.Row():
                            gr.Dropdown(
                                [
                                    f"Sensitivity: {config3['sensitivity']} | {config3['description']}"
                                ],
                                value=f"Sensitivity: {config3['sensitivity']} | {config3['description']}",
                                interactive=False,
                                scale=3,
                                show_label=False,
                            )
                        chatbot_sens3 = gr.Chatbot(
                            elem_classes=["extend-height"],
                            show_copy_button=True,
                            type="messages",
                        )
                        copy_btn_sens3 = gr.Button("üìã Í≤∞Í≥º Î≥µÏÇ¨", size="sm")

                    # ÎÑ§ Î≤àÏß∏ sensitivity ÏÑ§Ï†ï
                    with gr.Group(elem_classes=["extend-height"]):
                        config4 = SENSITIVITY_OPTIONS[sensitivity_keys[3]]
                        gr.Markdown(f"### üî• {sensitivity_keys[3]}")
                        with gr.Row():
                            gr.Dropdown(
                                [
                                    f"Sensitivity: {config4['sensitivity']} | {config4['description']}"
                                ],
                                value=f"Sensitivity: {config4['sensitivity']} | {config4['description']}",
                                interactive=False,
                                scale=3,
                                show_label=False,
                            )
                        chatbot_sens4 = gr.Chatbot(
                            elem_classes=["extend-height"],
                            show_copy_button=True,
                            type="messages",
                        )
                        copy_btn_sens4 = gr.Button("üìã Í≤∞Í≥º Î≥µÏÇ¨", size="sm")

        with gr.TabItem("üìä Legal Document Analysis Visualization", id="viz_tab"):
            gr.Markdown("### üìà Î≤ïÎ•† Î¨∏ÏÑú ÏÑ†ÌÉù Ìå®ÌÑ¥ ÏãúÍ∞ÅÌôî")
            gr.Markdown(
                "**Î≤ïÎ†πÎ≥Ñ Î¨∏ÏÑú Í≤ÄÏÉâ Í≤∞Í≥º ÏãúÍ∞ÅÌôî** | **S=1 (Ï†ÅÍ∑πÏ†Å) ‚Üí S=10 (Î≥¥ÏàòÏ†Å)** ÏàúÏúºÎ°ú Í∞Å sensitivityÏùò knee point Í∞êÏßÄ Ìå®ÌÑ¥Í≥º **Î≤ïÏ†Å Í∑ºÍ±∞ Î¨∏ÏÑú ÏÑ†ÌÉù Í≥ºÏ†ï**ÏùÑ ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌï©ÎãàÎã§."
            )

            with gr.Row():
                viz_refresh_btn = gr.Button("üîÑ Í∑∏ÎûòÌîÑ ÏÉàÎ°úÍ≥†Ïπ®", variant="secondary")
                viz_save_btn = gr.Button("üíæ Í∑∏ÎûòÌîÑ Ï†ÄÏû•", variant="secondary")

            # 4Í∞úÏùò sensitivityÎ≥Ñ ÏãúÍ∞ÅÌôî Í≤∞Í≥º (2x2 Í∑∏Î¶¨Îìú)
            with gr.Row():
                with gr.Column(scale=1):
                    config1 = SENSITIVITY_OPTIONS[sensitivity_keys[0]]
                    gr.Markdown(f"#### üîπ {sensitivity_keys[0]}")
                    viz_image_sens1 = gr.Image(
                        label=f"Sensitivity: {config1['sensitivity']} | {config1['description']}",
                        show_label=True,
                        interactive=False,
                        height=400,
                    )

                    config2 = SENSITIVITY_OPTIONS[sensitivity_keys[1]]
                    gr.Markdown(f"#### üî∏ {sensitivity_keys[1]}")
                    viz_image_sens2 = gr.Image(
                        label=f"Sensitivity: {config2['sensitivity']} | {config2['description']}",
                        show_label=True,
                        interactive=False,
                        height=400,
                    )

                with gr.Column(scale=1):
                    config3 = SENSITIVITY_OPTIONS[sensitivity_keys[2]]
                    gr.Markdown(f"#### üî∂ {sensitivity_keys[2]}")
                    viz_image_sens3 = gr.Image(
                        label=f"Sensitivity: {config3['sensitivity']} | {config3['description']}",
                        show_label=True,
                        interactive=False,
                        height=400,
                    )

                    config4 = SENSITIVITY_OPTIONS[sensitivity_keys[3]]
                    gr.Markdown(f"#### üî• {sensitivity_keys[3]}")
                    viz_image_sens4 = gr.Image(
                        label=f"Sensitivity: {config4['sensitivity']} | {config4['description']}",
                        show_label=True,
                        interactive=False,
                        height=400,
                    )

    # ÏãúÍ∞ÅÌôî Í¥ÄÎ†® Ìï®ÏàòÎì§
    def refresh_visualizations():
        """Î™®Îì† sensitivity ÏÑ§Ï†ïÏùò knee detection Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôî"""
        results = get_all_knee_visualizations()
        sensitivity_keys = list(SENSITIVITY_OPTIONS.keys())
        return (
            results.get(sensitivity_keys[0], "No data available"),
            results.get(sensitivity_keys[1], "No data available"),
            results.get(sensitivity_keys[2], "No data available"),
            results.get(sensitivity_keys[3], "No data available"),
        )

    def save_visualizations():
        """ÏãúÍ∞ÅÌôî Í≤∞Í≥ºÎ•º ÌååÏùºÎ°ú Ï†ÄÏû•"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_dir = Path(f"sensitivity_visualizations_{timestamp}")
            save_dir.mkdir(exist_ok=True)

            # Sensitivity Í∏∞Î∞ò ÌååÏùºÎ™Ö Îß§Ìïë
            sensitivity_keys = list(SENSITIVITY_OPTIONS.keys())
            methods_mapping = {}
            for key in sensitivity_keys:
                config = SENSITIVITY_OPTIONS[key]
                clean_name = f"sensitivity_{config['sensitivity']}"
                methods_mapping[key] = clean_name

            saved_files = []
            with shared_state_lock:
                retrievers = shared_state.get("last_retrievers", {})

            for method, retriever in retrievers.items():
                try:
                    save_name = methods_mapping.get(
                        method,
                        method.replace(" ", "_").replace("(", "").replace(")", ""),
                    )
                    save_path = save_dir / f"{save_name}.png"

                    if hasattr(retriever, "visualize_knee_detection"):
                        retriever.visualize_knee_detection(str(save_path))
                        saved_files.append(str(save_path))
                except Exception as e:
                    print(f"Save failed - {method}: {e}")

            if saved_files:
                return f"{len(saved_files)} graphs have been saved:\n" + "\n".join(
                    saved_files
                )
            else:
                return "No graphs available to save. Please run a query first."
        except Exception as e:
            return f"Save failed: {str(e)}"

    # Ïù¥Î≤§Ìä∏ Ìï∏Îì§Îü¨ (GeneratorÎ°ú ÏàòÏ†ï)
    def init_client_on_first_query(user_query, request: gr.Request):
        session_id = get_session_id(request)
        with session_lock:
            if session_id not in sessions:
                init_session(session_id)
            if sessions[session_id]["client"] is None:
                sessions[session_id]["client"] = get_client(
                    shared_state["current_model"]
                )

        # GeneratorÎ•º Ï†úÎåÄÎ°ú yield
        for result in handle_multi_query(user_query, request):
            yield result

    # Î™®Îç∏ Î≥ÄÍ≤Ω Ïù¥Î≤§Ìä∏
    model_dropdown.change(
        fn=change_model, inputs=[model_dropdown], outputs=[model_status]
    )

    # ÏûÑÎ≤†Îî© Î™®Îç∏ Î≥ÄÍ≤Ω Ïù¥Î≤§Ìä∏
    embedding_dropdown.change(
        fn=change_embedding_model,
        inputs=[embedding_dropdown],
        outputs=[embedding_status, database_status],
    )

    # Ï∂îÍ∞Ä PDF ÏóÖÎ°úÎìú
    additional_pdf_upload.upload(
        fn=handle_additional_pdf_upload,
        inputs=[additional_pdf_upload],
        outputs=[database_status],
    )

    # Î©ÄÌã∞ ÏøºÎ¶¨ Ï≤òÎ¶¨ (Sensitivity Í∏∞Î∞ò)
    submit_btn.click(
        fn=init_client_on_first_query,
        inputs=[user_input],
        outputs=[
            chatbot_sens1,
            chatbot_sens2,
            chatbot_sens3,
            chatbot_sens4,
        ],
    )

    user_input.submit(
        fn=init_client_on_first_query,
        inputs=[user_input],
        outputs=[
            chatbot_sens1,
            chatbot_sens2,
            chatbot_sens3,
            chatbot_sens4,
        ],
    )

    # Ï¥àÍ∏∞Ìôî
    reset_btn.click(
        fn=reset_all_chats,
        outputs=[
            chatbot_sens1,
            chatbot_sens2,
            chatbot_sens3,
            chatbot_sens4,
        ],
    )

    # ÏãúÍ∞ÅÌôî Ïù¥Î≤§Ìä∏ Ïó∞Í≤∞
    viz_refresh_btn.click(
        fn=refresh_visualizations,
        inputs=[],
        outputs=[viz_image_sens1, viz_image_sens2, viz_image_sens3, viz_image_sens4],
    )

    viz_save_btn.click(
        fn=save_visualizations,
        inputs=[],
        outputs=[gr.Textbox(visible=False)],  # Í≤∞Í≥ºÎ•º ÏΩòÏÜîÏóêÎßå ÌëúÏãú
    )

    # Î≥µÏÇ¨ Í∏∞Îä• (Sensitivity Í∏∞Î∞ò)
    sensitivity_keys = list(SENSITIVITY_OPTIONS.keys())

    copy_btn_sens1.click(
        fn=lambda h: copy_as_markdown(h, sensitivity_keys[0]),
        inputs=[chatbot_sens1],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_sens2.click(
        fn=lambda h: copy_as_markdown(h, sensitivity_keys[1]),
        inputs=[chatbot_sens2],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_sens3.click(
        fn=lambda h: copy_as_markdown(h, sensitivity_keys[2]),
        inputs=[chatbot_sens3],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_sens4.click(
        fn=lambda h: copy_as_markdown(h, sensitivity_keys[3]),
        inputs=[chatbot_sens4],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

if __name__ == "__main__":
    print("‚öñÔ∏è GIST Legal Rules Analyzer (Professional Legal Assistant) Ï§ÄÎπÑÏôÑÎ£å!")
    print("üèõÔ∏è Î≤ïÌïôÏ†Å Ìï¥ÏÑùÎ∞©Î≤ïÎ°† Í∏∞Î∞ò Ï†ÑÎ¨∏ Î≤ïÎ•† Î∂ÑÏÑù ÏãúÏä§ÌÖú")
    print("üìö Ï≤¥Í≥ÑÏ†Å Î≤ïÎ†π Ìï¥ÏÑù: Î¨∏Î¶¨Ìï¥ÏÑù ‚Üí Ï≤¥Í≥ÑÏ†ÅÌï¥ÏÑù ‚Üí Î™©Ï†ÅÎ°†Ï†ÅÌï¥ÏÑù ‚Üí Ïö∞ÏÑ†ÏàúÏúÑÏ†ÅÏö©")
    print("üìã Legal Template System: Ï†ÑÎ¨∏ ÏãúÏä§ÌÖúÌîÑÎ°¨ÌîÑÌä∏ + ÏøºÎ¶¨ÌÖúÌîåÎ¶ø Ï†ÅÏö©")
    print("üéØ S=1,3,5,10 sensitivityÎ°ú Î≤ïÏ†Å Í∑ºÍ±∞ Î¨∏ÏÑú ÏÑ†ÌÉù ÏµúÏ†ÅÌôî")
    print("‚ö° LiteLLM ÌÜµÌï©ÏúºÎ°ú Îã§ÏñëÌïú LLM ÌîÑÎ°úÎ∞îÏù¥Îçî ÏßÄÏõê")
    print("üåê http://localhost:7860 ÏóêÏÑú Ïã§Ìñâ Ï§ë...")
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

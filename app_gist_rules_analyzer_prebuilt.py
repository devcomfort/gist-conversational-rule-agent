"""
GIST Rules Analyzer - Prebuilt Database Version (LiteLLM Integrated)
==================================================================

ì‚¬ì „ êµ¬ì¶•ëœ FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•˜ëŠ” ê³ ì† ì‹œì‘ ë²„ì „ì…ë‹ˆë‹¤.
LiteLLMì˜ ìë™ ëª¨ë¸ ê°ì§€ì™€ í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ API í‚¤ ê´€ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

ğŸš€ ì‚¬ìš© ì „ ìš”êµ¬ì‚¬í•­:
    python build_rule_database.py  # ë¨¼ì € ì‹¤í–‰

âœ¨ ì£¼ìš” íŠ¹ì§•:
- âš¡ 3ì´ˆ ë‚´ ì•± ì‹œì‘ ì™„ë£Œ
- ğŸ¤– LiteLLM ì™„ì „ í†µí•©ìœ¼ë¡œ 15+ LLM í”„ë¡œë°”ì´ë” ìë™ ì§€ì›
- ğŸ¯ Dynamic Knee Detectionìœ¼ë¡œ ì ì‘í˜• ë¬¸ì„œ ì„ íƒ
- ğŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- ğŸ“„ ì¶”ê°€ PDF ì—…ë¡œë“œ ì§€ì› (ì„ íƒì )
- ğŸ”‘ í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìë™ API í‚¤ ê´€ë¦¬

ğŸŒ ì§€ì›í•˜ëŠ” LLM í”„ë¡œë°”ì´ë” (í™˜ê²½ë³€ìˆ˜ë§Œ ì„¤ì •í•˜ë©´ ìë™ ê°ì§€):
- OpenAI: OPENAI_API_KEY
- Anthropic: ANTHROPIC_API_KEY
- Google: GOOGLE_API_KEY (Vertex AI, Gemini, Palm)
- Azure: AZURE_API_KEY
- Fireworks AI: FIREWORKS_AI_API_KEY â­ (ê¸°ë³¸ ëª¨ë¸)
- Together AI: TOGETHER_AI_API_KEY
- Groq: GROQ_API_KEY
- Cohere: COHERE_API_KEY
- DeepSeek: DEEPSEEK_API_KEY
- Perplexity: PERPLEXITY_API_KEY
- Replicate: REPLICATE_API_TOKEN
- HuggingFace: HUGGINGFACE_API_KEY
- Novita AI: NOVITA_API_KEY
- ê¸°íƒ€ LiteLLM ì§€ì› í”„ë¡œë°”ì´ë”

ğŸ“– ì‚¬ìš©ë²•:
1. ì›í•˜ëŠ” í”„ë¡œë°”ì´ë”ì˜ API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •
2. ì•±ì„ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ê°ì§€
3. UIì—ì„œ ëª¨ë¸ ì„ íƒí•˜ì—¬ ì‚¬ìš©
"""

import gradio as gr
import os
import json
import time
import hashlib
import html
import threading
import fitz
import litellm
import pystache
from pathlib import Path
from collections import OrderedDict
from datetime import datetime
from dotenv import load_dotenv
from litellm import get_valid_models
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Cross-encoder reranking ì œê±° - pure kneed optimizationë§Œ ì‚¬ìš©
from langchain_core.retrievers import BaseRetriever
from kneed import KneeLocator
from typing import Dict, Generator, List, Optional
from pydantic import Field
import matplotlib.pyplot as plt
import matplotlib
import base64
from io import BytesIO

# matplotlib ë°±ì—”ë“œ ì„¤ì • (ì„œë²„ í™˜ê²½ ëŒ€ì‘)
matplotlib.use("Agg")

# Environment variables - LiteLLMì´ ìë™ìœ¼ë¡œ ê°ì§€í•˜ë¯€ë¡œ ìµœì†Œí™”
load_dotenv()
# LiteLLMì€ ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ë“¤ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì‚¬ìš©:
# OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, GROQ_API_KEY,
# TOGETHER_AI_API_KEY, DEEPSEEK_API_KEY, PERPLEXITY_API_KEY,
# REPLICATE_API_TOKEN, HUGGINGFACE_API_KEY, NOVITA_API_KEY,
# FIREWORKS_AI_API_KEY ë“±

# Configuration
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

# Template paths
LEGAL_SYSTEM_PROMPT_PATH = Path("system_prompts/legal_agent_system_prompt.mustache")
LEGAL_QUERY_TEMPLATE_PATH = Path("templates/legal_query_template.mustache")

# ì§€ì›í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ ì„¤ì • (TODO.txt ê¸°ë°˜ - build_multi_embedding_databases.pyì™€ ë™ì¼)
EMBEDDING_MODELS = {
    "Qwen/Qwen3-Embedding-0.6B": {
        "model_name": "Qwen/Qwen3-Embedding-0.6B",
        "db_name": "faiss_qwen3_embedding_0.6b",
        "dimension": 1024,
        "mteb_rank": 3,
        "description": "Qwen3 Embedding 0.6B - MTEB 3ìœ„",
    },
    "jinaai/jina-embeddings-v3": {
        "model_name": "jinaai/jina-embeddings-v3",
        "db_name": "faiss_jina_embeddings_v3",
        "dimension": 1024,
        "mteb_rank": 22,
        "description": "Jina Embeddings v3 - MTEB 22ìœ„",
    },
    "BAAI/bge-m3": {
        "model_name": "BAAI/bge-m3",
        "db_name": "faiss_bge_m3",
        "dimension": 1024,
        "mteb_rank": 23,
        "description": "BGE M3 - MTEB 23ìœ„",
    },
    "sentence-transformers/all-MiniLM-L6-v2": {
        "model_name": "sentence-transformers/all-MiniLM-L6-v2",
        "db_name": "faiss_all_minilm_l6_v2",
        "dimension": 384,
        "mteb_rank": 117,
        "description": "All MiniLM L6 v2 - MTEB 117ìœ„ (ê¸°ì¡´ ê¸°ë³¸ ëª¨ë¸)",
    },
}

# ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸
DEFAULT_EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# ë ˆê±°ì‹œ ì§€ì›ì„ ìœ„í•œ ê¸°ë³¸ DB ê²½ë¡œ
LEGACY_DB_PATH = Path("faiss_db")

# Default model configuration
DEFAULT_MODEL_ID = "fireworks_ai/accounts/fireworks/models/gpt-oss-20b"


def _detect_provider(model_id: str) -> str:
    """LiteLLMì´ ì§€ì›í•˜ëŠ” í”„ë¡œë°”ì´ë”ë“¤ì„ ê°ì§€"""
    # Anthropic
    if model_id.startswith(("claude", "anthropic/")):
        return "anthropic"

    # Google providers
    if model_id.startswith(("vertex_ai/", "gemini")):
        return "vertex_ai"
    if model_id.startswith("palm/"):
        return "palm"

    # Azure
    if model_id.startswith("azure/"):
        return "azure"

    # AWS
    if model_id.startswith("bedrock/"):
        return "bedrock"
    if model_id.startswith("sagemaker/"):
        return "sagemaker"

    # Specialized providers
    if model_id.startswith("novita/"):
        return "novita"
    if model_id.startswith("fireworks_ai/"):
        return "fireworks_ai"
    if model_id.startswith("together_ai/"):
        return "together_ai"
    if model_id.startswith("groq/"):
        return "groq"
    if model_id.startswith("cohere/"):
        return "cohere"
    if model_id.startswith("deepseek/"):
        return "deepseek"
    if model_id.startswith("perplexity/"):
        return "perplexity"
    if model_id.startswith("ollama/"):
        return "ollama"
    if model_id.startswith("replicate/"):
        return "replicate"
    if model_id.startswith("huggingface/"):
        return "huggingface"

    # Default to OpenAI for unrecognized patterns
    return "openai"


def _load_dynamic_models() -> Dict[str, Dict[str, str]]:
    """LiteLLM get_valid_models()ë¥¼ í™œìš©í•œ ë™ì  ëª¨ë¸ ë¡œë”©"""

    print("ğŸ” LiteLLMì„ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ì„ ê²€ìƒ‰ ì¤‘...")

    try:
        # LiteLLMì´ í™˜ê²½ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ë°˜í™˜
        model_ids = get_valid_models(check_provider_endpoint=True)
        print(f"âœ… {len(model_ids)}ê°œì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print("ğŸ“‹ ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
        model_ids = []

    dynamic: Dict[str, Dict[str, str]] = {}
    for mid in model_ids:
        dynamic[mid] = {"model_id": mid, "provider": _detect_provider(mid)}

    # ê¸°ë³¸ ëª¨ë¸ì´ ëª©ë¡ì— ì—†ëŠ” ê²½ìš° ì¶”ê°€ (í™˜ê²½ë³€ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •)
    if DEFAULT_MODEL_ID not in dynamic:
        dynamic[DEFAULT_MODEL_ID] = {
            "model_id": DEFAULT_MODEL_ID,
            "provider": "fireworks_ai",
        }
        print(f"â• ê¸°ë³¸ ëª¨ë¸ ì¶”ê°€: {DEFAULT_MODEL_ID}")

    # ìµœì†Œí•œ í•˜ë‚˜ì˜ ëª¨ë¸ì´ ìˆì–´ì•¼ í•¨
    if not dynamic:
        # í´ë°±ìœ¼ë¡œ ì¼ë°˜ì ì¸ ëª¨ë¸ë“¤ ì¶”ê°€
        fallback_models = [
            {"model_id": "gpt-4o-mini", "provider": "openai"},
            {"model_id": "claude-3-haiku-20240307", "provider": "anthropic"},
            {"model_id": DEFAULT_MODEL_ID, "provider": "fireworks_ai"},
        ]
        for model in fallback_models:
            dynamic[model["model_id"]] = model
        print("ğŸ“‹ í´ë°± ëª¨ë¸ ëª©ë¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")

    print(f"ğŸ¯ ì´ {len(dynamic)}ê°œ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
    return dynamic


# Models setup (ë™ì  ë¡œë”©)
MODELS = _load_dynamic_models()

# Kneed Sensitivity Testing Options (ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ ì˜¬ë°”ë¥¸ ì •ì˜)
# ì‘ì€ S = ë¹ ë¥¸ knee ê°ì§€ = ì ì€ ë¬¸ì„œ ì„ íƒ = ì ê·¹ì 
# í° S = ë³´ìˆ˜ì  ê°ì§€ = ë§ì€ ë¬¸ì„œ ì„ íƒ = ë³´ìˆ˜ì 
SENSITIVITY_OPTIONS = {
    "ì ê·¹ì  ì„ íƒ (S=1)": {
        "sensitivity": 1,
        "description": "ë¹ ë¥¸ knee ê°ì§€, ì ì€ ë¬¸ì„œ ì„ íƒ",
    },
    "ê· í˜• ì„ íƒ (S=3)": {"sensitivity": 3, "description": "ì¤‘ê°„ ì •ë„ ë¬¸ì„œ ì„ íƒ"},
    "í‘œì¤€ ì„ íƒ (S=5)": {"sensitivity": 5, "description": "í‘œì¤€ì  ë¬¸ì„œ ì„ íƒ"},
    "ë³´ìˆ˜ì  ì„ íƒ (S=10)": {
        "sensitivity": 10,
        "description": "ì‹ ì¤‘í•œ ë¬¸ì„œ ì„ íƒ, ë§ì€ ë¬¸ì„œ í¬í•¨",
    },
}

# ì¶”ê°€ ì‹¤í—˜ì„ ìœ„í•œ ì „ì²´ sensitivity ë²”ìœ„ (ë¬¸ì„œ ì˜ˆì‹œ ê¸°ë°˜)
FULL_SENSITIVITY_RANGE = [1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 100]

# Initialize embeddings
EMBED_MODEL = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)


# Template loading functions
def load_legal_system_prompt() -> str:
    """Load legal agent system prompt from mustache template"""
    try:
        if LEGAL_SYSTEM_PROMPT_PATH.exists():
            with open(LEGAL_SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as f:
                template_content = f.read()

            # Legal system promptëŠ” static templateì´ë¯€ë¡œ ë°”ë¡œ ë°˜í™˜
            return template_content
        else:
            print(f"âš ï¸ Legal system prompt not found: {LEGAL_SYSTEM_PROMPT_PATH}")
            # Fallback to basic system prompt
            return """You are a GIST Rules and Regulations Expert Assistant. 
You have comprehensive knowledge of all GIST academic rules, regulations, guidelines, and policies.
Always provide accurate, detailed answers based on the provided context.
When answering questions about GIST rules, cite specific regulation numbers and titles when available."""
    except Exception as e:
        print(f"âŒ Error loading legal system prompt: {e}")
        return "You are a GIST legal expert assistant."


def load_legal_query_template() -> str:
    """Load legal query template from mustache file"""
    try:
        if LEGAL_QUERY_TEMPLATE_PATH.exists():
            with open(LEGAL_QUERY_TEMPLATE_PATH, "r", encoding="utf-8") as f:
                return f.read()
        else:
            print(f"âš ï¸ Legal query template not found: {LEGAL_QUERY_TEMPLATE_PATH}")
            return "{{user_query}}\n\nContext:\n{{context}}"
    except Exception as e:
        print(f"âŒ Error loading legal query template: {e}")
        return "{{user_query}}\n\nContext:\n{{context}}"


# Load templates
LEGAL_SYSTEM_PROMPT = load_legal_system_prompt()
LEGAL_QUERY_TEMPLATE = load_legal_query_template()

print("ğŸ“‹ Legal templates loaded successfully")


def render_legal_query(user_query: str, context_documents: List[Document]) -> str:
    """
    Legal query templateì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë Œë”ë§

    Args:
        user_query: ì‚¬ìš©ìì˜ ì§ˆë¬¸
        context_documents: ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë“¤

    Returns:
        ë Œë”ë§ëœ legal query ë¬¸ìì—´
    """
    try:
        # í˜„ì¬ ì‹œê°„
        current_time = datetime.now()

        # ë¬¸ì„œë“¤ì„ í…œí”Œë¦¿ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        template_docs = []
        for idx, doc in enumerate(context_documents):
            doc_data = {
                "index": idx + 1,
                "source": doc.metadata.get(
                    "source", doc.metadata.get("filename", "Unknown")
                ),
                "category": doc.metadata.get("category", ""),
                "page_content": doc.page_content,
                "metadata": doc.metadata,
            }

            # priority ì„¤ì • (ì¹´í…Œê³ ë¦¬ë³„ ìš°ì„ ìˆœìœ„)
            if doc.metadata.get("category") == "í•™ì¹™":
                doc_data["priority"] = "High (í•™ì¹™)"
            elif doc.metadata.get("category") in ["ê·œì •", "ì‹œí–‰ì„¸ì¹™"]:
                doc_data["priority"] = "Medium (ê·œì •/ì‹œí–‰ì„¸ì¹™)"
            else:
                doc_data["priority"] = "Normal"

            template_docs.append(doc_data)

        # ë³µìˆ˜ ë²•ë ¹ì´ ê´€ë ¨ëœ ê²½ìš° ì²´í¬ (2ê°œ ì´ìƒì˜ ì„œë¡œ ë‹¤ë¥¸ source)
        unique_sources = set(doc.get("source", "") for doc in template_docs)
        has_multiple_regulations = len(unique_sources) > 1

        # í…œí”Œë¦¿ ë°ì´í„° êµ¬ì„±
        template_data = {
            "user_query": user_query,
            "query_timestamp": current_time.strftime("%Y-%m-%d %H:%M:%S"),
            "current_datetime": current_time.strftime("%Y-%m-%d %H:%M:%S"),
            "document_count": len(context_documents),
            "context_documents": template_docs,
            "multiple_regulations": has_multiple_regulations,
        }

        # Mustache template ë Œë”ë§
        renderer = pystache.Renderer()
        rendered_query = renderer.render(LEGAL_QUERY_TEMPLATE, template_data)

        return rendered_query

    except Exception as e:
        print(f"âŒ Legal query rendering failed: {e}")
        # Fallback: ê°„ë‹¨í•œ í˜•ì‹
        context_text = "\n\n".join(
            [
                f"Document {i + 1}: {doc.page_content}"
                for i, doc in enumerate(context_documents)
            ]
        )
        return f"User Query: {user_query}\n\nContext Documents:\n{context_text}"


# --------- (A) GLOBAL STATE ---------


# Session variables
MAX_SESSIONS = 100
sessions = OrderedDict()
session_lock = threading.Lock()

# Global shared state
shared_state: Dict = {
    "current_model": None,
    "vectorstore": None,
    "database_loaded": False,
    "database_info": {},
    "last_retrievers": {},  # rerank_methodë³„ ë§ˆì§€ë§‰ ì‚¬ìš©ëœ retriever ì €ì¥
}
shared_state_lock = threading.Lock()


def get_embedding_model(model_key: str = None):
    """ì„ë² ë”© ëª¨ë¸ì„ ë™ì ìœ¼ë¡œ ë¡œë“œí•˜ê³  ë°˜í™˜"""
    if model_key is None:
        model_key = shared_state["current_embedding_model"]

    if model_key not in EMBEDDING_MODELS:
        print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ëª¨ë¸: {model_key}")
        model_key = DEFAULT_EMBEDDING_MODEL

    model_config = EMBEDDING_MODELS[model_key]

    try:
        print(f"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_config['description']}")
        embed_model = HuggingFaceEmbeddings(
            model_name=model_config["model_name"],
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True},
        )
        print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_config['description']}")
        return embed_model
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±
        if model_key != DEFAULT_EMBEDDING_MODEL:
            print(
                f"ğŸ”„ ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±: {EMBEDDING_MODELS[DEFAULT_EMBEDDING_MODEL]['description']}"
            )
            return get_embedding_model(DEFAULT_EMBEDDING_MODEL)
        else:
            raise e


def get_database_path(embedding_model_key: str = None) -> Path:
    """ì„ë² ë”© ëª¨ë¸ì— ë”°ë¥¸ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ë°˜í™˜"""
    if embedding_model_key is None:
        embedding_model_key = shared_state["current_embedding_model"]

    if embedding_model_key in EMBEDDING_MODELS:
        db_name = EMBEDDING_MODELS[embedding_model_key]["db_name"]
        db_path = Path(db_name)

        # ë©€í‹° ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if db_path.exists() and (db_path / "index.faiss").exists():
            return db_path

    # í´ë°±: ë ˆê±°ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
    if LEGACY_DB_PATH.exists() and (LEGACY_DB_PATH / "index.faiss").exists():
        print(f"âš ï¸ ë ˆê±°ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©: {LEGACY_DB_PATH}")
        return LEGACY_DB_PATH

    # ë©€í‹° ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ë°˜í™˜ (ì¡´ì¬í•˜ì§€ ì•Šì•„ë„)
    return Path(EMBEDDING_MODELS[embedding_model_key]["db_name"])


def load_database_for_embedding_model(embedding_model_key: str = None):
    """íŠ¹ì • ì„ë² ë”© ëª¨ë¸ì— ëŒ€í•œ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
    if embedding_model_key is None:
        embedding_model_key = shared_state["current_embedding_model"]

    db_path = get_database_path(embedding_model_key)
    model_config = EMBEDDING_MODELS[embedding_model_key]

    print(f"ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘: {model_config['description']}")
    print(f"ğŸ“ ê²½ë¡œ: {db_path}")

    if not db_path.exists():
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {db_path}")
        print("ğŸ› ï¸ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë©€í‹° ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”:")
        print(
            f"   python build_multi_embedding_databases.py --model '{embedding_model_key}'"
        )
        return False

    if not (db_path / "index.faiss").exists() or not (db_path / "index.pkl").exists():
        print("âŒ FAISS ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("ğŸ› ï¸ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë©€í‹° ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”:")
        print(
            f"   python build_multi_embedding_databases.py --model '{embedding_model_key}'"
        )
        return False

    try:
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        embed_model = get_embedding_model(embedding_model_key)

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        info_file = db_path / "database_info.json"
        if info_file.exists():
            with open(info_file, "r", encoding="utf-8") as f:
                database_info = json.load(f)
                shared_state["database_info"] = database_info
                print(
                    f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´: {database_info['total_documents']}ê°œ ë¬¸ì„œ, {database_info['total_chunks']}ê°œ ì²­í¬"
                )

        # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        print("ğŸ”„ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        vectorstore = FAISS.load_local(
            str(db_path), embed_model, allow_dangerous_deserialization=True
        )

        with shared_state_lock:
            shared_state["vectorstore"] = vectorstore
            shared_state["embed_model"] = embed_model
            shared_state["current_embedding_model"] = embedding_model_key
            shared_state["database_loaded"] = True

        print(f"âœ… {model_config['description']} ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
        return True

    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def load_existing_database():
    """ê¸°ì¡´ FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ (ë©€í‹° ì„ë² ë”© ëª¨ë¸ ì§€ì›)"""
    return load_database_for_embedding_model(DEFAULT_EMBEDDING_MODEL)


# --------- (B) SESSION SETUP ---------
def get_session_id(request: gr.Request):
    raw_id = request.client.host + str(request.headers.get("user-agent"))
    return hashlib.md5(raw_id.encode()).hexdigest()


def init_session(session_id: str):
    sessions[session_id] = {
        "client": None,
        "history": {method: [] for method in SENSITIVITY_OPTIONS.keys()},
    }


def get_client(model_name: str):
    """LiteLLM ìë™ ì„¤ì • ë°˜í™˜ - í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìë™ ê°ì§€"""
    model_info = MODELS[model_name]

    # LiteLLMì´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ API í‚¤ë¥¼ ì°¾ì•„ì„œ ì‚¬ìš©
    # ìˆ˜ë™ ì„¤ì • ìµœì†Œí™”í•˜ê³  LiteLLMì˜ ìë™ ê°ì§€ ê¸°ëŠ¥ í™œìš©
    config = {"type": "litellm", "model_id": model_info["model_id"]}

    return config


# --------- (C) DYNAMIC KNEE RETRIEVER CLASS ---------
class DynamicKneeRetriever(BaseRetriever):
    """
    Knee Point Detectionì„ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê°œìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” Retriever

    ê³ ì •ëœ kê°œ ëŒ€ì‹  ìœ ì‚¬ë„ ê³¡ì„ ì˜ knee pointë¥¼ ì°¾ì•„ì„œ
    ìì—°ìŠ¤ëŸ¬ìš´ cutoff ì§€ì ê¹Œì§€ì˜ ëª¨ë“  ê´€ë ¨ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    # Pydantic v2 ëª¨ë¸ í•„ë“œ ì •ì˜
    vectorstore: FAISS
    min_docs: int = 2
    max_docs: int = 30  # ë³´ìˆ˜ì  ì„¤ì •ì„ ìœ„í•´ ì¦ê°€
    sensitivity: float = 5.0  # í‘œì¤€ ì„ íƒ (ë¬¸ì„œ ê¸°ë°˜)
    direction: str = "decreasing"
    curve: str = "convex"
    last_knee_info: Dict = Field(default_factory=dict)

    # Pydantic ëª¨ë¸ êµ¬ì„±ì„ ìœ„í•œ ì„¤ì •
    class Config:
        arbitrary_types_allowed = True

    def __init__(
        self,
        vectorstore: FAISS,
        min_docs: int = 2,
        max_docs: int = 30,
        sensitivity: float = 5.0,
        direction: str = "decreasing",
        curve: str = "convex",
        **data,
    ):
        """
        Args:
            vectorstore: FAISS ë²¡í„°ìŠ¤í† ì–´
            min_docs: ìµœì†Œ ë°˜í™˜ ë¬¸ì„œ ìˆ˜
            max_docs: ìµœëŒ€ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (knee ì°¾ê¸°ìš©)
            sensitivity: knee detection ë¯¼ê°ë„ (ê¸°ë³¸ 5.0, ì‘ì„ìˆ˜ë¡ ì ê·¹ì , í´ìˆ˜ë¡ ë³´ìˆ˜ì )
            direction: "decreasing" (ê±°ë¦¬ ê¸°ì¤€) ë˜ëŠ” "increasing" (ìœ ì‚¬ë„ ê¸°ì¤€)
            curve: "convex" ë˜ëŠ” "concave"
        """
        # Pydantic v2 ì´ˆê¸°í™”
        super().__init__(
            vectorstore=vectorstore,
            min_docs=min_docs,
            max_docs=max_docs,
            sensitivity=sensitivity,
            direction=direction,
            curve=curve,
            **data,
        )

    def _get_relevant_documents(self, query: str, **kwargs) -> List[Document]:
        """ì¿¼ë¦¬ì— ëŒ€í•´ knee point ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ë°˜í™˜"""
        try:
            # 1. ìµœëŒ€ ê°œìˆ˜ë¡œ ë¬¸ì„œì™€ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            docs_and_scores = self.vectorstore.similarity_search_with_score(
                query, k=self.max_docs
            )

            if len(docs_and_scores) <= self.min_docs:
                # ë¬¸ì„œê°€ ë„ˆë¬´ ì ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ ë°˜í™˜
                self.last_knee_info = {
                    "total_docs": len(docs_and_scores),
                    "selected_docs": len(docs_and_scores),
                    "knee_point": None,
                    "reason": "Too few documents",
                }
                return [doc for doc, _ in docs_and_scores]

            # 2. ì ìˆ˜ ì¶”ì¶œ ë° ì •ë ¬ (FAISSëŠ” ê±°ë¦¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
            scores = [score for _, score in docs_and_scores]
            documents = [doc for doc, _ in docs_and_scores]

            # 3. Knee point ì°¾ê¸°
            knee_idx = self._find_knee_point(scores)

            # 4. ìµœì¢… ë¬¸ì„œ ì„ íƒ
            if knee_idx is None or knee_idx < self.min_docs:
                # Kneeë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ë„ˆë¬´ ì ìœ¼ë©´ ìµœì†Œ ê°œìˆ˜ ë°˜í™˜
                selected_docs = documents[: self.min_docs]
                knee_reason = "No clear knee found, using min_docs"
            else:
                # Knee pointê¹Œì§€ ì„ íƒ (inclusive)
                selected_docs = documents[: knee_idx + 1]
                knee_reason = f"Knee point detected at index {knee_idx}"

            # 5. ë¶„ì„ ê²°ê³¼ ì €ì¥ (ì‹œê°í™”ë¥¼ ìœ„í•´ ì „ì²´ ì ìˆ˜ ì €ì¥)
            self.last_knee_info = {
                "total_docs": len(docs_and_scores),
                "selected_docs": len(selected_docs),
                "knee_point": knee_idx,
                "scores": scores,  # ì‹œê°í™”ë¥¼ ìœ„í•´ ëª¨ë“  ì ìˆ˜ ì €ì¥
                "scores_preview": scores[:10],  # ë¡œê·¸ìš© ì²˜ìŒ 10ê°œ
                "selected_scores": [
                    score for _, score in docs_and_scores[: len(selected_docs)]
                ],
                "reason": knee_reason,
                "sensitivity": self.sensitivity,
            }

            print(
                f"ğŸ” Dynamic Retrieval: {len(selected_docs)}/{len(docs_and_scores)} docs selected (knee at {knee_idx})"
            )
            return selected_docs

        except Exception as e:
            print(f"âŒ DynamicKneeRetriever error: {e}")
            # ì—ëŸ¬ ì‹œ fallbackìœ¼ë¡œ ê¸°ë³¸ ê²€ìƒ‰
            return self.vectorstore.similarity_search(query, k=self.min_docs)

    def _find_knee_point(self, scores: List[float]) -> Optional[int]:
        """ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ knee point ì°¾ê¸°"""
        if len(scores) < 3:  # ìµœì†Œ 3ê°œëŠ” ìˆì–´ì•¼ knee ì°¾ê¸° ê°€ëŠ¥
            return None

        try:
            # xì¶•ì€ ë¬¸ì„œ ì¸ë±ìŠ¤, yì¶•ì€ ê±°ë¦¬/ì ìˆ˜
            x = list(range(len(scores)))
            y = scores

            # KneeLocatorë¡œ knee point ì°¾ê¸°
            kl = KneeLocator(
                x=x,
                y=y,
                curve=self.curve,
                direction=self.direction,
                S=self.sensitivity,
                online=True,  # ì˜¨ë¼ì¸ ëª¨ë“œë¡œ ë” ì •í™•í•œ íƒì§€
            )

            return kl.knee

        except Exception as e:
            print(f"âš ï¸ Knee detection failed: {e}")
            return None

    def get_knee_info(self) -> Dict:
        """ë§ˆì§€ë§‰ knee ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        return self.last_knee_info.copy()

    def visualize_knee_detection(self, save_path: Optional[str] = None) -> str:
        """
        Knee Point Detection ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³  base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°˜í™˜

        Returns:
            base64ë¡œ ì¸ì½”ë”©ëœ PNG ì´ë¯¸ì§€ ë°ì´í„° ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€
        """
        if not self.last_knee_info:
            return "âš ï¸ No knee detection data available. Please run a query first."

        try:
            scores = self.last_knee_info.get("scores", [])
            knee_point = self.last_knee_info.get("knee_point")
            selected_docs = self.last_knee_info.get("selected_docs", 0)
            total_docs = self.last_knee_info.get("total_docs", len(scores))
            reason = self.last_knee_info.get("reason", "Unknown")
            sensitivity = self.last_knee_info.get("sensitivity", 1.0)

            if not scores:
                return "âš ï¸ No similarity scores available for visualization."

            # ì˜ì–´ í°íŠ¸ë§Œ ì‚¬ìš© (í•œê¸€ í°íŠ¸ ì˜¤ë¥˜ ë°©ì§€)
            plt.rcParams["font.family"] = ["DejaVu Sans", "Arial", "Liberation Sans"]
            plt.rcParams["axes.unicode_minus"] = False

            # ì‹œê°í™” ìƒì„±
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
            fig.suptitle(
                f"Dynamic Knee Point Detection Analysis\nSensitivity: {sensitivity} | Selected: {selected_docs}/{total_docs} docs",
                fontsize=14,
                fontweight="bold",
            )

            # ë¬¸ì„œ ì¸ë±ìŠ¤ (xì¶•)
            x = list(range(len(scores)))

            # ìƒìœ„ í”Œë¡¯: ì „ì²´ ìœ ì‚¬ë„ ê³¡ì„ 
            ax1.plot(
                x,
                scores,
                "b-o",
                linewidth=2,
                markersize=5,
                label="Document Similarity Distance",
                alpha=0.7,
            )

            # Knee point í‘œì‹œ
            if knee_point is not None and knee_point < len(scores):
                ax1.axvline(
                    x=knee_point,
                    color="red",
                    linestyle="--",
                    linewidth=2,
                    label=f"Knee Point (idx={knee_point})",
                )
                ax1.plot(
                    knee_point,
                    scores[knee_point],
                    "ro",
                    markersize=10,
                    label=f"Knee: {scores[knee_point]:.4f}",
                )

            # ì„ íƒëœ ë¬¸ì„œ ì˜ì—­ í‘œì‹œ
            if selected_docs > 0:
                selected_x = x[:selected_docs]
                selected_scores = scores[:selected_docs]
                ax1.fill_between(
                    selected_x,
                    0,
                    selected_scores,
                    alpha=0.3,
                    color="green",
                    label=f"Selected Documents ({selected_docs})",
                )

            ax1.set_xlabel("Document Index (ranked by similarity)")
            ax1.set_ylabel("Similarity Distance (lower = more similar)")
            ax1.set_title(f"Document Similarity Curve\nReason: {reason}")
            ax1.legend(loc="upper right")
            ax1.grid(True, alpha=0.3)

            # í•˜ìœ„ í”Œë¡¯: Knee Detection ì„¸ë¶€ì‚¬í•­ (í™•ëŒ€)
            if knee_point is not None and len(scores) > 3:
                # Knee ì£¼ë³€ ë°ì´í„° í™•ëŒ€ í‘œì‹œ
                start_idx = max(0, knee_point - 3)
                end_idx = min(len(scores), knee_point + 4)
                zoom_x = x[start_idx:end_idx]
                zoom_scores = scores[start_idx:end_idx]

                ax2.plot(
                    zoom_x, zoom_scores, "b-o", linewidth=3, markersize=8, alpha=0.8
                )
                ax2.axvline(x=knee_point, color="red", linestyle="--", linewidth=2)
                ax2.plot(knee_point, scores[knee_point], "ro", markersize=12)

                # ë°ì´í„° í¬ì¸íŠ¸ ë¼ë²¨ë§
                for i, (xi, yi) in enumerate(zip(zoom_x, zoom_scores)):
                    ax2.annotate(
                        f"{yi:.4f}",
                        (xi, yi),
                        textcoords="offset points",
                        xytext=(0, 10),
                        ha="center",
                        fontsize=9,
                    )

                ax2.set_xlabel("Document Index (zoomed around knee)")
                ax2.set_ylabel("Similarity Distance")
                ax2.set_title("Knee Point Detail View")
                ax2.grid(True, alpha=0.3)
            else:
                # Knee pointê°€ ì—†ëŠ” ê²½ìš°
                ax2.plot(x, scores, "b-", linewidth=2, alpha=0.5)
                ax2.set_xlabel("Document Index")
                ax2.set_ylabel("Similarity Distance")
                ax2.set_title("No Clear Knee Point Detected")
                ax2.text(
                    0.5,
                    0.5,
                    reason,
                    transform=ax2.transAxes,
                    ha="center",
                    va="center",
                    fontsize=12,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7),
                )
                ax2.grid(True, alpha=0.3)

            plt.tight_layout()

            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            buffer = BytesIO()
            plt.savefig(buffer, format="png", dpi=150, bbox_inches="tight")
            buffer.seek(0)

            if save_path:
                plt.savefig(save_path, dpi=150, bbox_inches="tight")
                print(f"Knee detection graph saved to: {save_path}")

            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            plt.close(fig)  # ë©”ëª¨ë¦¬ ì •ë¦¬

            return f"data:image/png;base64,{image_base64}"

        except Exception as e:
            print(f"Visualization error: {e}")
            return f"Visualization failed: {str(e)}"


# DynamicKneeCompressionRetriever ì œê±° - pure kneed detectionë§Œ ì‚¬ìš©


# --------- (D) RETRIEVER CREATION ---------
def create_retriever_with_sensitivity(
    vectorstore,
    sensitivity_config: dict,
    min_docs: int = 2,
    max_docs: int = 30,  # ë” ë§ì€ ë²”ìœ„ ê²€ìƒ‰ìœ¼ë¡œ ì¦ê°€
    direction: str = "decreasing",
    curve: str = "convex",
):
    """
    Sensitivity ê¸°ë°˜ Dynamic Knee Point Detection Retriever ìƒì„±

    Args:
        vectorstore: FAISS ë²¡í„°ìŠ¤í† ì–´
        sensitivity_config: sensitivity ì„¤ì • (SENSITIVITY_OPTIONSì—ì„œ ê°€ì ¸ì˜¨ dict)
        min_docs: ìµœì†Œ ë°˜í™˜ ë¬¸ì„œ ìˆ˜
        max_docs: ìµœëŒ€ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (knee ì°¾ê¸°ìš©, ë³´ìˆ˜ì  ì„¤ì •ì„ ìœ„í•´ ì¦ê°€)
        direction: knee detection ë°©í–¥ ("decreasing" ë˜ëŠ” "increasing")
        curve: knee detection ê³¡ì„  íƒ€ì… ("convex" ë˜ëŠ” "concave")

    Note:
        - ì‘ì€ sensitivity (S=1) = ë¹ ë¥¸ knee ê°ì§€ = ì ì€ ë¬¸ì„œ ì„ íƒ = ì ê·¹ì 
        - í° sensitivity (S=10) = ë³´ìˆ˜ì  ê°ì§€ = ë§ì€ ë¬¸ì„œ ì„ íƒ = ë³´ìˆ˜ì 
    """
    sensitivity = sensitivity_config["sensitivity"]
    description = sensitivity_config["description"]

    print(
        f"ğŸ¯ Creating DynamicKneeRetriever with sensitivity={sensitivity} ({description})"
    )

    return DynamicKneeRetriever(
        vectorstore=vectorstore,
        min_docs=min_docs,
        max_docs=max_docs,
        sensitivity=sensitivity,
        direction=direction,
        curve=curve,
    )


# --------- (D) QUERY HANDLERS ---------
def handle_query_for_sensitivity(
    user_query: str, sensitivity_key: str, request: gr.Request
) -> Generator:
    """íŠ¹ì • sensitivity ì„¤ì •ìœ¼ë¡œ ì¿¼ë¦¬ ì²˜ë¦¬"""
    session_id = get_session_id(request)

    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        session = sessions[session_id]
        sessions.move_to_end(session_id)

    # íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    history = session["history"][sensitivity_key]
    messages = history.copy()
    client = session["client"]

    # í˜„ì¬ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    with shared_state_lock:
        current_model = str(shared_state["current_model"])
        vectorstore = shared_state["vectorstore"]

    model_info = MODELS[current_model]
    sensitivity_config = SENSITIVITY_OPTIONS[sensitivity_key]

    # Extract relevant text data from PDFs with Dynamic Knee Detection
    docs = []
    context_documents = []

    if vectorstore:
        print(f"ğŸ” [{sensitivity_key}] Retrieving relevant GIST rules...")

        retriever = create_retriever_with_sensitivity(vectorstore, sensitivity_config)
        if retriever:
            # Global stateì— retriever ì €ì¥ (ì‹œê°í™”ìš©)
            with shared_state_lock:
                shared_state["last_retrievers"][sensitivity_key] = retriever
            docs = retriever.invoke(user_query)
            context_documents = docs

        print(f"ğŸ“Š [{sensitivity_key}] Retrieved {len(docs)} documents")

        # Knee detection ê²°ê³¼ ì¶œë ¥
        if hasattr(retriever, "get_knee_info"):
            knee_info = retriever.get_knee_info()
            if knee_info:
                print(
                    f"ğŸ¯ [{sensitivity_key}] Knee Detection: {knee_info.get('selected_docs', 0)}/{knee_info.get('total_docs', 0)} docs, reason: {knee_info.get('reason', 'N/A')}"
                )

    # Legal query templateì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
    if context_documents:
        rendered_query = render_legal_query(user_query, context_documents)
    else:
        rendered_query = (
            f"User Query: {user_query}\n\nNo relevant documents found in the database."
        )

    # Legal system promptì™€ rendered query ì‚¬ìš©
    messages = [
        {"role": "system", "content": LEGAL_SYSTEM_PROMPT},
        {"role": "user", "content": rendered_query},
    ]

    # Add user message to history first
    history.append({"role": "user", "content": user_query})

    # Create initial assistant message placeholder
    history.append({"role": "assistant", "content": ""})

    # Yield initial state with user query
    yield history

    # Invoke client with user query using streaming
    print(f"ğŸ’¬ [{sensitivity_key}] Inquiring LLM with streaming...")

    try:
        # LiteLLM ìë™ ê°ì§€ë¥¼ ì‚¬ìš©í•œ í†µí•© ìŠ¤íŠ¸ë¦¬ë°
        # í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •ëœ API í‚¤ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©
        completion = litellm.completion(
            model=model_info["model_id"],
            messages=messages,
            stream=True,
        )

        bot_response = ""
        for chunk in completion:
            if (
                hasattr(chunk, "choices")
                and chunk.choices
                and chunk.choices[0].delta
                and chunk.choices[0].delta.content
            ):
                chunk_content = chunk.choices[0].delta.content
                bot_response += chunk_content

                # Update the last message (assistant's response)
                history[-1]["content"] = html.escape(bot_response)
                yield history

    except Exception as e:
        error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e) if str(e) else 'ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'}"
        print(
            f"âŒ [{sensitivity_key}] Streaming error: {e if str(e) else 'Unknown streaming error'}"
        )
        history[-1]["content"] = error_msg
        yield history

    yield history


def handle_multi_query(user_query, request: gr.Request):
    """ëª¨ë“  sensitivity ëª¨ë“œì—ì„œ ë™ì‹œì— ì¿¼ë¦¬ ì‹¤í–‰"""
    if not user_query.strip():
        return [[] for _ in SENSITIVITY_OPTIONS.keys()]

    print(f"ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {user_query[:50]}...")

    # ëª¨ë“  sensitivity ëª¨ë“œì— ëŒ€í•´ ì œë„ˆë ˆì´í„° ìƒì„±
    generators = {
        method: handle_query_for_sensitivity(user_query, method, request)
        for method in SENSITIVITY_OPTIONS.keys()
    }

    # í˜„ì¬ ìƒíƒœ ì¶”ì 
    current_states = {method: [] for method in SENSITIVITY_OPTIONS.keys()}
    active_generators = set(SENSITIVITY_OPTIONS.keys())

    while active_generators:
        updated_methods = set()

        for method in list(active_generators):
            try:
                history = next(generators[method])
                current_states[method] = history
                updated_methods.add(method)
            except StopIteration:
                active_generators.remove(method)
                print(f"âœ… {method} completed")

        if updated_methods or len(active_generators) == 0:
            # ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
            results = []
            for method in SENSITIVITY_OPTIONS.keys():
                history = current_states[method]
                results.append(history)

            yield results

    print("âœ… ëª¨ë“  sensitivity ì„¤ì •ìœ¼ë¡œ ë‹µë³€ ì™„ë£Œ!")


def handle_additional_pdf_upload(pdfs, request: gr.Request):
    """ì¶”ê°€ PDF ì—…ë¡œë“œ ì²˜ë¦¬"""
    if not pdfs:
        return "ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

    print("ğŸ“„ Processing additional PDF(s)...")

    try:
        # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        additional_docs = []
        for pdf in pdfs:
            text = ""
            try:
                doc = fitz.open(pdf)
                text = "\n".join([page.get_text("text") for page in doc])
                doc.close()
            except Exception as e:
                print(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨ {pdf}: {e}")
                continue

            if text.strip():
                document = Document(
                    page_content=text,
                    metadata={"source": pdf, "filename": os.path.basename(pdf)},
                )
                docs = TEXT_SPLITTER.split_documents([document])
                additional_docs.extend(docs)

        if not additional_docs:
            return "ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
        with shared_state_lock:
            vectorstore = shared_state["vectorstore"]
            embed_model = shared_state["embed_model"]
            if vectorstore and embed_model:
                print("ğŸ”„ Merging with existing documents...")
                new_vectorstore = FAISS.from_documents(additional_docs, embed_model)
                vectorstore.merge_from(new_vectorstore)
                shared_state["vectorstore"] = vectorstore

        return f"âœ… Added {len(pdfs)} PDFs in {time.time():.2f} seconds"

    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# --------- (E) VISUALIZATION FUNCTIONS ---------
def generate_knee_visualization(sensitivity_key: str):
    """íŠ¹ì • sensitivity ì„¤ì •ì˜ knee detection ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    try:
        with shared_state_lock:
            retrievers = shared_state.get("last_retrievers", {})

        if sensitivity_key not in retrievers:
            return "No data available for this sensitivity setting. Please run a query first."

        retriever = retrievers[sensitivity_key]

        # DynamicKneeRetrieverì˜ ì‹œê°í™” í˜¸ì¶œ
        if hasattr(retriever, "visualize_knee_detection"):
            return retriever.visualize_knee_detection()
        else:
            return f"'{sensitivity_key}' setting does not use knee detection."

    except Exception as e:
        return f"Visualization generation failed: {str(e)}"


def get_all_knee_visualizations():
    """ëª¨ë“  sensitivity ì„¤ì •ì˜ knee detection ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    results = {}
    try:
        with shared_state_lock:
            retrievers = shared_state.get("last_retrievers", {})

        for method in SENSITIVITY_OPTIONS.keys():
            if method in retrievers:
                results[method] = generate_knee_visualization(method)
            else:
                results[method] = "No data available"

        return results
    except Exception as e:
        return {method: f"Error: {str(e)}" for method in SENSITIVITY_OPTIONS.keys()}


# --------- (F) UTILITY FUNCTIONS ---------
def copy_as_markdown(history, rerank_method):
    """ëŒ€í™” ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³µì‚¬"""
    if not history:
        return "ë³µì‚¬í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

    markdown_content = f"# GIST Rules Analyzer - {rerank_method} ê²€ìƒ‰ ê²°ê³¼\n\n"
    markdown_content += (
        f"**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    )

    for i, message in enumerate(history):
        if message["role"] == "user":
            markdown_content += f"## ğŸ™‹â€â™‚ï¸ ì§ˆë¬¸ {(i // 2) + 1}\n{message['content']}\n\n"
        elif message["role"] == "assistant":
            markdown_content += (
                f"## ğŸ¤– ë‹µë³€ ({rerank_method})\n{message['content']}\n\n"
            )

    return markdown_content


def reset_all_chats():
    """ëª¨ë“  ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"""
    with session_lock:
        for session in sessions.values():
            for method in SENSITIVITY_OPTIONS.keys():
                session["history"][method] = []

    return [[] for _ in SENSITIVITY_OPTIONS.keys()]


def change_model(model_name: str):
    """ëª¨ë¸ ë³€ê²½"""
    print(f"ğŸ”„ Model changed to: {model_name}")

    with shared_state_lock:
        shared_state["current_model"] = model_name

    # ëª¨ë“  ì„¸ì…˜ì˜ í´ë¼ì´ì–¸íŠ¸ ì—…ë°ì´íŠ¸
    with session_lock:
        for session in sessions.values():
            session["client"] = get_client(model_name)

    return f"âœ… {model_name} ì¤€ë¹„ì™„ë£Œ"


def change_embedding_model(embedding_model_key: str):
    """ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ë° í•´ë‹¹ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
    print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë³€ê²½: {embedding_model_key}")

    if embedding_model_key not in EMBEDDING_MODELS:
        status_msg = f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ëª¨ë¸: {embedding_model_key}"
        return status_msg, get_database_status()

    model_config = EMBEDDING_MODELS[embedding_model_key]

    # ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹œë„
    if load_database_for_embedding_model(embedding_model_key):
        status_msg = f"âœ… {model_config['description']} ë¡œë”© ì™„ë£Œ"
        db_status = get_database_status()
        return status_msg, db_status
    else:
        status_msg = f"âŒ {model_config['description']} ë¡œë”© ì‹¤íŒ¨"
        db_status = f"âŒ ì„ë² ë”© ëª¨ë¸ '{model_config['description']}' ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nğŸ› ï¸ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”:\n   python build_multi_embedding_databases.py --model \"{embedding_model_key}\""
        return status_msg, db_status


# --------- (F) DATABASE STATUS ---------
def get_database_status():
    """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ë°˜í™˜"""
    with shared_state_lock:
        if not shared_state["database_loaded"]:
            return "âŒ ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\nğŸ› ï¸ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë©€í‹° ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”:\n   `python build_multi_embedding_databases.py`"

        db_info = shared_state["database_info"]
        current_embedding = shared_state.get("current_embedding_model", "Unknown")

        # í˜„ì¬ ì„ë² ë”© ëª¨ë¸ ì •ë³´
        embedding_info = EMBEDDING_MODELS.get(current_embedding, {})
        embedding_desc = embedding_info.get("description", current_embedding)
        mteb_rank = embedding_info.get("mteb_rank", "N/A")
        dimension = embedding_info.get("dimension", "N/A")

        status_lines = [
            "âœ… **ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ**",
            "",
            "ğŸ¤– **í˜„ì¬ ì„ë² ë”© ëª¨ë¸**:",
            f"- ëª¨ë¸: {embedding_desc}",
            f"- MTEB ìˆœìœ„: {mteb_rank}ìœ„",
            f"- ì°¨ì›: {dimension}",
            "",
            "ğŸ“Š **í†µê³„**:",
            f"- ì´ ë¬¸ì„œ: {db_info.get('total_documents', 'N/A')}ê°œ",
            f"- ì´ ì²­í¬: {db_info.get('total_chunks', 'N/A')}ê°œ",
            f"- ìƒì„± ì¼ì‹œ: {db_info.get('created_at', 'N/A').split('T')[0] if db_info.get('created_at') else 'N/A'}",
        ]

        return "\n".join(status_lines)


# --------- (G) UI SETUP ---------
css = """
div {
    flex-wrap: nowrap !important;
}
.responsive-height {
    height: 100vh !important;
    padding-bottom: 20px !important;
}
.fill-height {
    height: 100% !important;
    flex-wrap: nowrap !important;
}
.extend-height {
    min-height: 300px !important;
    flex: 1 !important;
    overflow: auto !important;
}
.status-box {
    background: linear-gradient(135deg, #1a237e 0%, #283593 100%) !important;
    border: 2px solid #3f51b5 !important;
    border-radius: 12px !important;
    padding: 20px !important;
    margin: 12px 0 !important;
    color: #ffffff !important;
    font-weight: 500 !important;
    box-shadow: 0 4px 12px rgba(63, 81, 181, 0.3) !important;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif !important;
}
.status-box h2, .status-box h3 {
    color: #e3f2fd !important;
    margin-bottom: 8px !important;
    text-shadow: 0 1px 2px rgba(0,0,0,0.3) !important;
}
.status-box strong {
    color: #e8eaf6 !important;
    font-weight: 600 !important;
}
.status-box p {
    color: #ffffff !important;
    margin: 6px 0 !important;
}
footer {
    display: none !important;
}
"""

# ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ)
print("ğŸš€ GIST Rules Analyzer (Prebuilt Database Version) ì‹œì‘!")
if not load_existing_database():
    print("âŒ ì•±ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
    exit(1)

with gr.Blocks(
    title="GIST Rules Analyzer - Prebuilt DB", css=css, fill_height=True
) as demo:
    gr.Markdown(
        "<center><h1>âš–ï¸ GIST Legal Rules Analyzer (Professional Legal Assistant)</h1><p><strong>ğŸ¯ ì „ë¬¸ ë²•ë¥  ë¶„ì„</strong> | ì²´ê³„ì  ë²•ë ¹í•´ì„ê³¼ Knee Point Detection | <strong>ğŸ“š ë²•í•™ì  í•´ì„ë°©ë²•ë¡  ì ìš©</strong> | <strong>âš¡ LiteLLM í†µí•©</strong></p></center>"
    )

    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í‘œì‹œ
    with gr.Row():
        database_status = gr.Markdown(
            value=get_database_status(), elem_classes=["status-box"]
        )

    # ê³µí†µ ì»¨íŠ¸ë¡¤
    with gr.Row():
        with gr.Column(scale=2):
            with gr.Row():
                # ë™ì  ëª¨ë¸ ëª©ë¡ ìƒì„± ë° ê¸°ë³¸ê°’ ì„¤ì •
                model_choices = list(MODELS.keys())
                default_value = (
                    DEFAULT_MODEL_ID
                    if DEFAULT_MODEL_ID in MODELS
                    else (model_choices[0] if model_choices else "")
                )
                with shared_state_lock:
                    shared_state["current_model"] = default_value

                model_dropdown = gr.Dropdown(
                    model_choices,
                    label="ğŸ§  LLM ëª¨ë¸ ì„ íƒ",
                    value=default_value,
                    scale=2,
                    allow_custom_value=True,
                )
                model_status = gr.Textbox(
                    label="ëª¨ë¸ ìƒíƒœ",
                    value=f"âœ… {DEFAULT_MODEL_ID.split('/')[-1]} ì¤€ë¹„ì™„ë£Œ",
                    interactive=False,
                    scale=1,
                )

            # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
            with gr.Row():
                embedding_choices = list(EMBEDDING_MODELS.keys())
                embedding_dropdown = gr.Dropdown(
                    embedding_choices,
                    label="ğŸ“Š ì„ë² ë”© ëª¨ë¸ ì„ íƒ (MTEB ìˆœìœ„ ê¸°ì¤€)",
                    value=DEFAULT_EMBEDDING_MODEL,
                    scale=2,
                    info="ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì‹œ í•´ë‹¹ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìë™ ë¡œë“œë©ë‹ˆë‹¤",
                )
                embedding_status = gr.Textbox(
                    label="ì„ë² ë”© ìƒíƒœ",
                    value=f"âœ… {EMBEDDING_MODELS[DEFAULT_EMBEDDING_MODEL]['description']} ì¤€ë¹„ì™„ë£Œ",
                    interactive=False,
                    scale=1,
                )

            additional_pdf_upload = gr.Files(
                label="ğŸ“„ ì¶”ê°€ ë¬¸ì„œ ì—…ë¡œë“œ (Vector Store í™•ì¥)", file_types=[".pdf"]
            )

            user_input = gr.Textbox(
                label="âš–ï¸ ë²•ë¥  ì§ˆì˜ë¬¸ ì…ë ¥ (Legal Query Input)",
                placeholder="ì˜ˆ: êµìˆ˜ë‹˜ì´ ë°•ì‚¬ê³¼ì • í•™ìƒì„ ì§€ë„í•  ìˆ˜ ìˆëŠ” ê¸°ê°„ì€ ì–¸ì œê¹Œì§€ì¸ê°€ìš”? (í•™ì¹™ ì œâ—‹ì¡° ê´€ë ¨)",
                info="ğŸ›ï¸ ë²•í•™ì  í•´ì„ë°©ë²•ë¡  ê¸°ë°˜ ì „ë¬¸ ë¶„ì„ | ğŸ“š ì²´ê³„ì  ë²•ë ¹ í•´ì„ ë° ì¡°ë¬¸ ì¸ìš© | ğŸ¯ Dynamic Knee Detection ë¬¸ì„œ ì„ íƒ",
                lines=3,
                interactive=True,
            )

        with gr.Column(scale=1):
            submit_btn = gr.Button("ğŸš€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", variant="primary", size="lg")
            reset_btn = gr.Button("ğŸ”„ ì´ˆê¸°í™”", size="lg")

    # íƒ­ìœ¼ë¡œ ì±„íŒ…ê³¼ ì‹œê°í™” ë¶„ë¦¬
    with gr.Tabs() as main_tabs:
        with gr.TabItem("âš–ï¸ Legal Analysis Results", id="chat_tab"):
            gr.Markdown("### ğŸ›ï¸ ì „ë¬¸ ë²•ë¥  ë¶„ì„ ë° Sensitivity ë¹„êµ")
            gr.Markdown(
                "**ë²•í•™ì  í•´ì„ë°©ë²•ë¡ ** ê¸°ë°˜ ì²´ê³„ì  ë¶„ì„ | **ì¡°ë¬¸ ì¸ìš© ë° ë²•ë¦¬ì  í•´ì„** | **ì‘ì€ Sê°’ = ì ê·¹ì  ë¬¸ì„œì„ íƒ**, **í° Sê°’ = ë³´ìˆ˜ì  ë¬¸ì„œì„ íƒ**"
            )

            # 4ê°œì˜ sensitivity ì„¤ì •ë³„ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ (2x2 ê·¸ë¦¬ë“œ)
            sensitivity_keys = list(SENSITIVITY_OPTIONS.keys())
            with gr.Row(elem_classes=["fill-height"]):
                with gr.Column(scale=1, elem_classes=["fill-height"]):
                    # ì²« ë²ˆì§¸ sensitivity ì„¤ì •
                    with gr.Group(elem_classes=["extend-height"]):
                        config1 = SENSITIVITY_OPTIONS[sensitivity_keys[0]]
                        gr.Markdown(f"### ğŸ”¹ {sensitivity_keys[0]}")
                        with gr.Row():
                            gr.Dropdown(
                                [
                                    f"Sensitivity: {config1['sensitivity']} | {config1['description']}"
                                ],
                                value=f"Sensitivity: {config1['sensitivity']} | {config1['description']}",
                                interactive=False,
                                scale=3,
                                show_label=False,
                            )
                        chatbot_sens1 = gr.Chatbot(
                            elem_classes=["extend-height"],
                            show_copy_button=True,
                            type="messages",
                        )
                        copy_btn_sens1 = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")

                    # ë‘ ë²ˆì§¸ sensitivity ì„¤ì •
                    with gr.Group(elem_classes=["extend-height"]):
                        config2 = SENSITIVITY_OPTIONS[sensitivity_keys[1]]
                        gr.Markdown(f"### ğŸ”¸ {sensitivity_keys[1]}")
                        with gr.Row():
                            gr.Dropdown(
                                [
                                    f"Sensitivity: {config2['sensitivity']} | {config2['description']}"
                                ],
                                value=f"Sensitivity: {config2['sensitivity']} | {config2['description']}",
                                interactive=False,
                                scale=3,
                                show_label=False,
                            )
                        chatbot_sens2 = gr.Chatbot(
                            elem_classes=["extend-height"],
                            show_copy_button=True,
                            type="messages",
                        )
                        copy_btn_sens2 = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")

                with gr.Column(scale=1, elem_classes=["fill-height"]):
                    # ì„¸ ë²ˆì§¸ sensitivity ì„¤ì •
                    with gr.Group(elem_classes=["extend-height"]):
                        config3 = SENSITIVITY_OPTIONS[sensitivity_keys[2]]
                        gr.Markdown(f"### ğŸ”¶ {sensitivity_keys[2]}")
                        with gr.Row():
                            gr.Dropdown(
                                [
                                    f"Sensitivity: {config3['sensitivity']} | {config3['description']}"
                                ],
                                value=f"Sensitivity: {config3['sensitivity']} | {config3['description']}",
                                interactive=False,
                                scale=3,
                                show_label=False,
                            )
                        chatbot_sens3 = gr.Chatbot(
                            elem_classes=["extend-height"],
                            show_copy_button=True,
                            type="messages",
                        )
                        copy_btn_sens3 = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")

                    # ë„¤ ë²ˆì§¸ sensitivity ì„¤ì •
                    with gr.Group(elem_classes=["extend-height"]):
                        config4 = SENSITIVITY_OPTIONS[sensitivity_keys[3]]
                        gr.Markdown(f"### ğŸ”¥ {sensitivity_keys[3]}")
                        with gr.Row():
                            gr.Dropdown(
                                [
                                    f"Sensitivity: {config4['sensitivity']} | {config4['description']}"
                                ],
                                value=f"Sensitivity: {config4['sensitivity']} | {config4['description']}",
                                interactive=False,
                                scale=3,
                                show_label=False,
                            )
                        chatbot_sens4 = gr.Chatbot(
                            elem_classes=["extend-height"],
                            show_copy_button=True,
                            type="messages",
                        )
                        copy_btn_sens4 = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")

        with gr.TabItem("ğŸ“Š Legal Document Analysis Visualization", id="viz_tab"):
            gr.Markdown("### ğŸ“ˆ ë²•ë¥  ë¬¸ì„œ ì„ íƒ íŒ¨í„´ ì‹œê°í™”")
            gr.Markdown(
                "**ë²•ë ¹ë³„ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™”** | **S=1 (ì ê·¹ì ) â†’ S=10 (ë³´ìˆ˜ì )** ìˆœìœ¼ë¡œ ê° sensitivityì˜ knee point ê°ì§€ íŒ¨í„´ê³¼ **ë²•ì  ê·¼ê±° ë¬¸ì„œ ì„ íƒ ê³¼ì •**ì„ ì‹œê°ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."
            )

            with gr.Row():
                viz_refresh_btn = gr.Button("ğŸ”„ ê·¸ë˜í”„ ìƒˆë¡œê³ ì¹¨", variant="secondary")
                viz_save_btn = gr.Button("ğŸ’¾ ê·¸ë˜í”„ ì €ì¥", variant="secondary")

            # 4ê°œì˜ sensitivityë³„ ì‹œê°í™” ê²°ê³¼ (2x2 ê·¸ë¦¬ë“œ)
            with gr.Row():
                with gr.Column(scale=1):
                    config1 = SENSITIVITY_OPTIONS[sensitivity_keys[0]]
                    gr.Markdown(f"#### ğŸ”¹ {sensitivity_keys[0]}")
                    viz_image_sens1 = gr.Image(
                        label=f"Sensitivity: {config1['sensitivity']} | {config1['description']}",
                        show_label=True,
                        interactive=False,
                        height=400,
                    )

                    config2 = SENSITIVITY_OPTIONS[sensitivity_keys[1]]
                    gr.Markdown(f"#### ğŸ”¸ {sensitivity_keys[1]}")
                    viz_image_sens2 = gr.Image(
                        label=f"Sensitivity: {config2['sensitivity']} | {config2['description']}",
                        show_label=True,
                        interactive=False,
                        height=400,
                    )

                with gr.Column(scale=1):
                    config3 = SENSITIVITY_OPTIONS[sensitivity_keys[2]]
                    gr.Markdown(f"#### ğŸ”¶ {sensitivity_keys[2]}")
                    viz_image_sens3 = gr.Image(
                        label=f"Sensitivity: {config3['sensitivity']} | {config3['description']}",
                        show_label=True,
                        interactive=False,
                        height=400,
                    )

                    config4 = SENSITIVITY_OPTIONS[sensitivity_keys[3]]
                    gr.Markdown(f"#### ğŸ”¥ {sensitivity_keys[3]}")
                    viz_image_sens4 = gr.Image(
                        label=f"Sensitivity: {config4['sensitivity']} | {config4['description']}",
                        show_label=True,
                        interactive=False,
                        height=400,
                    )

    # ì‹œê°í™” ê´€ë ¨ í•¨ìˆ˜ë“¤
    def refresh_visualizations():
        """ëª¨ë“  sensitivity ì„¤ì •ì˜ knee detection ê²°ê³¼ë¥¼ ì‹œê°í™”"""
        results = get_all_knee_visualizations()
        sensitivity_keys = list(SENSITIVITY_OPTIONS.keys())
        return (
            results.get(sensitivity_keys[0], "No data available"),
            results.get(sensitivity_keys[1], "No data available"),
            results.get(sensitivity_keys[2], "No data available"),
            results.get(sensitivity_keys[3], "No data available"),
        )

    def save_visualizations():
        """ì‹œê°í™” ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_dir = Path(f"sensitivity_visualizations_{timestamp}")
            save_dir.mkdir(exist_ok=True)

            # Sensitivity ê¸°ë°˜ íŒŒì¼ëª… ë§¤í•‘
            sensitivity_keys = list(SENSITIVITY_OPTIONS.keys())
            methods_mapping = {}
            for key in sensitivity_keys:
                config = SENSITIVITY_OPTIONS[key]
                clean_name = f"sensitivity_{config['sensitivity']}"
                methods_mapping[key] = clean_name

            saved_files = []
            with shared_state_lock:
                retrievers = shared_state.get("last_retrievers", {})

            for method, retriever in retrievers.items():
                try:
                    save_name = methods_mapping.get(
                        method,
                        method.replace(" ", "_").replace("(", "").replace(")", ""),
                    )
                    save_path = save_dir / f"{save_name}.png"

                    if hasattr(retriever, "visualize_knee_detection"):
                        retriever.visualize_knee_detection(str(save_path))
                        saved_files.append(str(save_path))
                except Exception as e:
                    print(f"Save failed - {method}: {e}")

            if saved_files:
                return f"{len(saved_files)} graphs have been saved:\n" + "\n".join(
                    saved_files
                )
            else:
                return "No graphs available to save. Please run a query first."
        except Exception as e:
            return f"Save failed: {str(e)}"

    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ (Generatorë¡œ ìˆ˜ì •)
    def init_client_on_first_query(user_query, request: gr.Request):
        session_id = get_session_id(request)
        with session_lock:
            if session_id not in sessions:
                init_session(session_id)
            if sessions[session_id]["client"] is None:
                sessions[session_id]["client"] = get_client(
                    shared_state["current_model"]
                )

        # Generatorë¥¼ ì œëŒ€ë¡œ yield
        for result in handle_multi_query(user_query, request):
            yield result

    # ëª¨ë¸ ë³€ê²½ ì´ë²¤íŠ¸
    model_dropdown.change(
        fn=change_model, inputs=[model_dropdown], outputs=[model_status]
    )

    # ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì´ë²¤íŠ¸
    embedding_dropdown.change(
        fn=change_embedding_model,
        inputs=[embedding_dropdown],
        outputs=[embedding_status, database_status],
    )

    # ì¶”ê°€ PDF ì—…ë¡œë“œ
    additional_pdf_upload.upload(
        fn=handle_additional_pdf_upload,
        inputs=[additional_pdf_upload],
        outputs=[database_status],
    )

    # ë©€í‹° ì¿¼ë¦¬ ì²˜ë¦¬ (Sensitivity ê¸°ë°˜)
    submit_btn.click(
        fn=init_client_on_first_query,
        inputs=[user_input],
        outputs=[
            chatbot_sens1,
            chatbot_sens2,
            chatbot_sens3,
            chatbot_sens4,
        ],
    )

    user_input.submit(
        fn=init_client_on_first_query,
        inputs=[user_input],
        outputs=[
            chatbot_sens1,
            chatbot_sens2,
            chatbot_sens3,
            chatbot_sens4,
        ],
    )

    # ì´ˆê¸°í™”
    reset_btn.click(
        fn=reset_all_chats,
        outputs=[
            chatbot_sens1,
            chatbot_sens2,
            chatbot_sens3,
            chatbot_sens4,
        ],
    )

    # ì‹œê°í™” ì´ë²¤íŠ¸ ì—°ê²°
    viz_refresh_btn.click(
        fn=refresh_visualizations,
        inputs=[],
        outputs=[viz_image_sens1, viz_image_sens2, viz_image_sens3, viz_image_sens4],
    )

    viz_save_btn.click(
        fn=save_visualizations,
        inputs=[],
        outputs=[gr.Textbox(visible=False)],  # ê²°ê³¼ë¥¼ ì½˜ì†”ì—ë§Œ í‘œì‹œ
    )

    # ë³µì‚¬ ê¸°ëŠ¥ (Sensitivity ê¸°ë°˜)
    sensitivity_keys = list(SENSITIVITY_OPTIONS.keys())

    copy_btn_sens1.click(
        fn=lambda h: copy_as_markdown(h, sensitivity_keys[0]),
        inputs=[chatbot_sens1],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_sens2.click(
        fn=lambda h: copy_as_markdown(h, sensitivity_keys[1]),
        inputs=[chatbot_sens2],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_sens3.click(
        fn=lambda h: copy_as_markdown(h, sensitivity_keys[2]),
        inputs=[chatbot_sens3],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

    copy_btn_sens4.click(
        fn=lambda h: copy_as_markdown(h, sensitivity_keys[3]),
        inputs=[chatbot_sens4],
        outputs=[gr.Textbox(visible=False)],
        js="(result) => navigator.clipboard.writeText(result)",
    )

if __name__ == "__main__":
    print("âš–ï¸ GIST Legal Rules Analyzer (Professional Legal Assistant) ì¤€ë¹„ì™„ë£Œ!")
    print("ğŸ›ï¸ ë²•í•™ì  í•´ì„ë°©ë²•ë¡  ê¸°ë°˜ ì „ë¬¸ ë²•ë¥  ë¶„ì„ ì‹œìŠ¤í…œ")
    print("ğŸ“š ì²´ê³„ì  ë²•ë ¹ í•´ì„: ë¬¸ë¦¬í•´ì„ â†’ ì²´ê³„ì í•´ì„ â†’ ëª©ì ë¡ ì í•´ì„ â†’ ìš°ì„ ìˆœìœ„ì ìš©")
    print("ğŸ“‹ Legal Template System: ì „ë¬¸ ì‹œìŠ¤í…œí”„ë¡¬í”„íŠ¸ + ì¿¼ë¦¬í…œí”Œë¦¿ ì ìš©")
    print("ğŸ¯ S=1,3,5,10 sensitivityë¡œ ë²•ì  ê·¼ê±° ë¬¸ì„œ ì„ íƒ ìµœì í™”")
    print("âš¡ LiteLLM í†µí•©ìœ¼ë¡œ ë‹¤ì–‘í•œ LLM í”„ë¡œë°”ì´ë” ì§€ì›")
    print("ğŸŒ http://localhost:7860 ì—ì„œ ì‹¤í–‰ ì¤‘...")
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

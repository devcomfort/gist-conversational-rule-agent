# í•™ì¹™ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ - LangChain ëŒ€ì²´ ì „ëµ ë° êµ¬í˜„ ë°©ì•ˆ

## ğŸ¯ **ê°œìš”**

ë³¸ ë¬¸ì„œëŠ” í•™ì¹™ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ í•µì‹¬ ëª¨ë“ˆì¸ **app.py**ì—ì„œ ì‚¬ìš© ì¤‘ì¸ LangChain ìƒíƒœê³„ì˜ ê° êµ¬ì„±ìš”ì†Œë¥¼ ë¶„ì„í•˜ê³ , í˜„ëŒ€ì  AI í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•œ ëŒ€ì²´ ê°€ëŠ¥ì„±ì„ ì „ëµì ìœ¼ë¡œ ê²€í† í•©ë‹ˆë‹¤.

---

## ğŸ“Š **í˜„ì¬ LangChain ì˜ì¡´ì„± ë¶„ì„**

### **app.py LangChain ì˜ì¡´ì„± ë§¤íŠ¸ë¦­ìŠ¤**

| ì»´í¬ë„ŒíŠ¸                        | ë¼ì¸ìˆ˜ | ì‚¬ìš© ë¹ˆë„ | ëŒ€ì²´ ë‚œì´ë„   | í•µì‹¬ ê¸°ëŠ¥                |
| ------------------------------- | ------ | --------- | ------------- | ------------------------ |
| **langchain-community.FAISS**   | 30, 93 | â­â­â­â­      | ğŸŸ¡ ë³´í†µ        | ë²¡í„° ìŠ¤í† ì–´              |
| **langchain-huggingface**       | 32, 54 | â­â­        | ğŸŸ¢ ì‰¬ì›€        | ì„ë² ë”© ëª¨ë¸              |
| **langchain-text-splitters**    | 33, 56 | â­â­        | ğŸŸ¢ ì‰¬ì›€        | í…ìŠ¤íŠ¸ ì²­í‚¹              |
| **langchain-core.Document**     | 31, 90 | â­â­        | ğŸŸ¢ ì‰¬ì›€        | ë¬¸ì„œ ê°ì²´                |
| **ì „ì²´ LangChain ì˜ì¡´ë„**       | -      | -         | ğŸŸ¡ **ë³´í†µ**    | RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ ì œì–´ |

### **ì˜ì¡´ì„±ë³„ ìƒì„¸ ë¶„ì„**

#### **ğŸŸ¡ ë³´í†µ: FAISS VectorStore**
**ë³µì¡ì„± ìš”ì¸**:
- **LangChain FAISS ë˜í¼**: ì§ì ‘ FAISS ì‚¬ìš© ëŒ€ë¹„ ì¶”ìƒí™” ê³„ì¸µ ì¶”ê°€
- **Document ê°ì²´ ì˜ì¡´**: LangChain Document í˜•ì‹ í•„ìˆ˜
- **ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤**: `.as_retriever()` ë° `.invoke()` ë©”ì„œë“œ ì²´ì¸

**í˜„ì¬ êµ¬í˜„**:
```python
# ë¼ì¸ 93: LangChain FAISS ìƒì„±
return FAISS.from_documents(all_docs, EMBED_MODEL)

# ë¼ì¸ 133-135: LangChain ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤
retriever = vectorstore.as_retriever()
docs = retriever.invoke(user_query)
context = "\n".join([doc.page_content for doc in docs])
```

**ëŒ€ì²´ ê°€ëŠ¥ì„±**: âœ… **ì™„ì „ ê°€ëŠ¥** - LlamaIndex/ì§ì ‘ êµ¬í˜„ìœ¼ë¡œ 1:1 ëŒ€ì²´

#### **ğŸŸ¢ ì‰¬ì›€: HuggingFace ì„ë² ë”©**
**ë‹¨ìˆœì„± ìš”ì¸**:
- **í‘œì¤€ ëª¨ë¸**: sentence-transformers/all-MiniLM-L6-v2
- **ë‹¨ìˆœ ì„¤ì •**: ëª¨ë¸ëª…ë§Œ ì§€ì •
- **ìµœì†Œ ì‚¬ìš©**: ì„ë² ë”© ëª¨ë¸ ì •ì˜ í•œ ê³³ì—ì„œë§Œ ì‚¬ìš©

**í˜„ì¬ êµ¬í˜„**:
```python
# ë¼ì¸ 54: HuggingFace ì„ë² ë”© ì„¤ì •
EMBED_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
```

**ëŒ€ì²´ ê°€ëŠ¥ì„±**: âœ… **ì¦‰ì‹œ ê°€ëŠ¥** - sentence-transformers ì§ì ‘ ì‚¬ìš©

#### **ğŸŸ¢ ì‰¬ì›€: RecursiveCharacterTextSplitter**
**ë‹¨ìˆœì„± ìš”ì¸**:
- **ê¸°ë³¸ ì„¤ì •**: chunk_size=500, chunk_overlap=50
- **í‘œì¤€ ì‚¬ìš©**: ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ë¶„í•  íŒ¨í„´
- **ë‹¨ì¼ ëª©ì **: PDF í…ìŠ¤íŠ¸ ì²­í‚¹ì—ë§Œ ì‚¬ìš©

**í˜„ì¬ êµ¬í˜„**:
```python
# ë¼ì¸ 56: í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

# ë¼ì¸ 91: ë¬¸ì„œ ë¶„í• 
docs = TEXT_SPLITTER.split_documents([doc])
```

**ëŒ€ì²´ ê°€ëŠ¥ì„±**: âœ… **ì¦‰ì‹œ ê°€ëŠ¥** - ì§ì ‘ êµ¬í˜„ ë˜ëŠ” ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

#### **ğŸŸ¢ ì‰¬ì›€: Document ê°ì²´**
**ë‹¨ìˆœì„± ìš”ì¸**:
- **ë°ì´í„° ì»¨í…Œì´ë„ˆ**: page_content + metadata ë‹¨ìˆœ êµ¬ì¡°
- **ìµœì†Œ ì‚¬ìš©**: PDF í…ìŠ¤íŠ¸ ë˜í•‘ ìš©ë„ë§Œ
- **í‘œì¤€ íŒ¨í„´**: ì¼ë°˜ì ì¸ ë¬¸ì„œ ì¶”ìƒí™”

**í˜„ì¬ êµ¬í˜„**:
```python
# ë¼ì¸ 90: Document ê°ì²´ ìƒì„±
doc = Document(page_content=text, metadata={"source": pdf})
```

**ëŒ€ì²´ ê°€ëŠ¥ì„±**: âœ… **ì¦‰ì‹œ ê°€ëŠ¥** - ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ë‹¤ë¥¸ ë¬¸ì„œ í´ë˜ìŠ¤ ì‚¬ìš©

---

## ğŸš€ **ëŒ€ì²´ ì „ëµë³„ êµ¬í˜„ ë°©ì•ˆ**

## **ì „ëµ 1: LlamaIndex ì¤‘ì‹¬ ì „í™˜** â­â­â­â­â­

### **ğŸ¦™ ì™„ì „ LlamaIndex ìƒíƒœê³„ êµ¬ì¶•**

#### **í•µì‹¬ ë³€í™˜ ë§¤í•‘**

| LangChain ì»´í¬ë„ŒíŠ¸             | LlamaIndex ëŒ€ì²´ì¬               | ë³€í™˜ ë‚œì´ë„ |
| ------------------------------ | ------------------------------- | ----------- |
| `FAISS.from_documents()`       | `VectorStoreIndex.from_documents()` | ğŸŸ¢ ì‰¬ì›€      |
| `HuggingFaceEmbeddings()`      | `HuggingFaceEmbedding()`        | ğŸŸ¢ ì‰¬ì›€      |
| `RecursiveCharacterTextSplitter` | `SentenceSplitter`              | ğŸŸ¢ ì‰¬ì›€      |
| `Document(page_content=...)`   | `Document(text=...)`            | ğŸŸ¢ ì‰¬ì›€      |
| `retriever.invoke()`           | `query_engine.query()`          | ğŸŸ¡ ë³´í†µ      |

#### **êµ¬í˜„ ì˜ˆì‹œ**

**1. ì„¤ì • ë° ì´ˆê¸°í™”**
```python
# í˜„ì¬: LangChain ì„¤ì • (ë¼ì¸ 54-56)
EMBED_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

# ëŒ€ì²´ì•ˆ: LlamaIndex ì„¤ì •
from llama_index.core import Settings, VectorStoreIndex, Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.text_splitter import SentenceSplitter
from llama_index.vector_stores.faiss import FaissVectorStore

# ê¸€ë¡œë²Œ ì„¤ì •
Settings.embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
Settings.text_splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)
```

**2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±**
```python
# í˜„ì¬: LangChain FAISS (ë¼ì¸ 86-93)
def create_vectorstore_from_pdfs(pdfs):
    all_docs = []
    for pdf in pdfs:
        text = extract_text_from_pdf(pdf)
        doc = Document(page_content=text, metadata={"source": pdf})
        docs = TEXT_SPLITTER.split_documents([doc])
        all_docs.extend(docs)
    return FAISS.from_documents(all_docs, EMBED_MODEL)

# ëŒ€ì²´ì•ˆ: LlamaIndex (50% ì½”ë“œ ê°ì†Œ)
def create_vectorstore_from_pdfs(pdfs):
    documents = []
    for pdf in pdfs:
        text = extract_text_from_pdf(pdf)
        doc = Document(text=text, metadata={"source": pdf})
        documents.append(doc)
    
    # ìë™ ì²­í‚¹ ë° ì„ë² ë”©
    return VectorStoreIndex.from_documents(documents)
```

**3. ì§ˆì˜ì‘ë‹µ**
```python
# í˜„ì¬: LangChain ê²€ìƒ‰ + LLM (ë¼ì¸ 130-143)
if vectorstore:
    retriever = vectorstore.as_retriever()
    docs = retriever.invoke(user_query)
    context = "\n".join([doc.page_content for doc in docs])
messages.append({"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_query}"})
completion = client.chat.completions.create(model=session["model_id"], messages=messages)

# ëŒ€ì²´ì•ˆ: LlamaIndex í†µí•© ì¿¼ë¦¬ (70% ì½”ë“œ ê°ì†Œ)
if index:
    query_engine = index.as_query_engine(
        llm=get_llm_wrapper(session["model_id"]),  # LLM ë˜í¼ í•¨ìˆ˜
        response_mode="compact"
    )
    response = query_engine.query(user_query)
    return str(response)
```

#### **ğŸ¯ LlamaIndex ì „í™˜ ì¥ì **

**ê°œë°œì ê²½í—˜ (DX) ê·¹ëŒ€í™”**:
- âœ… **50% ì½”ë“œ ê°ì†Œ**: ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ â†’ ê°„ë‹¨í•œ ì¸ë±ìŠ¤ ìƒì„±
- âœ… **í†µí•© API**: ê²€ìƒ‰ + ìƒì„±ì´ í•˜ë‚˜ì˜ ë©”ì„œë“œë¡œ í†µí•©
- âœ… **ìë™ ìµœì í™”**: ì²­í‚¹, ì„ë² ë”©, ê²€ìƒ‰ì´ ìë™ìœ¼ë¡œ ìµœì í™”
- âœ… **í’ë¶€í•œ ê¸°ëŠ¥**: ê³ ê¸‰ ê²€ìƒ‰ ëª¨ë“œ, ë©”íƒ€ë°ì´í„° í•„í„°ë§ ë‚´ì¥

**ì‚¬ìš©ì ê²½í—˜ (UX) ê°œì„ **:
- âœ… **ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ**: LlamaIndexì˜ ê³ ê¸‰ RAG ì•Œê³ ë¦¬ì¦˜
- âœ… **ë¹ ë¥¸ ì‘ë‹µ**: ìµœì í™”ëœ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸
- âœ… **ì¶œì²˜ ì¶”ì **: ìë™ìœ¼ë¡œ ë¬¸ì„œ ì¶œì²˜ ì •ë³´ í¬í•¨

**ì„±ëŠ¥ ìµœì í™”**:
- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ë” íš¨ìœ¨ì ì¸ ì¸ë±ìŠ¤ êµ¬ì¡°
- âœ… **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ë¬¸ì„œ ë™ì‹œ ì²˜ë¦¬ ìµœì í™”
- âœ… **ìºì‹±**: ë‚´ì¥ëœ ì¿¼ë¦¬ ìºì‹± ì‹œìŠ¤í…œ

#### **âš ï¸ LlamaIndex ì „í™˜ ê³ ë ¤ì‚¬í•­**

**ì „í™˜ ë¹„ìš©**:
- âŒ **í•™ìŠµ ê³¡ì„ **: ìƒˆë¡œìš´ API ë° ê°œë… í•™ìŠµ í•„ìš”
- âŒ **í…ŒìŠ¤íŠ¸ í•„ìš”**: ê¸°ì¡´ ê¸°ëŠ¥ê³¼ì˜ ë™ë“±ì„± ê²€ì¦
- âŒ **ì˜ì¡´ì„± ë³€ê²½**: pyproject.toml ì—…ë°ì´íŠ¸ í•„ìš”

**ê¸°ëŠ¥ì  ì°¨ì´**:
- âŒ **ì„¸ë¶€ ì œì–´**: LangChain ëŒ€ë¹„ low-level ì œì–´ ì œí•œ
- âŒ **ì»¤ìŠ¤í…€ ë¡œì§**: í˜„ì¬ì˜ ì •êµí•œ ì„¸ì…˜ ê´€ë¦¬ ì¬êµ¬í˜„ í•„ìš”

---

## **ì „ëµ 2: SmolAgents ì¤‘ì‹¬ ëª¨ë“ˆí™”** â­â­â­â­

### **ğŸ­ ë„êµ¬ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì¬êµ¬ì„±**

#### **ëª¨ë“ˆ ë¶„í•´ ì „ëµ**

```python
from smolagents import tool, CodeAgent
import chromadb
from sentence_transformers import SentenceTransformer
import litellm

@tool
def process_pdf_documents(pdf_files: list) -> str:
    """PDF íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì €ì¥"""
    # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = []
    for pdf_file in pdf_files:
        text = extract_text_from_pdf(pdf_file)
        chunks = split_text(text, chunk_size=500, overlap=50)
        documents.extend(chunks)
    
    # ChromaDBì— ì €ì¥
    client = chromadb.Client()
    collection = client.get_or_create_collection("academic_rules")
    
    # ì„ë² ë”© ìƒì„± ë° ì €ì¥
    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    embeddings = embedder.encode(documents)
    
    collection.add(
        documents=documents,
        embeddings=embeddings.tolist(),
        ids=[f"doc_{i}" for i in range(len(documents))]
    )
    
    return f"âœ… {len(pdf_files)}ê°œ PDF íŒŒì¼ì—ì„œ {len(documents)}ê°œ ë¬¸ì„œ ì²­í¬ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."

@tool
def search_academic_rules(query: str, top_k: int = 3) -> str:
    """í•™ì¹™ ë° ê·œì •ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰"""
    client = chromadb.Client()
    collection = client.get_collection("academic_rules")
    
    # ì¿¼ë¦¬ ì„ë² ë”©
    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    query_embedding = embedder.encode([query])
    
    # ê²€ìƒ‰ ì‹¤í–‰
    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=top_k
    )
    
    relevant_docs = results['documents'][0]
    return "\n\n".join(relevant_docs)

@tool
def answer_academic_question(question: str, context: str, model: str = "gpt-4") -> str:
    """ê²€ìƒ‰ëœ í•™ì¹™ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€"""
    prompt = f"""
    ë‹¤ìŒ í•™ì¹™/ê·œì • ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.

    ê´€ë ¨ ê·œì •:
    {context}

    ì§ˆë¬¸: {question}
    
    ë‹µë³€ ì‹œ ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
    1. í•´ë‹¹ ê·œì •ì˜ í•µì‹¬ ë‚´ìš©
    2. êµ¬ì²´ì ì¸ ì ˆì°¨ë‚˜ ì¡°ê±´
    3. ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì˜ˆì™¸ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
    """
    
    response = litellm.completion(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.1
    )
    
    return response.choices[0].message.content

# SmolAgents ì—ì´ì „íŠ¸ ìƒì„±
def create_academic_agent():
    return CodeAgent(
        tools=[process_pdf_documents, search_academic_rules, answer_academic_question],
        system_message="""
        í•™ì¹™ ë° ëŒ€í•™ ê·œì • ì „ë¬¸ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        
        ì£¼ìš” ê¸°ëŠ¥:
        1. PDF í˜•íƒœì˜ í•™ì¹™/ê·œì • ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥
        2. í•™ì¹™ ë‚´ìš©ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰  
        3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ì œê³µ
        
        í•­ìƒ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ë¬¸ì„œ ê²€ìƒ‰ì„ ë¨¼ì € ìˆ˜í–‰í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.
        """
    )

# ì‚¬ìš©ë²•
agent = create_academic_agent()
result = agent.run("ì¡¸ì—… ìš”ê±´ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”")
```

#### **ğŸ¯ SmolAgents ì „í™˜ ì¥ì **

**ê°œë°œì ê²½í—˜ (DX) í˜ì‹ **:
- âœ… **ê·¹ë„ì˜ ë‹¨ìˆœì„±**: ë³µì¡í•œ RAG íŒŒì´í”„ë¼ì¸ â†’ @tool í•¨ìˆ˜ë¡œ ë¶„í•´
- âœ… **ë…ë¦½ì  í…ŒìŠ¤íŠ¸**: ê° ë„êµ¬ë³„ë¡œ ê°œë³„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- âœ… **ì§ê´€ì  ë””ë²„ê¹…**: ë„êµ¬ ì‹¤í–‰ ê³¼ì •ì´ ëª…í™•í•˜ê²Œ í‘œì‹œ
- âœ… **ë¬´í•œ í™•ì¥**: ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ë„êµ¬ë¡œ ì‰½ê²Œ ì¶”ê°€

**ì‚¬ìš©ì ê²½í—˜ (UX) í˜ì‹ **:
- âœ… **íˆ¬ëª…í•œ ê³¼ì •**: ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•˜ëŠ” ë‹¨ê³„ë³„ ê³¼ì • í‘œì‹œ
- âœ… **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”**: "ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ê² ìŠµë‹ˆë‹¤"ì™€ ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ í”¼ë“œë°±
- âœ… **ì˜¤ë¥˜ ë³µêµ¬**: íŠ¹ì • ë„êµ¬ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ìë™ ì¬ì‹œë„

**ì•„í‚¤í…ì²˜ ê°œì„ **:
- âœ… **ëª¨ë“ˆí™”**: ê° ê¸°ëŠ¥ì´ ë…ë¦½ì ì¸ ë„êµ¬ë¡œ ë¶„ë¦¬
- âœ… **ì¬ì‚¬ìš©ì„±**: ë„êµ¬ë¥¼ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ì„œë„ í™œìš© ê°€ëŠ¥
- âœ… **ìœ ì§€ë³´ìˆ˜**: íŠ¹ì • ê¸°ëŠ¥ ìˆ˜ì • ì‹œ í•´ë‹¹ ë„êµ¬ë§Œ ìˆ˜ì •

#### **âš ï¸ SmolAgents ì „í™˜ ë‹¨ì **

**ê¸°ëŠ¥ì  ì œí•œ**:
- âŒ **RAG ìµœì í™” ë¶€ì¡±**: ì „ë¬¸ RAG í”„ë ˆì„ì›Œí¬ ëŒ€ë¹„ ê²€ìƒ‰ ì„±ëŠ¥ ì œí•œ
- âŒ **ì„¸ì…˜ ê´€ë¦¬ ë³µì¡ì„±**: í˜„ì¬ì˜ ì •êµí•œ ì„¸ì…˜ ê´€ë¦¬ë¥¼ ë„êµ¬ë¡œ êµ¬í˜„í•˜ê¸° ì–´ë ¤ì›€
- âŒ **ë™ì‹œì„± ì²˜ë¦¬**: ì—¬ëŸ¬ ì‚¬ìš©ì ë™ì‹œ ì²˜ë¦¬ ë¡œì§ ë³µì¡ë„ ì¦ê°€

**ì„±ëŠ¥ ìš°ë ¤**:
- âŒ **ì‘ë‹µ ì§€ì—°**: ë„êµ¬ ê°„ ì—°ê²°ë¡œ ì¸í•œ ì¶”ê°€ ì§€ì—° ì‹œê°„
- âŒ **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ê° ë„êµ¬ë³„ ë…ë¦½ì ì¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©

---

## **ì „ëµ 3: LiteLLM ì¤‘ì‹¬ í†µí•©** â­â­â­â­â­

### **âš¡ LLM ë ˆì´ì–´ í˜„ëŒ€í™”**

#### **í˜„ì¬ ë‹¤ì¤‘ LLM êµ¬ì¡° ë¶„ì„**

```python
# í˜„ì¬ êµ¬í˜„: ë³µì¡í•œ providerë³„ ê´€ë¦¬ (ë¼ì¸ 47-53, 158-169)
MODELS = {
    "GPT-4": {"model_id": "gpt-4", "provider": "openai"},
    "DeepSeek-R1": {"model_id": "deepseek-ai/DeepSeek-R1", "provider": "novita"},
    "Gemma-3-27B": {"model_id": "google/gemma-3-27b-it", "provider": "hf-inference"},
    "Llama-3.3-70B": {"model_id": "meta-llama/Llama-3.3-70B-Instruct", "provider": "hf-inference"},
    "QwQ-32B": {"model_id": "Qwen/QwQ-32B", "provider": "hf-inference"},
}

def create_client(provider):
    if provider == "openai":
        return openai.Client(api_key=OPENAI_API_KEY)
    return InferenceClient(
        provider=provider, 
        api_key=HF_API_KEY, 
        headers={"X-HF-Bill-To": HF_ENTERPRISE},
    )
```

#### **LiteLLM í†µí•© êµ¬í˜„**

```python
# ëŒ€ì²´ì•ˆ: LiteLLM ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤
import litellm
from litellm import completion

# ëª¨ë¸ ì„¤ì • ë‹¨ìˆœí™”
MODELS = {
    "GPT-4": "gpt-4",
    "GPT-4 Turbo": "gpt-4-turbo-preview", 
    "Claude-3 Opus": "claude-3-opus",
    "Gemini Pro": "gemini-pro",
    "DeepSeek-R1": "deepseek-ai/DeepSeek-R1",
    "Llama-3.3-70B": "meta-llama/Llama-3.3-70B-Instruct",
    "Command-R+": "command-r-plus",
    "Cohere": "cohere.command-r-v01",
}

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ìë™ ì¸ì‹)
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["ANTHROPIC_API_KEY"] = os.getenv("ANTHROPIC_API_KEY")
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")

def query_llm(model_name: str, messages: list, **kwargs) -> str:
    """í†µí•© LLM í˜¸ì¶œ í•¨ìˆ˜"""
    try:
        response = litellm.completion(
            model=MODELS[model_name],
            messages=messages,
            temperature=kwargs.get("temperature", 0.1),
            max_tokens=kwargs.get("max_tokens", 1000),
            # ìë™ ì¬ì‹œë„ ë° í´ë°±
            fallbacks=["gpt-4", "claude-3-sonnet", "gemini-pro"]
        )
        return response.choices[0].message.content
    
    except Exception as e:
        # ìë™ ëŒ€ì²´ ëª¨ë¸ ì‹œë„
        print(f"âš ï¸ {model_name} ì‹¤íŒ¨, ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: {e}")
        return litellm.completion(
            model="gpt-4",  # ì•ˆì „í•œ í´ë°± ëª¨ë¸
            messages=messages,
            **kwargs
        ).choices[0].message.content

# ê¸°ì¡´ handle_query í•¨ìˆ˜ ë‹¨ìˆœí™”
def handle_query(user_query, request: gr.Request):
    session_id = get_session_id(request)
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        session = sessions[session_id]
        sessions.move_to_end(session_id)
    
    # ë²¡í„° ê²€ìƒ‰ (ê¸°ì¡´ê³¼ ë™ì¼)
    context = ""
    if session["vectorstore"]:
        retriever = session["vectorstore"].as_retriever()
        docs = retriever.invoke(user_query)
        context = "\n".join([doc.page_content for doc in docs])
    
    # LiteLLMìœ¼ë¡œ ë‹¨ìˆœí™”ëœ LLM í˜¸ì¶œ
    messages = session["history"].copy()
    messages.append({
        "role": "user", 
        "content": f"Context:\n{context}\n\nQuestion: {user_query}"
    })
    
    # í†µí•©ëœ LLM í˜¸ì¶œ
    bot_response = query_llm(
        model_name=session["current_model"],  # UIì—ì„œ ì„ íƒëœ ëª¨ë¸
        messages=messages
    )
    
    # ê¸°ë¡ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ê³¼ ë™ì¼)
    session["history"].append({"role": "user", "content": user_query})
    session["history"].append({"role": "assistant", "content": bot_response})
    
    return session["history"]
```

#### **ğŸ¯ LiteLLM ì „í™˜ ì¥ì **

**ê°œë°œì ê²½í—˜ (DX) ë‹¨ìˆœí™”**:
- âœ… **90% ì½”ë“œ ê°ì†Œ**: ë³µì¡í•œ í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ â†’ ë‹¨ì¼ í•¨ìˆ˜ í˜¸ì¶œ
- âœ… **í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤**: ëª¨ë“  LLMì„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
- âœ… **ìë™ ì˜¤ë¥˜ ì²˜ë¦¬**: ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ëŒ€ì²´ ëª¨ë¸ ì‹œë„
- âœ… **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì‚¬ìš©ëŸ‰, ë¹„ìš©, ì„±ëŠ¥ ìë™ ì¶”ì 

**ì‚¬ìš©ì ê²½í—˜ (UX) ì•ˆì •ì„±**:
- âœ… **ì„œë¹„ìŠ¤ ì•ˆì •ì„±**: íŠ¹ì • ì œê³µì ì¥ì•  ì‹œì—ë„ ì„œë¹„ìŠ¤ ì§€ì†
- âœ… **ìµœì  ì„±ëŠ¥**: ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ìë™ ëª¨ë¸ ì„ íƒ
- âœ… **ë” ë§ì€ ì„ íƒ**: 50+ LLM ëª¨ë¸ ì§€ì›

**ìš´ì˜ íš¨ìœ¨ì„±**:
- âœ… **ë¹„ìš© ìµœì í™”**: ì‹¤ì‹œê°„ ê°€ê²© ë¹„êµ ë° ìµœì  ì„ íƒ
- âœ… **ë¡œë“œ ë°¸ëŸ°ì‹±**: ì—¬ëŸ¬ ì œê³µì ê°„ ìë™ ë¶€í•˜ ë¶„ì‚°
- âœ… **ì‚¬ìš©ëŸ‰ ë¶„ì„**: ìƒì„¸í•œ API í˜¸ì¶œ í†µê³„ ì œê³µ

#### **ğŸ“Š LiteLLM ì„±ëŠ¥ ê°œì„ **

| ë©”íŠ¸ë¦­                | í˜„ì¬ êµ¬í˜„  | LiteLLM ì „í™˜ | ê°œì„  íš¨ê³¼ |
| --------------------- | ---------- | ------------ | --------- |
| **ì½”ë“œ ë³µì¡ë„**       | ë³µì¡       | ë‹¨ìˆœ         | **-90%**  |
| **ëª¨ë¸ ì „í™˜ ì‹œê°„**    | ~2-3ì´ˆ     | ~0.1ì´ˆ       | **-95%**  |
| **ì˜¤ë¥˜ ë³µêµ¬ ì‹œê°„**    | ìˆ˜ë™       | ìë™         | **ì¦‰ì‹œ**  |
| **ì§€ì› ëª¨ë¸ ìˆ˜**      | 5ê°œ        | 50+ê°œ        | **+900%** |
| **ë¹„ìš© ìµœì í™”**       | ì—†ìŒ       | ìë™         | **-30%**  |

---

## ğŸ”„ **í†µí•© ëŒ€ì²´ ì „ëµ: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•** â­â­â­â­â­

### **ğŸ¯ ê¶Œì¥ ì•„í‚¤í…ì²˜: LlamaIndex + LiteLLM**

#### **ìµœì  ì¡°í•© ì´ìœ **

1. **ğŸ¦™ LlamaIndex**: RAG ì „ë¬¸ í”„ë ˆì„ì›Œí¬ë¡œ ê²€ìƒ‰ í’ˆì§ˆ ìµœì í™”
2. **âš¡ LiteLLM**: LLM ë ˆì´ì–´ ë‹¨ìˆœí™” ë° ì•ˆì •ì„± í™•ë³´  
3. **ğŸ¨ Gradio**: ê¸°ì¡´ UI ìœ ì§€ë¡œ ì‚¬ìš©ì ê²½í—˜ ì—°ì†ì„±

#### **í†µí•© êµ¬í˜„ ì˜ˆì‹œ**

```python
# === 1. ì˜ì¡´ì„± ë° ì„¤ì • ===
import litellm
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.faiss import FaissVectorStore

# LlamaIndex ê¸€ë¡œë²Œ ì„¤ì •
Settings.embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# LiteLLM ì„¤ì •
MODELS = {
    "GPT-4": "gpt-4",
    "Claude-3": "claude-3-opus",
    "Gemini Pro": "gemini-pro",
    "Llama-3.3-70B": "meta-llama/Llama-3.3-70B-Instruct",
}

# === 2. í•µì‹¬ í´ë˜ìŠ¤ ===
class ModernAcademicRAG:
    def __init__(self):
        self.sessions = OrderedDict()
        self.session_lock = threading.Lock()
        
    def process_pdfs(self, pdf_files: list) -> VectorStoreIndex:
        """PDF ì²˜ë¦¬ - LlamaIndex í™œìš©"""
        documents = []
        for pdf_file in pdf_files:
            text = self._extract_pdf_text(pdf_file)  # ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš©
            doc = Document(text=text, metadata={"source": pdf_file.name})
            documents.append(doc)
        
        # LlamaIndex ìë™ ì²­í‚¹ ë° ë²¡í„°í™”
        return VectorStoreIndex.from_documents(documents)
    
    def query(self, question: str, index: VectorStoreIndex, model: str) -> str:
        """ì§ˆì˜ì‘ë‹µ - LlamaIndex + LiteLLM ì¡°í•©"""
        
        # LlamaIndexë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retriever = index.as_retriever(similarity_top_k=3)
        relevant_docs = retriever.retrieve(question)
        context = "\n\n".join([doc.text for doc in relevant_docs])
        
        # LiteLLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        messages = [{
            "role": "system",
            "content": "í•™ì¹™ ë° ëŒ€í•™ ê·œì • ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        }, {
            "role": "user",
            "content": f"""
            ë‹¤ìŒ ê·œì • ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
            
            ê´€ë ¨ ê·œì •:
            {context}
            
            ì§ˆë¬¸: {question}
            """
        }]
        
        response = litellm.completion(
            model=MODELS[model],
            messages=messages,
            temperature=0.1,
            max_tokens=1000
        )
        
        return response.choices[0].message.content

# === 3. Gradio ì¸í„°í˜ì´ìŠ¤ ì—…ë°ì´íŠ¸ ===
rag_system = ModernAcademicRAG()

def handle_pdf_upload(pdfs, request: gr.Request):
    """PDF ì—…ë¡œë“œ ì²˜ë¦¬ - í˜„ëŒ€í™”ëœ ë²„ì „"""
    if not pdfs:
        return "âš ï¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    session_id = get_session_id(request)
    
    try:
        # LlamaIndexë¡œ ì²˜ë¦¬
        index = rag_system.process_pdfs(pdfs)
        
        # ì„¸ì…˜ì— ì €ì¥
        with rag_system.session_lock:
            rag_system.sessions[session_id] = {
                "index": index,
                "history": [],
                "current_model": "GPT-4"
            }
        
        return f"âœ… {len(pdfs)}ê°œ PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
        
    except Exception as e:
        return f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def handle_query(user_query, model_name, request: gr.Request):
    """ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ - í˜„ëŒ€í™”ëœ ë²„ì „"""
    session_id = get_session_id(request)
    
    with rag_system.session_lock:
        if session_id not in rag_system.sessions:
            return "âš ï¸ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        session = rag_system.sessions[session_id]
        session["current_model"] = model_name
    
    try:
        # í†µí•© RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬
        response = rag_system.query(
            question=user_query,
            index=session["index"], 
            model=model_name
        )
        
        # ê¸°ë¡ ì—…ë°ì´íŠ¸
        session["history"].append({
            "role": "user", 
            "content": user_query
        })
        session["history"].append({
            "role": "assistant", 
            "content": response
        })
        
        return session["history"]
        
    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        return session["history"] + [{"role": "assistant", "content": error_msg}]
```

#### **ğŸ“Š í†µí•© ì „í™˜ íš¨ê³¼ ì˜ˆì¸¡**

| ë©”íŠ¸ë¦­                | í˜„ì¬ êµ¬í˜„  | í•˜ì´ë¸Œë¦¬ë“œ ì „í™˜ | ê°œì„  íš¨ê³¼ |
| --------------------- | ---------- | --------------- | --------- |
| **ì „ì²´ ì½”ë“œ ë¼ì¸ìˆ˜**  | 265ì¤„      | ~180ì¤„          | **-32%**  |
| **LangChain ì˜ì¡´ì„±** | 4ê°œ íŒ¨í‚¤ì§€ | 0ê°œ             | **-100%** |
| **ìƒˆ ì˜ì¡´ì„±**         | -          | 2ê°œ (ìƒë‹¹íˆ ì ìŒ) | **-50%**  |
| **ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„**    | ~1.0ì´ˆ     | ~0.7ì´ˆ          | **-30%**  |
| **LLM ì‘ë‹µ ì‹œê°„**     | ëª¨ë¸ë³„ ìƒì´ | ìµœì  ëª¨ë¸ ìë™ ì„ íƒ | **-20%**  |
| **ê°œë°œ ìƒì‚°ì„±**       | ê¸°ì¤€       | +60%            | **+60%**  |
| **ì˜¤ë¥˜ ë³µêµ¬ ëŠ¥ë ¥**    | ìˆ˜ë™       | ìë™            | **ë¬´í•œ**  |
| **ì§€ì› ëª¨ë¸ ìˆ˜**      | 5ê°œ        | 50+ê°œ           | **+900%** |

---

## ğŸ› ï¸ **ë‹¨ê³„ë³„ ì „í™˜ ì‹¤í–‰ ê³„íš**

### **Phase 1: í™˜ê²½ ì¤€ë¹„ (1ì¼)**
```bash
# ê¸°ì¡´ ì‹œìŠ¤í…œ ë°±ì—…
git branch backup-original-system
git checkout -b modernize-rag-system

# ìƒˆë¡œìš´ ì˜ì¡´ì„± ì„¤ì¹˜
uv add llama-index-core
uv add llama-index-embeddings-huggingface
uv add llama-index-vector-stores-faiss  
uv add litellm

# LangChain ì˜ì¡´ì„± í™•ì¸ (ì œê±° ì˜ˆì •)
uv remove langchain-community langchain-huggingface langchain-text-splitters langchain-core
```

### **Phase 2: LiteLLM í†µí•© (1ì¼)**
```python
# 1. LLM ë ˆì´ì–´ êµì²´
# ê¸°ì¡´: ë³µì¡í•œ ë‹¤ì¤‘ í´ë¼ì´ì–¸íŠ¸ â†’ LiteLLM ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤
# ëŒ€ìƒ: ë¼ì¸ 47-53, 158-182

# 2. í…ŒìŠ¤íŠ¸
# ëª¨ë“  ê¸°ì¡´ ëª¨ë¸ì´ LiteLLMì„ í†µí•´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
```

### **Phase 3: LlamaIndex RAG ì „í™˜ (2ì¼)**
```python
# 1. ë²¡í„°ìŠ¤í† ì–´ êµì²´
# ê¸°ì¡´: LangChain FAISS â†’ LlamaIndex VectorStoreIndex
# ëŒ€ìƒ: ë¼ì¸ 86-93

# 2. ê²€ìƒ‰ ë¡œì§ êµì²´  
# ê¸°ì¡´: retriever.invoke() â†’ query_engine.query()
# ëŒ€ìƒ: ë¼ì¸ 130-143

# 3. ë¬¸ì„œ ì²˜ë¦¬ êµì²´
# ê¸°ì¡´: LangChain Document â†’ LlamaIndex Document  
# ëŒ€ìƒ: ë¼ì¸ 90
```

### **Phase 4: í†µí•© í…ŒìŠ¤íŠ¸ ë° ìµœì í™” (1ì¼)**
```python
# 1. ê¸°ëŠ¥ ë™ë“±ì„± í…ŒìŠ¤íŠ¸
# - PDF ì—…ë¡œë“œ ê¸°ëŠ¥ í™•ì¸
# - ê²€ìƒ‰ í’ˆì§ˆ ë¹„êµ 
# - ì‘ë‹µ ì‹œê°„ ë²¤ì¹˜ë§ˆí¬

# 2. ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ 
# - LiteLLM í´ë°± ì„¤ì •
# - ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬

# 3. ì„±ëŠ¥ ìµœì í™”
# - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
# - ì‘ë‹µ ì‹œê°„ íŠœë‹
```

### **Phase 5: ì •ë¦¬ ë° ë¬¸ì„œí™” (ë°˜ì¼)**
```python
# 1. ì½”ë“œ ì •ë¦¬
# - ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì„í¬íŠ¸ ì œê±°
# - ì£¼ì„ ì—…ë°ì´íŠ¸

# 2. ë¬¸ì„œ ì—…ë°ì´íŠ¸
# - README.md ìˆ˜ì •
# - ì˜ì¡´ì„± ëª©ë¡ ì—…ë°ì´íŠ¸

# 3. ë°°í¬ ì¤€ë¹„
# - pyproject.toml ì •ë¦¬
# - í™˜ê²½ë³€ìˆ˜ ê°€ì´ë“œ ì—…ë°ì´íŠ¸
```

---

## ğŸ“ˆ **ì˜ˆìƒ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**

### **ê¸°ìˆ ì  ê°œì„ ì‚¬í•­**
- **ğŸ”¥ 32% ì½”ë“œ ë‹¨ìˆœí™”**: 265ì¤„ â†’ ~180ì¤„
- **âš¡ 30% ì„±ëŠ¥ í–¥ìƒ**: ìµœì í™”ëœ ê²€ìƒ‰ + LLM ë¼ìš°íŒ…
- **ğŸ§© 100% LangChain ì œê±°**: ì™„ì „í•œ ì˜ì¡´ì„± ë…ë¦½
- **ğŸ›¡ï¸ ë¬´í•œ ì˜¤ë¥˜ ë³µêµ¬**: ìë™ í´ë°± ë° ì¬ì‹œë„

### **ìš´ì˜ ê°œì„ ì‚¬í•­**  
- **ğŸ’° 30% ë¹„ìš© ì ˆê°**: LiteLLM ìµœì  ì œê³µì ì„ íƒ
- **ğŸ”§ 60% ìœ ì§€ë³´ìˆ˜ íš¨ìœ¨**: ë‹¨ìˆœí•œ ì½”ë“œ êµ¬ì¡°
- **ğŸ“ˆ 900% ëª¨ë¸ ì„ íƒ**: 5ê°œ â†’ 50+ê°œ ì§€ì› ëª¨ë¸
- **ğŸš€ ì¦‰ì‹œ í™•ì¥**: ìƒˆ ê¸°ëŠ¥ ì¶”ê°€ ì‹œê°„ ë‹¨ì¶•

### **ì‚¬ìš©ì ê²½í—˜ ê°œì„ **
- **âš¡ ë¹ ë¥¸ ì‘ë‹µ**: ê²€ìƒ‰ ì‹œê°„ 30% ë‹¨ì¶•
- **ğŸ›¡ï¸ ì•ˆì •ì  ì„œë¹„ìŠ¤**: ì œê³µì ì¥ì•  ì‹œì—ë„ ì§€ì†
- **ğŸ¯ ë” ë‚˜ì€ ë‹µë³€**: ê³ ê¸‰ RAG ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í’ˆì§ˆ í–¥ìƒ
- **ğŸŒŸ í’ë¶€í•œ ì„ íƒ**: ë‹¤ì–‘í•œ LLM ëª¨ë¸ ì˜µì…˜

---

## ğŸ¯ **ìµœì¢… ê¶Œì¥ì‚¬í•­**

### **âœ… ê¶Œì¥: LlamaIndex + LiteLLM í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ**

**ì„ íƒ ì´ìœ **:
1. **âš–ï¸ ìµœì  ê· í˜•**: RAG ì „ë¬¸ì„± + LLM ìœ ì—°ì„±ì˜ ì™„ë²½í•œ ì¡°í•©
2. **ğŸš€ ë¹ ë¥¸ ì „í™˜**: 5ì¼ ë‚´ ì™„ì „ ì „í™˜ ê°€ëŠ¥
3. **ğŸ“Š ê²€ì¦ëœ íš¨ê³¼**: 32% ì½”ë“œ ê°ì†Œ, 30% ì„±ëŠ¥ í–¥ìƒ
4. **ğŸ”§ ë‚®ì€ ë¦¬ìŠ¤í¬**: ì ì§„ì  ì „í™˜ìœ¼ë¡œ ì•ˆì „ì„± í™•ë³´

### **êµ¬í˜„ ìš°ì„ ìˆœìœ„**
```
Day 1: í™˜ê²½ ì¤€ë¹„ + LiteLLM í†µí•© (ì¦‰ì‹œ íš¨ê³¼)
Day 2-3: LlamaIndex RAG ì „í™˜ (í•µì‹¬ ê¸°ëŠ¥ ê°œì„ )  
Day 4: í†µí•© í…ŒìŠ¤íŠ¸ (í’ˆì§ˆ ë³´ì¦)
Day 5: ì •ë¦¬ ë° ë°°í¬ (ì™„ë£Œ)
```

### **ê¸°ëŒ€ ê²°ê³¼**
- **í•™ì¹™ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**ì´ **í˜„ëŒ€ì ì´ê³  íš¨ìœ¨ì ì¸ RAG í”Œë«í¼**ìœ¼ë¡œ ì™„ì „ ì „í™˜
- **ê°œë°œ ìƒì‚°ì„± 60% í–¥ìƒ**ìœ¼ë¡œ ìƒˆë¡œìš´ ê¸°ëŠ¥ ë¹ ë¥¸ ê°œë°œ ê°€ëŠ¥  
- **ìš´ì˜ ì•ˆì •ì„± ê·¹ëŒ€í™”**ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í•™ì‚¬ ì§€ì› ì„œë¹„ìŠ¤ êµ¬ì¶•

ì´ ì „ëµì„ í†µí•´ í•™ì¹™ ì—ì´ì „íŠ¸ëŠ” **ë‹¨ìˆœí•˜ë©´ì„œë„ ê°•ë ¥í•œ ì°¨ì„¸ëŒ€ AI ì‹œìŠ¤í…œ**ìœ¼ë¡œ ì§„í™”í•  ê²ƒì…ë‹ˆë‹¤! ğŸš€âœ¨

---

## ğŸ“š **ì¶”ê°€ ì°¸ê³  ìë£Œ**

### **ê³µì‹ ë¬¸ì„œ**
- [LlamaIndex ê³µì‹ ë¬¸ì„œ](https://docs.llamaindex.ai/)
- [LiteLLM ì‚¬ìš© ê°€ì´ë“œ](https://docs.litellm.ai/)
- [SmolAgents GitHub](https://github.com/huggingfaceh4/smolagents)

### **ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ**
- [LangChain â†’ LlamaIndex ì „í™˜ ê°€ì´ë“œ](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)
- [ë©€í‹° LLM í†µí•© íŒ¨í„´](https://docs.litellm.ai/docs/tutorials/first_playground)
#!/usr/bin/env python3
"""
Multi-Embedding Database Builder
===============================

experiment_configurations.pyì˜ ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬
ëª¨ë“  í† í¬ë‚˜ì´ì €, ì²­ì»¤, ì„ë² ë”© ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì‹¤í–‰ ìˆœì„œ:
1. ë¬¸ì„œ ë¡œë”©
2. í† í¬ë‚˜ì´ì €ë³„ í† í°í™” (ì„ íƒì )
3. ì²­ì»¤ë³„ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ì²­í‚¹
4. ì„ë² ë”© ëª¨ë¸ë³„ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥

ì‚¬ìš©ë²•:
    python multi_embedding_database_builder.py
    python multi_embedding_database_builder.py --dry-run  # ì„¤ì • í™•ì¸ë§Œ
"""

import os
import sys
import json
import time
import pickle
import hashlib
import itertools
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Iterator
from dataclasses import dataclass

import torch
import gc
import numpy as np
import pandas as pd
from loguru import logger

# ë¡œì»¬ ëª¨ë“ˆ
from experiment_configurations import (
    TOKENIZERS,
    CHUNKERS,
    EMBEDDING_MODELS,
    EXPERIMENT_CONFIG,
    validate_configurations,
    print_configuration_summary,
)
from loaders import load_document, collect_document_paths

# Vector store and embeddings
import faiss
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings

# ===================================================================
# ë°ì´í„°í´ë˜ìŠ¤
# ===================================================================


@dataclass
class ExperimentState:
    """ì‹¤í—˜ ì§„í–‰ ìƒíƒœ"""

    total_documents: int = 0
    tokenization_results: int = 0
    chunking_results: int = 0
    embedding_results: int = 0
    successful_experiments: int = 0
    failed_experiments: int = 0
    start_time: float = 0
    current_phase: str = "ì´ˆê¸°í™”"


@dataclass
class ExperimentResult:
    """ê°œë³„ ì‹¤í—˜ ê²°ê³¼"""

    experiment_id: str
    tokenizer_name: Optional[str]
    chunker_name: str
    chunker_params: Dict[str, Any]
    embedding_name: str
    embedding_model: str
    total_chunks: int
    embedding_time: float
    database_size_mb: float
    database_path: str
    success: bool
    error_message: Optional[str] = None
    timestamp: str = ""


# ===================================================================
# ë„¤ì´ë° ì»¨ë²¤ì…˜ (ê°„ì†Œí™”)
# ===================================================================


class ExperimentNaming:
    """ì‹¤í—˜ ê²°ê³¼ ë„¤ì´ë° ì»¨ë²¤ì…˜"""

    ABBREVIATIONS = {
        # í† í¬ë‚˜ì´ì €
        "character": "char",
        "gpt2": "gpt2",
        "cl100k_base": "tiktoken",
        # ì„ë² ë”© ëª¨ë¸ (í‚¤ ì‚¬ìš©)
        "qwen3_8b": "qwen8b",
        "qwen3_0_6b": "qwen06b",
        "jina_v3": "jina",
        "bge_m3": "bgem3",
        "all_minilm_l6": "minilm",
        "multilingual_e5": "e5",
        # Boolean
        True: "T",
        False: "F",
        None: "null",
    }

    @classmethod
    def abbreviate(cls, value: Any) -> str:
        """ê°’ì„ ì•½ì–´ë¡œ ë³€í™˜"""
        if value in cls.ABBREVIATIONS:
            return cls.ABBREVIATIONS[value]
        elif isinstance(value, str) and len(value) > 12:
            return value.replace("-", "").replace("_", "")[:8]
        return str(value).replace("-", "_")

    @classmethod
    def generate_chunk_id(cls, chunker_name: str, params: Dict[str, Any]) -> str:
        """ì²­í‚¹ ID ìƒì„±"""
        # íŒŒë¼ë¯¸í„° í•´ì‹œ
        param_hash = cls._hash_params(params)

        # ì²­ì»¤ë³„ íŠ¹í™” ID
        if chunker_name == "token":
            tokenizer = cls.abbreviate(params.get("tokenizer", "char"))
            size = params.get("chunk_size", 512)
            overlap = params.get("chunk_overlap", 0)
            return f"token_{tokenizer}_s{size}_o{overlap}_{param_hash}"

        elif chunker_name == "sentence":
            tokenizer = cls.abbreviate(params.get("tokenizer_or_token_counter", "char"))
            size = params.get("chunk_size", 1024)
            min_sent = params.get("min_sentences_per_chunk", 2)
            return f"sent_{tokenizer}_s{size}_m{min_sent}_{param_hash}"

        elif chunker_name in ["late", "semantic"]:
            model = cls.abbreviate(params.get("embedding_model", "unknown"))
            size = params.get("chunk_size", 1024)
            return f"{chunker_name}_{model}_s{size}_{param_hash}"

        elif chunker_name == "neural":
            model_name = params.get("model", "bert")
            model = cls.abbreviate(model_name.split("/")[-1])
            min_char = params.get("min_characters_per_chunk", 50)
            return f"neural_{model}_c{min_char}_{param_hash}"

        elif chunker_name == "recursive":
            tokenizer = cls.abbreviate(params.get("tokenizer_or_token_counter", "char"))
            size = params.get("chunk_size", 1024)
            return f"recur_{tokenizer}_s{size}_{param_hash}"

        else:
            return f"{chunker_name}_{param_hash}"

    @classmethod
    def generate_database_path(cls, embedding_name: str, chunk_id: str) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ìƒì„±"""
        embedding_abbrev = cls.abbreviate(embedding_name)
        return f"{embedding_abbrev}/{chunk_id}"

    @classmethod
    def _hash_params(cls, params: Dict[str, Any]) -> str:
        """íŒŒë¼ë¯¸í„° 8ì í•´ì‹œ ìƒì„±"""
        clean_params = {
            k: v for k, v in params.items() if k not in ["_target_", "class"]
        }
        param_str = json.dumps(clean_params, sort_keys=True, separators=(",", ":"))
        return hashlib.md5(param_str.encode()).hexdigest()[:8]


# ===================================================================
# ì‹¤í—˜ ê´€ë¦¬ìë“¤
# ===================================================================


class DocumentManager:
    """ë¬¸ì„œ ë¡œë”© ê´€ë¦¬"""

    def __init__(self):
        self.config = EXPERIMENT_CONFIG["data"]

    def load_all_documents(self) -> List[Document]:
        """ëª¨ë“  ì§€ì› ë¬¸ì„œ ë¡œë“œ"""
        logger.info("ğŸ“– ë¬¸ì„œ ë¡œë”© ì‹œì‘...")

        # ë¬¸ì„œ ê²½ë¡œ ìˆ˜ì§‘
        document_paths = list(
            collect_document_paths(self.config["rules_directory"], lazy=False)
        )

        # í¬ê¸° ë° íŒ¨í„´ í•„í„°ë§
        filtered_paths = []
        for path in document_paths:
            try:
                # í¬ê¸° ì²´í¬
                size_mb = os.path.getsize(path) / (1024 * 1024)
                if size_mb > self.config["max_file_size_mb"]:
                    logger.debug(f"âš ï¸ í¬ê¸° ì´ˆê³¼: {path} ({size_mb:.1f}MB)")
                    continue

                # íŒ¨í„´ ì²´í¬ (ê°„ë‹¨í•œ êµ¬í˜„)
                path_str = str(path)
                skip = False
                for pattern in self.config.get("exclude_patterns", []):
                    if pattern.replace("**/", "").replace("/**", "") in path_str:
                        skip = True
                        break

                if not skip:
                    filtered_paths.append(path)

            except OSError as e:
                logger.debug(f"âš ï¸ ì ‘ê·¼ ì‹¤íŒ¨: {path} - {e}")
                continue

        logger.info(f"ğŸ“„ í•„í„°ë§ëœ ê²½ë¡œ: {len(filtered_paths)}ê°œ")

        # ë¬¸ì„œ ë¡œë“œ
        all_documents = []
        for i, path in enumerate(filtered_paths):
            logger.info(f"ğŸ“„ ë¡œë”© [{i + 1}/{len(filtered_paths)}]: {path.name}")

            try:
                docs = load_document(path)

                # ë©”íƒ€ë°ì´í„° ë³´ê°•
                for doc in docs:
                    doc.metadata.update(
                        {
                            "file_path": str(path),
                            "file_name": path.name,
                            "file_extension": path.suffix.lower(),
                            "load_timestamp": datetime.now().isoformat(),
                            "doc_id": f"{path.stem}_{hashlib.md5(str(path).encode()).hexdigest()[:8]}",
                        }
                    )

                all_documents.extend(docs)

            except Exception as e:
                logger.error(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {path} - {e}")
                continue

        logger.info(f"âœ… ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(all_documents)}ê°œ")
        return all_documents


class TokenizationManager:
    """í† í°í™” ê´€ë¦¬ (ì„ íƒì )"""

    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.token_dir = (
            self.output_dir / EXPERIMENT_CONFIG["output"]["tokenization_subdir"]
        )
        self.token_dir.mkdir(parents=True, exist_ok=True)

    def tokenize_documents(self, documents: List[Document]) -> Dict[str, str]:
        """ë¬¸ì„œë“¤ì„ ëª¨ë“  í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”"""
        if not EXPERIMENT_CONFIG["execution"]["enable_tokenization_phase"]:
            logger.info("ğŸ”¤ í† í°í™” ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”ë¨)")
            return {}

        logger.info("ğŸ”¤ í† í°í™” ë‹¨ê³„ ì‹œì‘...")

        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ê²°í•©
        combined_text = "\n\n".join([doc.page_content for doc in documents])
        doc_hash = hashlib.md5(combined_text.encode()).hexdigest()[:8]

        tokenization_results = {}

        # í•„í„° ì ìš©
        tokenizers = TOKENIZERS
        if EXPERIMENT_CONFIG["filters"]["tokenizers"]:
            tokenizers = {
                k: v
                for k, v in tokenizers.items()
                if k in EXPERIMENT_CONFIG["filters"]["tokenizers"]
            }

        for tokenizer_name, config in tokenizers.items():
            logger.info(f"ğŸ”¤ í† í°í™”: {tokenizer_name}")

            try:
                start_time = time.time()

                # í† í¬ë‚˜ì´ì €ë³„ êµ¬í˜„
                impl = config["implementation"]
                if impl == "character":
                    tokens = list(combined_text)
                elif impl in ["gpt2", "cl100k_base"]:
                    import tiktoken

                    tokenizer = tiktoken.get_encoding(impl)
                    tokens = tokenizer.encode(combined_text)
                    tokens = [str(t) for t in tokens]
                else:
                    logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í† í¬ë‚˜ì´ì €: {impl}")
                    continue

                processing_time = time.time() - start_time

                # ê²°ê³¼ ì €ì¥
                result_id = self._save_tokenization(
                    tokenizer_name, doc_hash, tokens, processing_time
                )
                tokenization_results[tokenizer_name] = result_id

                logger.info(
                    f"âœ… í† í°í™” ì™„ë£Œ: {tokenizer_name} ({len(tokens):,}ê°œ, {processing_time:.1f}ì´ˆ)"
                )

            except Exception as e:
                logger.error(f"âŒ í† í°í™” ì‹¤íŒ¨: {tokenizer_name} - {e}")
                continue

        logger.info(f"ğŸ‰ í† í°í™” ì™„ë£Œ: {len(tokenization_results)}ê°œ ê²°ê³¼")
        return tokenization_results

    def _save_tokenization(
        self,
        tokenizer_name: str,
        doc_hash: str,
        tokens: List[str],
        processing_time: float,
    ) -> str:
        """í† í°í™” ê²°ê³¼ ì €ì¥"""
        result_id = f"{ExperimentNaming.abbreviate(tokenizer_name)}_{doc_hash}"

        # í† í° ì €ì¥
        token_file = self.token_dir / f"{result_id}_tokens.pkl"
        with open(token_file, "wb") as f:
            pickle.dump(tokens, f)

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "result_id": result_id,
            "tokenizer_name": tokenizer_name,
            "doc_hash": doc_hash,
            "token_count": len(tokens),
            "unique_tokens": len(set(tokens)),
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "file_size_mb": token_file.stat().st_size / (1024 * 1024),
        }

        metadata_file = self.token_dir / f"{result_id}_metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        return result_id


class ChunkingManager:
    """ì²­í‚¹ ê´€ë¦¬"""

    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.chunk_dir = (
            self.output_dir / EXPERIMENT_CONFIG["output"]["chunking_subdir"]
        )
        self.chunk_dir.mkdir(parents=True, exist_ok=True)

    def generate_chunking_combinations(self) -> Iterator[Tuple[str, Dict[str, Any]]]:
        """ì²­í‚¹ ì¡°í•© ìƒì„±ê¸°"""
        logger.info("ğŸ§© ì²­í‚¹ ì¡°í•© ìƒì„±...")

        # í•„í„° ì ìš©
        chunkers = CHUNKERS
        if EXPERIMENT_CONFIG["filters"]["chunkers"]:
            chunkers = {
                k: v
                for k, v in chunkers.items()
                if k in EXPERIMENT_CONFIG["filters"]["chunkers"]
            }

        total_combinations = 0

        for chunker_name, config in chunkers.items():
            logger.info(f"ğŸ§© ì²­ì»¤: {chunker_name}")

            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            base_params = config["parameters"]["base"].copy()
            variations = config["parameters"]["variations"]

            # íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
            param_lists = {}
            static_params = base_params.copy()

            for param_name, param_values in variations.items():
                if param_name == "embedding_model" and param_values == "ALL_EMBEDDINGS":
                    # ì„ë² ë”© ëª¨ë¸ í•„í„° ì ìš©
                    embedding_models = EMBEDDING_MODELS
                    if EXPERIMENT_CONFIG["filters"]["embedding_models"]:
                        embedding_models = {
                            k: v
                            for k, v in embedding_models.items()
                            if k in EXPERIMENT_CONFIG["filters"]["embedding_models"]
                        }
                    param_lists[param_name] = [
                        v["model_name"] for v in embedding_models.values()
                    ]
                elif isinstance(param_values, list):
                    param_lists[param_name] = param_values
                else:
                    static_params[param_name] = param_values

            # ì¡°í•© ìƒì„±
            if param_lists:
                keys, values = zip(*param_lists.items())
                for combination in itertools.product(*values):
                    params = static_params.copy()
                    params.update(dict(zip(keys, combination)))
                    total_combinations += 1
                    yield chunker_name, params
            else:
                total_combinations += 1
                yield chunker_name, static_params

        logger.info(f"ğŸ“Š ì´ ì²­í‚¹ ì¡°í•©: {total_combinations}ê°œ")

    def chunk_documents(
        self, chunker_name: str, params: Dict[str, Any], documents: List[Document]
    ) -> Optional[str]:
        """ë¬¸ì„œ ì²­í‚¹ ìˆ˜í–‰"""
        try:
            # ì²­ì»¤ ìƒì„±
            chunker_class = CHUNKERS[chunker_name]["class"]
            clean_params = {k: v for k, v in params.items() if v is not None}

            logger.debug(f"  ğŸ“„ ì²­í‚¹: {chunker_name} with {clean_params}")

            start_time = time.time()
            chunker = chunker_class(**clean_params)

            # ë¬¸ì„œë“¤ ì²­í‚¹
            all_chunks = []
            for doc in documents:
                try:
                    chunks = chunker.chunk(doc.page_content)

                    for i, chunk in enumerate(chunks):
                        # ì²­í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        chunk_text = (
                            chunk.text if hasattr(chunk, "text") else str(chunk)
                        )

                        # Document ê°ì²´ ìƒì„±
                        chunk_doc = Document(
                            page_content=chunk_text,
                            metadata={
                                **doc.metadata,
                                "chunk_index": i,
                                "chunk_size": len(chunk_text),
                                "parent_doc_id": doc.metadata.get("doc_id", "unknown"),
                                "chunker_name": chunker_name,
                                "chunker_params": clean_params,
                            },
                        )
                        all_chunks.append(chunk_doc)

                except Exception as e:
                    logger.debug(f"    âš ï¸ ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨: {e}")
                    continue

            if not all_chunks:
                logger.warning(f"  âŒ ì²­í‚¹ ê²°ê³¼ ì—†ìŒ: {chunker_name}")
                return None

            processing_time = time.time() - start_time

            # ê²°ê³¼ ì €ì¥
            chunk_id = self._save_chunking_result(
                chunker_name, params, all_chunks, processing_time
            )

            logger.debug(
                f"  âœ… ì²­í‚¹ ì™„ë£Œ: {chunk_id} ({len(all_chunks)}ê°œ, {processing_time:.1f}ì´ˆ)"
            )
            return chunk_id

        except Exception as e:
            logger.error(f"  âŒ ì²­í‚¹ ì‹¤íŒ¨: {chunker_name} - {e}")
            return None

    def _save_chunking_result(
        self,
        chunker_name: str,
        params: Dict[str, Any],
        chunks: List[Document],
        processing_time: float,
    ) -> str:
        """ì²­í‚¹ ê²°ê³¼ ì €ì¥"""
        chunk_id = ExperimentNaming.generate_chunk_id(chunker_name, params)

        # ì²­í¬ ë°ì´í„° ì €ì¥
        chunks_file = self.chunk_dir / f"{chunk_id}_chunks.pkl"
        with open(chunks_file, "wb") as f:
            pickle.dump(chunks, f)

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "chunk_id": chunk_id,
            "chunker_name": chunker_name,
            "chunker_params": params,
            "total_chunks": len(chunks),
            "avg_chunk_size": float(np.mean([len(doc.page_content) for doc in chunks])),
            "chunk_size_variance": float(
                np.var([len(doc.page_content) for doc in chunks])
            ),
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "file_size_mb": chunks_file.stat().st_size / (1024 * 1024),
        }

        metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        return chunk_id

    def load_chunking_result(
        self, chunk_id: str
    ) -> Tuple[List[Document], Dict[str, Any]]:
        """ì²­í‚¹ ê²°ê³¼ ë¡œë“œ"""
        chunks_file = self.chunk_dir / f"{chunk_id}_chunks.pkl"
        metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"

        if not chunks_file.exists():
            raise FileNotFoundError(f"ì²­í‚¹ ê²°ê³¼ ì—†ìŒ: {chunk_id}")

        with open(chunks_file, "rb") as f:
            chunks = pickle.load(f)

        metadata = {}
        if metadata_file.exists():
            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)

        return chunks, metadata


class EmbeddingManager:
    """ì„ë² ë”© ë° FAISS ì €ì¥ ê´€ë¦¬"""

    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)

    def generate_embedding_combinations(self) -> Iterator[Tuple[str, Dict[str, Any]]]:
        """ì„ë² ë”© ëª¨ë¸ ì¡°í•© ìƒì„±ê¸°"""
        # í•„í„° ì ìš©
        embedding_models = EMBEDDING_MODELS
        if EXPERIMENT_CONFIG["filters"]["embedding_models"]:
            embedding_models = {
                k: v
                for k, v in embedding_models.items()
                if k in EXPERIMENT_CONFIG["filters"]["embedding_models"]
            }

        for name, config in embedding_models.items():
            yield name, config

    def create_vectorstore_and_save(
        self,
        embedding_name: str,
        embedding_config: Dict[str, Any],
        chunk_id: str,
        chunks: List[Document],
        chunk_metadata: Dict[str, Any],
    ) -> ExperimentResult:
        """ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥"""
        start_time = time.time()

        try:
            # ì„ë² ë”© ëª¨ë¸ ìƒì„±
            embedding_model = self._create_embedding_model(embedding_config)

            # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            logger.debug(f"    ğŸ”® ë²¡í„°ìŠ¤í† ì–´ ìƒì„±: {embedding_name}")
            vectorstore = FAISS.from_documents(chunks, embedding_model)
            embedding_time = time.time() - start_time

            # ì €ì¥ ê²½ë¡œ ìƒì„±
            db_relative_path = ExperimentNaming.generate_database_path(
                embedding_name, chunk_id
            )
            db_path = self.output_dir / db_relative_path
            db_path.mkdir(parents=True, exist_ok=True)

            # FAISS ì €ì¥
            vectorstore.save_local(str(db_path))

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            full_metadata = {
                "experiment_id": f"{embedding_name}_{chunk_id}",
                "embedding_name": embedding_name,
                "embedding_model": embedding_config["model_name"],
                "embedding_dimension": embedding_config["dimension"],
                "chunk_id": chunk_id,
                "database_path": str(db_path),
                "timestamp": datetime.now().isoformat(),
                **chunk_metadata,
            }

            with open(db_path / "metadata.json", "w", encoding="utf-8") as f:
                json.dump(full_metadata, f, ensure_ascii=False, indent=2)

            # DB í¬ê¸° ê³„ì‚°
            db_size_mb = sum(
                f.stat().st_size for f in db_path.rglob("*") if f.is_file()
            ) / (1024 * 1024)

            # Multi-GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if EXPERIMENT_CONFIG["execution"]["cleanup_memory_after_each"]:
                if torch.cuda.is_available():
                    # ëª¨ë“  GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    for i in range(torch.cuda.device_count()):
                        with torch.cuda.device(i):
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                gc.collect()

            return ExperimentResult(
                experiment_id=full_metadata["experiment_id"],
                tokenizer_name=None,  # í˜„ì¬ëŠ” ë¯¸ì‚¬ìš©
                chunker_name=chunk_metadata.get("chunker_name", "unknown"),
                chunker_params=chunk_metadata.get("chunker_params", {}),
                embedding_name=embedding_name,
                embedding_model=embedding_config["model_name"],
                total_chunks=len(chunks),
                embedding_time=embedding_time,
                database_size_mb=db_size_mb,
                database_path=str(db_path),
                success=True,
                timestamp=full_metadata["timestamp"],
            )

        except Exception as e:
            embedding_time = time.time() - start_time
            error_msg = f"ì„ë² ë”© ì‹¤íŒ¨: {embedding_name} Ã— {chunk_id} - {str(e)}"

            return ExperimentResult(
                experiment_id=f"{embedding_name}_{chunk_id}",
                tokenizer_name=None,
                chunker_name=chunk_metadata.get("chunker_name", "unknown"),
                chunker_params=chunk_metadata.get("chunker_params", {}),
                embedding_name=embedding_name,
                embedding_model=embedding_config["model_name"],
                total_chunks=0,
                embedding_time=embedding_time,
                database_size_mb=0,
                database_path="",
                success=False,
                error_message=error_msg,
                timestamp=datetime.now().isoformat(),
            )

    def _create_embedding_model(self, config: Dict[str, Any]) -> HuggingFaceEmbeddings:
        """ì„ë² ë”© ëª¨ë¸ ìƒì„± (Multi-GPU ì§€ì›)"""
        model_kwargs = config["model_kwargs"].copy()
        encode_kwargs = config["encode_kwargs"].copy()

        # Multi-GPU ìë™ ì„¤ì •
        if model_kwargs.get("device") == "auto":
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                if gpu_count > 1:
                    # Multi-GPU í™˜ê²½
                    logger.info(f"ğŸ® Multi-GPU ê°ì§€: {gpu_count}ê°œ GPU í™œìš©")
                    model_kwargs["device_map"] = "auto"
                    model_kwargs["torch_dtype"] = model_kwargs.get(
                        "torch_dtype", "float16"
                    )

                    # Multi-GPUì—ì„œ ë°°ì¹˜ í¬ê¸° ìµœì í™”
                    if "batch_size" in encode_kwargs:
                        original_batch_size = encode_kwargs["batch_size"]
                        # GPU ìˆ˜ë§Œí¼ ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ìµœëŒ€ 128ê¹Œì§€)
                        optimized_batch_size = min(original_batch_size * gpu_count, 128)
                        encode_kwargs["batch_size"] = optimized_batch_size
                        logger.info(
                            f"  ğŸ“ˆ ë°°ì¹˜ í¬ê¸° ìµœì í™”: {original_batch_size} â†’ {optimized_batch_size}"
                        )
                else:
                    # Single GPU
                    model_kwargs["device"] = "cuda"
                    logger.info("ğŸ® Single GPU ì‚¬ìš©")
            else:
                # CPU ëª¨ë“œ
                model_kwargs["device"] = "cpu"
                logger.info("ğŸ’» CPU ëª¨ë“œ ì‚¬ìš©")

        return HuggingFaceEmbeddings(
            model_name=config["model_name"],
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
        )


# ===================================================================
# ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ê¸°
# ===================================================================


class MultiEmbeddingExperimentRunner:
    """ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ê¸° - ì„¤ì • ê¸°ë°˜ ìˆœíšŒ ì‹¤í—˜"""

    def __init__(self, output_dir: Optional[str] = None):
        self.output_dir = Path(output_dir or EXPERIMENT_CONFIG["output"]["base_dir"])
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.doc_manager = DocumentManager()
        self.tokenization_manager = TokenizationManager(self.output_dir)
        self.chunking_manager = ChunkingManager(self.output_dir)
        self.embedding_manager = EmbeddingManager(self.output_dir)

        # ìƒíƒœ ë° ê²°ê³¼
        self.state = ExperimentState()
        self.results: List[ExperimentResult] = []

    def _log_gpu_info(self):
        """GPU í™˜ê²½ ì •ë³´ ë¡œê¹…"""
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸ® GPU í™˜ê²½: {gpu_count}ê°œ GPU ê°ì§€ë¨")

            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory // (
                    1024**3
                )
                logger.info(f"  â€¢ GPU {i}: {gpu_name} ({gpu_memory}GB VRAM)")

            # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            logger.info("ğŸ“Š GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
            for i in range(gpu_count):
                allocated = torch.cuda.memory_allocated(i) // (1024**2)
                reserved = torch.cuda.memory_reserved(i) // (1024**2)
                total = torch.cuda.get_device_properties(i).total_memory // (1024**2)
                logger.info(
                    f"  â€¢ GPU {i}: {allocated}MB ì‚¬ìš© ì¤‘, {reserved}MB ì˜ˆì•½ë¨ / {total}MB"
                )

            if gpu_count > 1:
                logger.info("ğŸš€ Multi-GPU ìµœì í™” í™œì„±í™” - ìì› ë‚­ë¹„ ìµœì†Œí™”")
        else:
            logger.info("ğŸ’» CPU ëª¨ë“œ - GPU ì—†ìŒ")

    def run_all_experiments(self) -> Dict[str, Any]:
        """ëª¨ë“  ì‹¤í—˜ ì‹¤í–‰ - ì„¤ì • ê¸°ë°˜ ìˆœíšŒ"""
        logger.info("ğŸš€ Multi-Embedding Database Builder ì‹œì‘!")
        logger.info("ğŸ“‹ ì„¤ì • ê¸°ë°˜ ìˆœíšŒ ì‹¤í—˜")

        # GPU í™˜ê²½ ì •ë³´
        self._log_gpu_info()
        logger.info("=" * 80)

        self.state.start_time = time.time()

        try:
            # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë”©
            self.state.current_phase = "ë¬¸ì„œ ë¡œë”©"
            logger.info(f"\nğŸ“– === {self.state.current_phase} ===")
            documents = self.doc_manager.load_all_documents()

            if not documents:
                return {"success": False, "error": "ë¬¸ì„œ ì—†ìŒ"}

            self.state.total_documents = len(documents)

            # ë‹¨ê³„ 2: í† í°í™” (ì„ íƒì )
            self.state.current_phase = "í† í°í™”"
            logger.info(f"\nğŸ”¤ === {self.state.current_phase} ===")
            tokenization_results = self.tokenization_manager.tokenize_documents(
                documents
            )
            self.state.tokenization_results = len(tokenization_results)

            # ë‹¨ê³„ 3: ì²­í‚¹ ë‹¨ê³„
            self.state.current_phase = "ì²­í‚¹"
            logger.info(f"\nğŸ§© === {self.state.current_phase} ===")
            chunk_results = self._run_chunking_experiments(documents)
            self.state.chunking_results = len(chunk_results)

            if not chunk_results:
                return {"success": False, "error": "ì²­í‚¹ ê²°ê³¼ ì—†ìŒ"}

            # ë‹¨ê³„ 4: ì„ë² ë”© ë‹¨ê³„
            self.state.current_phase = "ì„ë² ë”©"
            logger.info(f"\nğŸ”® === {self.state.current_phase} ===")

            # ëª¨ë“  ì‹¤í—˜ ìˆ˜í–‰ (ì œì•½ ì—†ìŒ)

            embedding_results = self._run_embedding_experiments(chunk_results)
            self.results = embedding_results
            self.state.embedding_results = len(embedding_results)

            # ê²°ê³¼ ì €ì¥ ë° ìš”ì•½
            summary = self._save_final_results()

            return {
                "success": True,
                "state": self.state,
                "summary": summary,
                "output_dir": str(self.output_dir),
            }

        except KeyboardInterrupt:
            logger.warning("\nâŒ ì‚¬ìš©ì ì¤‘ë‹¨")
            return {"success": False, "error": "ì¤‘ë‹¨ë¨"}

        except Exception as e:
            logger.error(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e)}

    def _run_chunking_experiments(self, documents: List[Document]) -> List[str]:
        """ì²­í‚¹ ì‹¤í—˜ ì‹¤í–‰"""
        chunk_results = []

        combination_count = 0
        for (
            chunker_name,
            params,
        ) in self.chunking_manager.generate_chunking_combinations():
            combination_count += 1
            logger.info(f"ğŸ§© ì²­í‚¹ [{combination_count}]: {chunker_name}")

            chunk_id = self.chunking_manager.chunk_documents(
                chunker_name, params, documents
            )
            if chunk_id:
                chunk_results.append(chunk_id)

        logger.info(f"ğŸ‰ ì²­í‚¹ ë‹¨ê³„ ì™„ë£Œ: {len(chunk_results)}ê°œ ê²°ê³¼")
        return chunk_results

    def _run_embedding_experiments(
        self, chunk_ids: List[str]
    ) -> List[ExperimentResult]:
        """ì„ë² ë”© ì‹¤í—˜ ì‹¤í–‰"""
        results = []

        # ì „ì²´ ì¡°í•© ê³„ì‚°
        embedding_combinations = list(
            self.embedding_manager.generate_embedding_combinations()
        )
        total_combinations = len(chunk_ids) * len(embedding_combinations)

        current_combination = 0

        for chunk_id in chunk_ids:
            try:
                # ì²­í‚¹ ê²°ê³¼ ë¡œë“œ
                chunks, chunk_metadata = self.chunking_manager.load_chunking_result(
                    chunk_id
                )

                # ê° ì„ë² ë”© ëª¨ë¸ë¡œ ì‹¤í—˜
                for embedding_name, embedding_config in embedding_combinations:
                    current_combination += 1
                    logger.info(
                        f"ğŸ”® ì„ë² ë”© [{current_combination}/{total_combinations}]: "
                        f"{embedding_name} Ã— {chunk_id}"
                    )

                    result = self.embedding_manager.create_vectorstore_and_save(
                        embedding_name,
                        embedding_config,
                        chunk_id,
                        chunks,
                        chunk_metadata,
                    )

                    results.append(result)

                    if result.success:
                        self.state.successful_experiments += 1
                        logger.info(
                            f"âœ… ì„±ê³µ: {embedding_name} ({result.embedding_time:.1f}ì´ˆ, {result.database_size_mb:.1f}MB)"
                        )
                    else:
                        self.state.failed_experiments += 1
                        logger.error(f"âŒ ì‹¤íŒ¨: {result.error_message}")

            except Exception as e:
                logger.error(f"âŒ ì²­í‚¹ ë¡œë“œ ì‹¤íŒ¨: {chunk_id} - {e}")
                continue

        logger.info(f"ğŸ‰ ì„ë² ë”© ë‹¨ê³„ ì™„ë£Œ: {self.state.successful_experiments}ê°œ ì„±ê³µ")
        return results

    def _save_final_results(self) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        # CSV ì €ì¥
        results_df = pd.DataFrame(
            [
                {
                    "experiment_id": r.experiment_id,
                    "tokenizer_name": r.tokenizer_name or "",
                    "chunker_name": r.chunker_name,
                    "chunker_params": json.dumps(r.chunker_params),
                    "embedding_name": r.embedding_name,
                    "embedding_model": r.embedding_model,
                    "total_chunks": r.total_chunks,
                    "embedding_time": r.embedding_time,
                    "database_size_mb": r.database_size_mb,
                    "database_path": r.database_path,
                    "success": r.success,
                    "error_message": r.error_message or "",
                    "timestamp": r.timestamp,
                }
                for r in self.results
            ]
        )

        csv_path = self.output_dir / "experiment_results.csv"
        results_df.to_csv(csv_path, index=False)

        # ìš”ì•½ í†µê³„
        successful_results = [r for r in self.results if r.success]
        total_time = time.time() - self.state.start_time

        summary = {
            "execution_time": total_time,
            "total_experiments": len(self.results),
            "successful_experiments": len(successful_results),
            "failed_experiments": len(self.results) - len(successful_results),
            "success_rate": len(successful_results) / len(self.results)
            if self.results
            else 0,
            "total_database_size_mb": sum(
                r.database_size_mb for r in successful_results
            ),
            "avg_embedding_time": np.mean(
                [r.embedding_time for r in successful_results]
            )
            if successful_results
            else 0,
            "state": {
                "total_documents": self.state.total_documents,
                "tokenization_results": self.state.tokenization_results,
                "chunking_results": self.state.chunking_results,
                "embedding_results": self.state.embedding_results,
            },
        }

        # JSON ì €ì¥
        summary_path = self.output_dir / "experiment_summary.json"
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        # ë¡œê·¸ ì¶œë ¥
        logger.info(f"\n" + "=" * 80)
        logger.info("ğŸ‰ Multi-Embedding Database Builder ì™„ë£Œ!")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        logger.info(f"ğŸ“„ ì²˜ë¦¬ëœ ë¬¸ì„œ: {self.state.total_documents}ê°œ")
        logger.info(f"ğŸ”¤ í† í°í™” ê²°ê³¼: {self.state.tokenization_results}ê°œ")
        logger.info(f"ğŸ§© ì²­í‚¹ ê²°ê³¼: {self.state.chunking_results}ê°œ")
        logger.info(f"ğŸ”® ì„ë² ë”© ê²°ê³¼: {self.state.embedding_results}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {len(successful_results)}ê°œ ë²¡í„°ìŠ¤í† ì–´")
        logger.info(f"âŒ ì‹¤íŒ¨: {len(self.results) - len(successful_results)}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']:.1%}")
        logger.info(f"ğŸ’¾ ì´ DB í¬ê¸°: {summary['total_database_size_mb']:.1f}MB")
        logger.info(f"\nğŸ“ ê²°ê³¼ ìœ„ì¹˜: {self.output_dir}")

        return summary


# ===================================================================
# CLI ë° ë©”ì¸ ì‹¤í–‰
# ===================================================================


def main():
    parser = argparse.ArgumentParser(description="Multi-Embedding Database Builder")
    parser.add_argument("--dry-run", action="store_true", help="ì„¤ì • ê²€ì¦ë§Œ ì‹¤í–‰")
    parser.add_argument("--output-dir", type=str, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--config-summary", action="store_true", help="ì„¤ì • ìš”ì•½ ì¶œë ¥")

    args = parser.parse_args()

    # ì„¤ì • ìš”ì•½ ì¶œë ¥
    if args.config_summary or args.dry_run:
        print_configuration_summary()

        # ì„¤ì • ê²€ì¦
        validation = validate_configurations()
        if not validation["valid"]:
            logger.error("âŒ ì„¤ì • ì˜¤ë¥˜:")
            for error in validation["errors"]:
                logger.error(f"  â€¢ {error}")
            return 1

        if args.dry_run:
            logger.info("âœ… ì„¤ì • ê²€ì¦ ì™„ë£Œ - ì‹¤í–‰í•˜ì§€ ì•ŠìŒ")
            return 0

    # ì‹¤ì œ ì‹¤í—˜ ì‹¤í–‰
    try:
        runner = MultiEmbeddingExperimentRunner(args.output_dir)
        result = runner.run_all_experiments()

        if result["success"]:
            logger.info("ğŸ‰ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
            return 0
        else:
            logger.error(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {result.get('error')}")
            return 1

    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())

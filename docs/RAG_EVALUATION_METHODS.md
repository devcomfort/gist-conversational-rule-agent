# ğŸ“Š RAG í‰ê°€ ë°©ë²•ë¡  - ë²•ì  ë¬¸ì„œ ê¸°ë°˜ ì²­í‚¹-ì„ë² ë”© ì¡°í•© ì—°êµ¬

## ğŸ¯ **í”„ë¡œì íŠ¸ ë§ì¶¤ í‰ê°€ ì „ëµ**

**ë°ì´í„°**: ë²•ì  ë¬¸ì„œ (rules ë””ë ‰í† ë¦¬)  
**ì¡°í•©**: 28,560ê°œ (6ê°œ ì²­ì»¤ Ã— ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° Ã— 6ê°œ ì„ë² ë”© ëª¨ë¸)  
**ëª©í‘œ**: ìµœì  Retriever êµ¬ì„±ì„ ìœ„í•œ ì²´ê³„ì  ì„±ëŠ¥ ë¶„ì„

---

## ğŸ“‹ **Phase 2: ë°ì´í„° í’ˆì§ˆ í‰ê°€ (í˜„ì¬ ë‹¨ê³„)**

### ğŸ” **1. ì²­í‚¹ í’ˆì§ˆ í‰ê°€**

#### **ë©”íŠ¸ë¦­ ì •ì˜**
```python
class ChunkQualityEvaluator:
    """ì²­í‚¹ ê²°ê³¼ í’ˆì§ˆ í‰ê°€"""
    
    def evaluate_chunking_quality(self, chunks, original_docs):
        metrics = {}
        
        # 1) ì²­í¬ í¬ê¸° ë¶„í¬
        chunk_sizes = [len(chunk.page_content) for chunk in chunks]
        metrics['avg_chunk_size'] = np.mean(chunk_sizes)
        metrics['std_chunk_size'] = np.std(chunk_sizes)
        metrics['chunk_size_cv'] = metrics['std_chunk_size'] / metrics['avg_chunk_size']
        
        # 2) ì²­í¬ ê°œìˆ˜ íš¨ìœ¨ì„±
        total_content = sum(len(doc.page_content) for doc in original_docs)
        metrics['chunks_per_1k_chars'] = len(chunks) / (total_content / 1000)
        
        # 3) ë¬¸ì¥ ê²½ê³„ ë³´ì¡´ë¥  (ë²•ì  ë¬¸ì„œ ì¤‘ìš”)
        sentence_boundary_preserved = self.check_sentence_boundaries(chunks)
        metrics['sentence_boundary_rate'] = sentence_boundary_preserved
        
        # 4) ë²•ì  ìš©ì–´ ë¶„í•  ì˜¤ë¥˜ë¥ 
        legal_term_split_errors = self.check_legal_term_integrity(chunks)
        metrics['legal_term_error_rate'] = legal_term_split_errors
        
        return metrics
    
    def check_sentence_boundaries(self, chunks):
        """ë¬¸ì¥ì´ ì¤‘ê°„ì— ëŠì–´ì§„ ì²­í¬ ë¹„ìœ¨ í™•ì¸"""
        boundary_preserved = 0
        for chunk in chunks:
            text = chunk.page_content.strip()
            if text.endswith(('.', '!', '?', 'ë‹¤', 'ìŒ')):  # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ë£Œ
                boundary_preserved += 1
        return boundary_preserved / len(chunks)
    
    def check_legal_term_integrity(self, chunks):
        """ë²•ì  ìš©ì–´ê°€ ì¤‘ê°„ì— ë¶„í• ëœ ê²½ìš° í™•ì¸"""
        legal_terms = ['ê³„ì•½ì„œ', 'ìœ„ì•½ê¸ˆ', 'ì†í•´ë°°ìƒ', 'ë²•ì •ì†í•´ê¸ˆ', 'ê³„ì•½í•´ì§€']
        split_errors = 0
        
        for chunk in chunks:
            content = chunk.page_content
            for term in legal_terms:
                # ìš©ì–´ê°€ ì²­í¬ ê²½ê³„ì—ì„œ ë¶„í• ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if self.is_term_split_at_boundary(content, term):
                    split_errors += 1
        
        return split_errors / len(chunks)
```

#### **ë²•ì  ë¬¸ì„œ íŠ¹í™” í‰ê°€**
```python
def evaluate_legal_document_chunking(chunks):
    """ë²•ì  ë¬¸ì„œ ì²­í‚¹ íŠ¹í™” í‰ê°€"""
    
    # 1) ì¡°ë¬¸/í•­ëª© êµ¬ì¡° ë³´ì¡´ë¥ 
    article_preservation = check_article_structure_preservation(chunks)
    
    # 2) ì°¸ì¡° ê´€ê³„ ìœ ì§€ë¥  (ì˜ˆ: "ì œ3ì¡°ì— ë”°ë¼")
    reference_integrity = check_legal_reference_integrity(chunks)
    
    # 3) ê³„ì•½ ì¡°ê±´ ì™„ì „ì„±
    contract_clause_completeness = check_contract_clause_completeness(chunks)
    
    return {
        'article_preservation_rate': article_preservation,
        'reference_integrity_rate': reference_integrity,  
        'clause_completeness_rate': contract_clause_completeness
    }
```

### ğŸ“ **2. QA ë°ì´í„°ì…‹ í’ˆì§ˆ í‰ê°€**

#### **ìë™ í’ˆì§ˆ í‰ê°€**
```python
class QAQualityEvaluator:
    """ìƒì„±ëœ QA ìŒì˜ í’ˆì§ˆ ìë™ í‰ê°€"""
    
    def __init__(self, korean_nlp_model="klue/roberta-large"):
        self.nlp_model = korean_nlp_model
        self.legal_keywords = self.load_legal_keywords()
    
    def evaluate_qa_quality(self, qa_pairs, source_chunks):
        metrics = {}
        
        # 1) ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„
        question_complexity = self.analyze_question_complexity(qa_pairs)
        metrics['avg_question_complexity'] = np.mean(question_complexity)
        
        # 2) ë‹µë³€ ì™„ì „ì„± (ì²­í¬ ë‚´ìš© ê¸°ë°˜)
        answer_completeness = self.check_answer_completeness(qa_pairs, source_chunks)
        metrics['answer_completeness_rate'] = np.mean(answer_completeness)
        
        # 3) ë²•ì  ìš©ì–´ í¬í•¨ë¥ 
        legal_term_coverage = self.check_legal_term_coverage(qa_pairs)
        metrics['legal_term_coverage'] = legal_term_coverage
        
        # 4) ì§ˆë¬¸-ë‹µë³€ ì¼ì¹˜ë„
        qa_consistency = self.measure_qa_consistency(qa_pairs)
        metrics['qa_consistency_score'] = np.mean(qa_consistency)
        
        # 5) ë‹¤ì–‘ì„± ì ìˆ˜
        diversity_score = self.calculate_qa_diversity(qa_pairs)
        metrics['qa_diversity_score'] = diversity_score
        
        return metrics
    
    def analyze_question_complexity(self, qa_pairs):
        """ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„ (ë¬¸ì¥ ê¸¸ì´, ì˜ë¬¸ì‚¬ ìœ í˜•, ë²•ì  ê°œë… í¬í•¨)"""
        complexity_scores = []
        
        for qa in qa_pairs:
            question = qa['question']
            
            # ë¬¸ì¥ ê¸¸ì´ ì ìˆ˜
            length_score = min(len(question) / 100, 1.0)
            
            # ì˜ë¬¸ì‚¬ ë³µì¡ë„ (ëˆ„ê°€, ì–¸ì œ < ì–´ë–»ê²Œ, ì™œ)
            complexity_words = ['ì–´ë–»ê²Œ', 'ì™œ', 'ì–´ë–¤ ì¡°ê±´', 'ì–´ë–¤ ê²½ìš°']
            complexity_score = sum(1 for word in complexity_words if word in question)
            
            # ë²•ì  ê°œë… í¬í•¨ ì ìˆ˜
            legal_concept_score = sum(1 for term in self.legal_keywords if term in question) / len(self.legal_keywords)
            
            total_score = (length_score + complexity_score + legal_concept_score) / 3
            complexity_scores.append(total_score)
        
        return complexity_scores
```

---

## ğŸ” **Phase 3: Retrieval ì„±ëŠ¥ í‰ê°€ (í•µì‹¬ ë‹¨ê³„)**

### ğŸ“Š **1. ê¸°ë³¸ ê²€ìƒ‰ ì„±ëŠ¥ ë©”íŠ¸ë¦­**

#### **êµ¬í˜„ ê°€ëŠ¥í•œ í•µì‹¬ ë©”íŠ¸ë¦­**
```python
class RetrievalEvaluator:
    """28,560ê°œ ì¡°í•© ëŒ€ìƒ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€"""
    
    def __init__(self, qa_dataset, vector_databases_dir):
        self.qa_dataset = qa_dataset  # Phase 2ì—ì„œ ìƒì„±ëœ QA ìŒ
        self.db_dir = vector_databases_dir
        
    def evaluate_all_combinations(self):
        """ëª¨ë“  ì²­í‚¹-ì„ë² ë”© ì¡°í•© í‰ê°€"""
        results = {}
        
        # ëª¨ë“  FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
        db_paths = self.get_all_database_paths()
        
        for db_path in tqdm(db_paths, desc="Evaluating combinations"):
            combination_id = self.extract_combination_id(db_path)
            
            # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
            vectorstore = self.load_vectorstore(db_path)
            
            # ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
            metrics = self.evaluate_single_combination(vectorstore)
            
            results[combination_id] = metrics
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del vectorstore
            gc.collect()
        
        return self.analyze_results(results)
    
    def evaluate_single_combination(self, vectorstore):
        """ë‹¨ì¼ ì¡°í•© í‰ê°€"""
        metrics = {
            'recall_at_1': [], 'recall_at_5': [], 'recall_at_10': [],
            'precision_at_1': [], 'precision_at_5': [], 'precision_at_10': [],
            'mrr': [], 'ndcg_at_10': [], 'map': []
        }
        
        for qa_item in self.qa_dataset:
            query = qa_item['question']
            relevant_chunk_ids = qa_item['relevant_chunks']
            
            # ê²€ìƒ‰ ìˆ˜í–‰ (ë‹¤ì–‘í•œ Kê°’)
            search_results = {
                k: vectorstore.similarity_search_with_score(query, k=k)
                for k in [1, 5, 10]
            }
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            for k in [1, 5, 10]:
                retrieved_ids = [doc.metadata.get('chunk_id') for doc, _ in search_results[k]]
                
                # Recall@K
                recall_k = len(set(retrieved_ids) & set(relevant_chunk_ids)) / len(relevant_chunk_ids)
                metrics[f'recall_at_{k}'].append(recall_k)
                
                # Precision@K
                precision_k = len(set(retrieved_ids) & set(relevant_chunk_ids)) / k
                metrics[f'precision_at_{k}'].append(precision_k)
            
            # MRR ê³„ì‚°
            mrr_score = self.calculate_mrr(search_results[10], relevant_chunk_ids)
            metrics['mrr'].append(mrr_score)
            
            # NDCG@10 ê³„ì‚°
            ndcg_score = self.calculate_ndcg(search_results[10], relevant_chunk_ids)
            metrics['ndcg_at_10'].append(ndcg_score)
        
        # í‰ê·  ê³„ì‚°
        return {metric: np.mean(scores) for metric, scores in metrics.items()}
    
    def calculate_mrr(self, search_results, relevant_ids):
        """Mean Reciprocal Rank ê³„ì‚°"""
        for rank, (doc, _) in enumerate(search_results, 1):
            chunk_id = doc.metadata.get('chunk_id')
            if chunk_id in relevant_ids:
                return 1.0 / rank
        return 0.0
    
    def calculate_ndcg(self, search_results, relevant_ids, k=10):
        """Normalized Discounted Cumulative Gain ê³„ì‚°"""
        dcg = 0.0
        idcg = sum(1.0 / math.log2(i + 2) for i in range(min(len(relevant_ids), k)))
        
        for rank, (doc, _) in enumerate(search_results[:k], 1):
            chunk_id = doc.metadata.get('chunk_id')
            if chunk_id in relevant_ids:
                dcg += 1.0 / math.log2(rank + 1)
        
        return dcg / idcg if idcg > 0 else 0.0
```

### ğŸ¯ **2. ë²•ì  ë¬¸ì„œ íŠ¹í™” í‰ê°€**

#### **ë„ë©”ì¸ íŠ¹í™” ë©”íŠ¸ë¦­**
```python
class LegalRAGEvaluator:
    """ë²•ì  ë¬¸ì„œ íŠ¹í™” RAG í‰ê°€"""
    
    def evaluate_legal_retrieval(self, vectorstore, legal_queries):
        """ë²•ì  ê²€ìƒ‰ íŠ¹í™” í‰ê°€"""
        
        # 1) ë²•ì  ê°œë… ê²€ìƒ‰ ì •í™•ë„
        legal_concept_accuracy = self.evaluate_legal_concept_retrieval(
            vectorstore, legal_queries['concept_queries']
        )
        
        # 2) ì¡°ë¬¸ ì°¸ì¡° ê²€ìƒ‰ ì •í™•ë„
        article_reference_accuracy = self.evaluate_article_reference_retrieval(
            vectorstore, legal_queries['article_queries']
        )
        
        # 3) ê³„ì•½ ì¡°ê±´ ê²€ìƒ‰ ì •í™•ë„  
        contract_clause_accuracy = self.evaluate_contract_clause_retrieval(
            vectorstore, legal_queries['clause_queries']
        )
        
        # 4) ì˜ˆì™¸ ì¡°ê±´ ê²€ìƒ‰ ì •í™•ë„
        exception_handling_accuracy = self.evaluate_exception_handling_retrieval(
            vectorstore, legal_queries['exception_queries']
        )
        
        return {
            'legal_concept_accuracy': legal_concept_accuracy,
            'article_reference_accuracy': article_reference_accuracy,
            'contract_clause_accuracy': contract_clause_accuracy,
            'exception_handling_accuracy': exception_handling_accuracy
        }
    
    def evaluate_legal_concept_retrieval(self, vectorstore, concept_queries):
        """ë²•ì  ê°œë… ê²€ìƒ‰ í‰ê°€ (ì˜ˆ: ìœ„ì•½ê¸ˆ, ì†í•´ë°°ìƒ)"""
        accuracies = []
        
        for query_item in concept_queries:
            query = query_item['question']  # "ìœ„ì•½ê¸ˆì€ ì–¸ì œ ì§€ë¶ˆí•˜ë‚˜ìš”?"
            expected_concepts = query_item['legal_concepts']  # ['ìœ„ì•½ê¸ˆ', 'ì§€ê¸‰ì‹œê¸°']
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            results = vectorstore.similarity_search(query, k=5)
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ë²•ì  ê°œë… ì¶”ì¶œ
            retrieved_concepts = self.extract_legal_concepts_from_results(results)
            
            # ì¼ì¹˜ìœ¨ ê³„ì‚°
            concept_match_rate = len(set(retrieved_concepts) & set(expected_concepts)) / len(expected_concepts)
            accuracies.append(concept_match_rate)
        
        return np.mean(accuracies)
```

### ğŸ“ˆ **3. íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„**

#### **ì²­í‚¹ íŒŒë¼ë¯¸í„° ì˜í–¥ ë¶„ì„**
```python
def analyze_chunking_parameter_sensitivity():
    """ì²­í‚¹ íŒŒë¼ë¯¸í„°ê°€ ê²€ìƒ‰ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„"""
    
    # ê²°ê³¼ ë°ì´í„° ë¡œë“œ (28,560ê°œ ì¡°í•© ê²°ê³¼)
    results_df = pd.read_csv('experiments/outputs/experiment_results.csv')
    
    # íŒŒë¼ë¯¸í„°ë³„ ë¶„ì„
    parameter_analysis = {}
    
    # 1) ì²­í¬ í¬ê¸° ì˜í–¥ ë¶„ì„
    chunk_size_analysis = results_df.groupby('chunk_size').agg({
        'recall_at_5': ['mean', 'std'],
        'precision_at_5': ['mean', 'std'],
        'mrr': ['mean', 'std']
    })
    parameter_analysis['chunk_size'] = chunk_size_analysis
    
    # 2) ì²­í¬ ê²¹ì¹¨ ë¹„ìœ¨ ì˜í–¥ ë¶„ì„
    overlap_analysis = results_df.groupby('chunk_overlap').agg({
        'recall_at_5': ['mean', 'std'],
        'precision_at_5': ['mean', 'std']
    })
    parameter_analysis['chunk_overlap'] = overlap_analysis
    
    # 3) ì²­ì»¤ ìœ í˜•ë³„ ì„±ëŠ¥ ë¹„êµ
    chunker_comparison = results_df.groupby('chunker_name').agg({
        'recall_at_5': ['mean', 'std', 'max'],
        'precision_at_5': ['mean', 'std', 'max'],
        'mrr': ['mean', 'std', 'max']
    })
    parameter_analysis['chunker_type'] = chunker_comparison
    
    # 4) ì„ë² ë”© ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
    embedding_comparison = results_df.groupby('embedding_name').agg({
        'recall_at_5': ['mean', 'std', 'max'],
        'precision_at_5': ['mean', 'std', 'max']
    })
    parameter_analysis['embedding_model'] = embedding_comparison
    
    return parameter_analysis

def generate_sensitivity_report(parameter_analysis):
    """íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    
    report = {
        'key_findings': {},
        'recommendations': {},
        'statistical_significance': {}
    }
    
    # ì²­í¬ í¬ê¸° ìµœì  ë²”ìœ„ ì‹ë³„
    chunk_size_perf = parameter_analysis['chunk_size']
    best_chunk_size = chunk_size_perf['recall_at_5']['mean'].idxmax()
    report['key_findings']['optimal_chunk_size'] = best_chunk_size
    
    # ì²­ì»¤ ìœ í˜• ìˆœìœ„
    chunker_ranking = parameter_analysis['chunker_type']['recall_at_5']['mean'].sort_values(ascending=False)
    report['key_findings']['chunker_ranking'] = chunker_ranking.to_dict()
    
    # ì„ë² ë”© ëª¨ë¸ ìˆœìœ„
    embedding_ranking = parameter_analysis['embedding_model']['recall_at_5']['mean'].sort_values(ascending=False)
    report['key_findings']['embedding_ranking'] = embedding_ranking.to_dict()
    
    return report
```

---

## ğŸ¨ **ì‹œê°í™” ë° ë¶„ì„ ë„êµ¬**

### ğŸ“Š **1. ì„±ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤ íˆíŠ¸ë§µ**

```python
def create_performance_heatmap(results_df):
    """ì²­ì»¤-ì„ë² ë”© ì¡°í•© ì„±ëŠ¥ íˆíŠ¸ë§µ"""
    
    # í”¼ë²— í…Œì´ë¸” ìƒì„±
    heatmap_data = results_df.pivot_table(
        values='recall_at_5',
        index='chunker_name', 
        columns='embedding_name',
        aggfunc='mean'
    )
    
    # íˆíŠ¸ë§µ ìƒì„±
    plt.figure(figsize=(12, 8))
    sns.heatmap(heatmap_data, annot=True, cmap='YlOrRd', fmt='.3f')
    plt.title('ì²­ì»¤-ì„ë² ë”© ì¡°í•©ë³„ Recall@5 ì„±ëŠ¥')
    plt.tight_layout()
    plt.savefig('performance_heatmap.png', dpi=300)
    
    return heatmap_data

def create_parameter_sensitivity_plots(parameter_analysis):
    """íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ì‹œê°í™”"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # ì²­í¬ í¬ê¸°ë³„ ì„±ëŠ¥
    chunk_size_data = parameter_analysis['chunk_size']['recall_at_5']['mean']
    axes[0,0].plot(chunk_size_data.index, chunk_size_data.values, marker='o')
    axes[0,0].set_title('ì²­í¬ í¬ê¸°ë³„ Recall@5')
    axes[0,0].set_xlabel('ì²­í¬ í¬ê¸°')
    axes[0,0].set_ylabel('Recall@5')
    
    # ì²­ì»¤ ìœ í˜•ë³„ ë°•ìŠ¤í”Œë¡¯
    chunker_box_data = results_df.boxplot(column='recall_at_5', by='chunker_name', ax=axes[0,1])
    axes[0,1].set_title('ì²­ì»¤ ìœ í˜•ë³„ ì„±ëŠ¥ ë¶„í¬')
    
    # ì„ë² ë”© ëª¨ë¸ë³„ ì„±ëŠ¥
    embedding_data = parameter_analysis['embedding_model']['recall_at_5']['mean']
    axes[1,0].bar(range(len(embedding_data)), embedding_data.values)
    axes[1,0].set_xticks(range(len(embedding_data)))
    axes[1,0].set_xticklabels(embedding_data.index, rotation=45)
    axes[1,0].set_title('ì„ë² ë”© ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥')
    
    plt.tight_layout()
    plt.savefig('parameter_sensitivity.png', dpi=300)
```

### ğŸ” **2. t-SNE ì„ë² ë”© ê³µê°„ ë¶„ì„**

```python
def create_embedding_space_analysis(vector_databases, sample_queries):
    """ì„ë² ë”© ê³µê°„ ì‹œê°í™”ë¥¼ í†µí•œ ì •ì„±ì  ë¶„ì„"""
    
    embeddings_analysis = {}
    
    for db_name, vectorstore in vector_databases.items():
        # ìƒ˜í”Œ ì¿¼ë¦¬ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±
        query_embeddings = []
        doc_embeddings = []
        
        for query in sample_queries:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_emb = vectorstore.embedding_function.embed_query(query)
            query_embeddings.append(query_emb)
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œ ì„ë² ë”©
            results = vectorstore.similarity_search(query, k=5)
            for doc in results:
                doc_emb = vectorstore.embedding_function.embed_documents([doc.page_content])
                doc_embeddings.extend(doc_emb)
        
        # t-SNE ì°¨ì› ì¶•ì†Œ
        all_embeddings = query_embeddings + doc_embeddings
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        embeddings_2d = tsne.fit_transform(all_embeddings)
        
        embeddings_analysis[db_name] = {
            'embeddings_2d': embeddings_2d,
            'query_count': len(query_embeddings),
            'doc_count': len(doc_embeddings)
        }
    
    return embeddings_analysis

def create_interactive_tsne_dashboard(embeddings_analysis, combination_metadata):
    """Gradio ê¸°ë°˜ t-SNE ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ"""
    
    def update_tsne_plot(chunker_type, embedding_model, threshold):
        # ì„ íƒëœ ì¡°í•©ì— í•´ë‹¹í•˜ëŠ” t-SNE ë°ì´í„° ë¡œë“œ
        combination_key = f"{chunker_type}_{embedding_model}"
        
        if combination_key in embeddings_analysis:
            data = embeddings_analysis[combination_key]
            
            # í”Œë¡¯ ìƒì„±
            fig, ax = plt.subplots(figsize=(10, 8))
            
            # ì¿¼ë¦¬ í¬ì¸íŠ¸ (ë¹¨ê°„ìƒ‰)
            query_points = data['embeddings_2d'][:data['query_count']]
            ax.scatter(query_points[:, 0], query_points[:, 1], 
                      c='red', s=100, alpha=0.7, label='Queries')
            
            # ë¬¸ì„œ í¬ì¸íŠ¸ (íŒŒë€ìƒ‰)
            doc_points = data['embeddings_2d'][data['query_count']:]
            ax.scatter(doc_points[:, 0], doc_points[:, 1], 
                      c='blue', s=50, alpha=0.5, label='Documents')
            
            ax.set_title(f'{chunker_type} + {embedding_model}')
            ax.legend()
            
            return fig
        else:
            return plt.figure()
    
    # Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks() as demo:
        with gr.Row():
            chunker_dropdown = gr.Dropdown(
                choices=['TokenChunker', 'SentenceChunker', 'SemanticChunker', 
                        'LateChunker', 'NeuralChunker', 'RecursiveChunker'],
                value='SentenceChunker',
                label="ì²­ì»¤ ìœ í˜•"
            )
            embedding_dropdown = gr.Dropdown(
                choices=['qwen3_8b', 'qwen3_0_6b', 'jina_v3', 'bge_m3', 
                        'all_minilm_l6', 'multilingual_e5'],
                value='all_minilm_l6',
                label="ì„ë² ë”© ëª¨ë¸"
            )
        
        tsne_plot = gr.Plot(label="t-SNE ì„ë² ë”© ê³µê°„")
        
        # ìƒí˜¸ì‘ìš© ì„¤ì •
        for input_component in [chunker_dropdown, embedding_dropdown]:
            input_component.change(
                update_tsne_plot,
                inputs=[chunker_dropdown, embedding_dropdown],
                outputs=[tsne_plot]
            )
    
    return demo
```

---

## ğŸ“‹ **ì‹¤í–‰ ê³„íš ë° êµ¬í˜„ ìˆœì„œ**

### ğŸš€ **Phase 3-1: ê¸°ë³¸ í‰ê°€ ì‹œìŠ¤í…œ (Week 1-2)**

```python
# 1ìˆœìœ„ êµ¬í˜„ ëª©ë¡
priority_1_implementation = [
    "RetrievalEvaluator í´ë˜ìŠ¤ êµ¬í˜„",
    "ê¸°ë³¸ ë©”íŠ¸ë¦­ (Recall@K, Precision@K, MRR) ê³„ì‚°",
    "28,560ê°œ ì¡°í•© ìë™ í‰ê°€ íŒŒì´í”„ë¼ì¸",
    "ê²°ê³¼ ì €ì¥ ë° CSV ì¶œë ¥"
]

# êµ¬í˜„ ì½”ë“œ í…œí”Œë¦¿
evaluation_pipeline = """
python -c "
from src.evaluation.retrieval_evaluator import RetrievalEvaluator
evaluator = RetrievalEvaluator('qa_dataset.json', 'experiments/outputs/')
results = evaluator.evaluate_all_combinations()
results.to_csv('evaluation_results.csv')
"
"""
```

### ğŸ“Š **Phase 3-2: ë¶„ì„ ë° ì‹œê°í™” (Week 3-4)**

```python
# 2ìˆœìœ„ êµ¬í˜„ ëª©ë¡  
priority_2_implementation = [
    "íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„ ì‹œìŠ¤í…œ",
    "ì„±ëŠ¥ íˆíŠ¸ë§µ ë° ì‹œê°í™” ë„êµ¬",
    "ë²•ì  ë¬¸ì„œ íŠ¹í™” ë©”íŠ¸ë¦­ êµ¬í˜„",
    "í†µê³„ì  ìœ ì˜ì„± ê²€ì¦"
]
```

### ğŸ¨ **Phase 3-3: ëŒ€ì‹œë³´ë“œ ë° ë³´ê³ ì„œ (Week 5-6)**

```python
# 3ìˆœìœ„ êµ¬í˜„ ëª©ë¡
priority_3_implementation = [
    "Gradio í‰ê°€ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•",
    "t-SNE ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”",
    "ì¢…í•© ì„±ëŠ¥ ë³´ê³ ì„œ ìë™ ìƒì„±",
    "ìµœì  ì¡°í•© ì¶”ì²œ ì‹œìŠ¤í…œ"
]
```

---

## ğŸ¯ **ê¸°ëŒ€ ì„±ê³¼ ë° í™œìš© ë°©ì•ˆ**

### ğŸ“ˆ **ì •ëŸ‰ì  ì„±ê³¼**
- **28,560ê°œ ì¡°í•©** ì²´ê³„ì  í‰ê°€ ì™„ë£Œ
- **Top 10% ì„±ëŠ¥ ì¡°í•©** ëª…í™•íˆ ì‹ë³„ (ì•½ 2,856ê°œ)
- **ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„ ë¥ ** ì •ëŸ‰ì  ì¸¡ì •
- **íŒŒë¼ë¯¸í„°ë³„ ê¸°ì—¬ë„** í†µê³„ì  ê²€ì¦

### ğŸ“ **í•™ìˆ ì  ê¸°ì—¬**
- **í¬ê´„ì  ë¹„êµ ì—°êµ¬**: ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„ 10ë°° ì´ìƒ ì‹¤í—˜ ê·œëª¨
- **ë²•ì  ë¬¸ì„œ íŠ¹í™”**: ë„ë©”ì¸ íŠ¹í™” RAG í‰ê°€ ë°©ë²•ë¡  ì œì‹œ
- **ì¬í˜„ ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬**: ì˜¤í”ˆì†ŒìŠ¤ í‰ê°€ ë„êµ¬ ì œê³µ

### ğŸ’¼ **ì‹¤ìš©ì  í™œìš©**
- **ìµœì  ì„¤ì • ê°€ì´ë“œ**: ë²•ì  ë¬¸ì„œìš© Retriever êµ¬ì„± ê°€ì´ë“œë¼ì¸
- **ì„±ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸**: ìƒˆë¡œìš´ ì¡°í•©ì˜ ì„±ëŠ¥ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- **ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„**: ì„±ëŠ¥ ëŒ€ë¹„ ê³„ì‚° ë¹„ìš© ìµœì í™”

---

## ğŸ“š **ì°¸ê³  ìë£Œ ë° ë„êµ¬**

### ğŸ› ï¸ **êµ¬í˜„ ë„êµ¬**
```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
required_libraries = [
    "scikit-learn",      # í‰ê°€ ë©”íŠ¸ë¦­
    "numpy", "pandas",   # ë°ì´í„° ì²˜ë¦¬
    "matplotlib", "seaborn",  # ì‹œê°í™”
    "gradio",           # ëŒ€ì‹œë³´ë“œ
    "tqdm",             # ì§„í–‰ë¥  í‘œì‹œ
    "scipy"             # í†µê³„ ê²€ì¦
]

# ì„¤ì¹˜ ëª…ë ¹
pip install scikit-learn numpy pandas matplotlib seaborn gradio tqdm scipy
```

### ğŸ“– **í‰ê°€ ê¸°ì¤€ ì°¸ê³ **
- **MS MARCO**: ëŒ€ê·œëª¨ ê²€ìƒ‰ í‰ê°€ í‘œì¤€
- **BEIR**: ì •ë³´ ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬  
- **RAGAS**: RAG íŠ¹í™” í‰ê°€ í”„ë ˆì„ì›Œí¬
- **TruLens**: RAG ì‹œìŠ¤í…œ í‰ê°€ ë° ëª¨ë‹ˆí„°ë§

---

*ì´ ë¬¸ì„œëŠ” í˜„ì¬ í”„ë¡œì íŠ¸ ë°ì´í„°ì™€ ìƒí™©ì— íŠ¹í™”ëœ ì‹¤í–‰ ê°€ëŠ¥í•œ í‰ê°€ ë°©ë²•ë“¤ì„ ì •ë¦¬í•œ ê²ƒìœ¼ë¡œ, Phase 3 êµ¬í˜„ ì‹œ ì§ì ‘ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.*

**Last Updated**: 2025-09-03  
**Status**: Ready for Phase 3 Implementation

#!/usr/bin/env python3
"""
Chunking Database Builder
========================

Phase 2-1: ì²­í‚¹ ë°ì´í„° ìƒì„± ì „ìš© ëª¨ë“ˆ
ë²•ë¥  ë¬¸ì„œë¥¼ ë‹¤ì–‘í•œ ì²­í‚¹ ì „ëµìœ¼ë¡œ ë¶„í• í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- 4,760ê°œ ì²­í‚¹ ì¡°í•© ì²˜ë¦¬
- í† í°, ë¬¸ì¥, ì˜ë¯¸, ì‹ ê²½ë§, ì¬ê·€ ì²­í‚¹ ì§€ì›
- ì²­í‚¹ ê²°ê³¼ pickle í˜•íƒœë¡œ ì €ì¥
- ì²­í‚¹ í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

ë°ì´í„° íë¦„:
    Documents â†’ Chunking â†’ chunks/*.pkl + metadata

ì‚¬ìš©ë²•:
    python chunking_database_builder.py
    python chunking_database_builder.py --max-chunks 100
    python chunking_database_builder.py --chunkers token,sentence
    python chunking_database_builder.py --dry-run
"""

import os
import sys
import json
import time
import pickle
import itertools
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Iterator

import numpy as np
import pandas as pd
from loguru import logger
from llama_index.core import Document

# ë¡œì»¬ ëª¨ë“ˆ
from core.experiment.commons import (
    DocumentManager,
    TokenizationManager,
    ExperimentState,
    ExperimentNaming,
    log_experiment_start,
    log_experiment_end,
)
from experiment_configurations import (
    CHUNKERS,
    EMBEDDING_MODELS,
    EXPERIMENT_CONFIG,
    validate_configurations,
    print_configuration_summary,
)

# ===================================================================
# ì²­í‚¹ ê´€ë¦¬ì
# ===================================================================


class ChunkingManager:
    """ì²­í‚¹ ê´€ë¦¬"""

    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.chunk_dir = (
            self.output_dir / EXPERIMENT_CONFIG["output"]["chunking_subdir"]
        )
        self.chunk_dir.mkdir(parents=True, exist_ok=True)

    def generate_chunking_combinations(self) -> Iterator[Tuple[str, Dict[str, Any]]]:
        """ì²­í‚¹ ì¡°í•© ìƒì„±ê¸°"""
        logger.info("ğŸ§© ì²­í‚¹ ì¡°í•© ìƒì„±...")

        # í•„í„° ì ìš©
        chunkers = CHUNKERS
        if EXPERIMENT_CONFIG["filters"]["chunkers"]:
            chunkers = {
                k: v
                for k, v in chunkers.items()
                if k in EXPERIMENT_CONFIG["filters"]["chunkers"]
            }

        total_combinations = 0

        for chunker_name, config in chunkers.items():
            logger.info(f"ğŸ§© ì²­ì»¤: {chunker_name}")

            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            base_params = config["parameters"]["base"].copy()
            variations = config["parameters"]["variations"]

            # íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
            param_lists = {}
            static_params = base_params.copy()

            for param_name, param_values in variations.items():
                if param_name == "embedding_model" and param_values == "ALL_EMBEDDINGS":
                    # ì„ë² ë”© ëª¨ë¸ í•„í„° ì ìš©
                    embedding_models = EMBEDDING_MODELS
                    if EXPERIMENT_CONFIG["filters"]["embedding_models"]:
                        embedding_models = {
                            k: v
                            for k, v in embedding_models.items()
                            if k in EXPERIMENT_CONFIG["filters"]["embedding_models"]
                        }
                    param_lists[param_name] = [
                        v["model_name"] for v in embedding_models.values()
                    ]
                elif isinstance(param_values, list):
                    param_lists[param_name] = param_values
                else:
                    static_params[param_name] = param_values

            # ì¡°í•© ìƒì„±
            if param_lists:
                keys, values = zip(*param_lists.items())
                for combination in itertools.product(*values):
                    params = static_params.copy()
                    params.update(dict(zip(keys, combination)))
                    total_combinations += 1
                    yield chunker_name, params
            else:
                total_combinations += 1
                yield chunker_name, static_params

        logger.info(f"ğŸ“Š ì´ ì²­í‚¹ ì¡°í•©: {total_combinations}ê°œ")

    def chunk_documents(
        self, chunker_name: str, params: Dict[str, Any], documents: List[Document]
    ) -> Optional[str]:
        """ë¬¸ì„œ ì²­í‚¹ ìˆ˜í–‰"""
        try:
            # ì²­ì»¤ ìƒì„±
            chunker_class = CHUNKERS[chunker_name]["class"]
            clean_params = {k: v for k, v in params.items() if v is not None}

            logger.debug(f"  ğŸ“„ ì²­í‚¹: {chunker_name} with {clean_params}")

            start_time = time.time()
            chunker = chunker_class(**clean_params)

            # ë¬¸ì„œë“¤ ì²­í‚¹
            all_chunks = []
            for doc in documents:
                try:
                    chunks = chunker.chunk(doc.text)

                    for i, chunk in enumerate(chunks):
                        # ì²­í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        chunk_text = (
                            chunk.text if hasattr(chunk, "text") else str(chunk)
                        )

                        # Document ê°ì²´ ìƒì„±
                        chunk_doc = Document(
                            text=chunk_text,
                            metadata={
                                **doc.metadata,
                                "chunk_index": i,
                                "chunk_size": len(chunk_text),
                                "parent_doc_id": doc.metadata.get("doc_id", "unknown"),
                                "chunker_name": chunker_name,
                                "chunker_params": clean_params,
                            },
                        )
                        all_chunks.append(chunk_doc)

                except Exception as e:
                    logger.debug(f"    âš ï¸ ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨: {e}")
                    continue

            if not all_chunks:
                logger.warning(f"  âŒ ì²­í‚¹ ê²°ê³¼ ì—†ìŒ: {chunker_name}")
                return None

            processing_time = time.time() - start_time

            # ê²°ê³¼ ì €ì¥
            chunk_id = self._save_chunking_result(
                chunker_name, params, all_chunks, processing_time
            )

            logger.debug(
                f"  âœ… ì²­í‚¹ ì™„ë£Œ: {chunk_id} ({len(all_chunks)}ê°œ, {processing_time:.1f}ì´ˆ)"
            )
            return chunk_id

        except Exception as e:
            logger.error(f"  âŒ ì²­í‚¹ ì‹¤íŒ¨: {chunker_name} - {e}")
            return None

    def _save_chunking_result(
        self,
        chunker_name: str,
        params: Dict[str, Any],
        chunks: List[Document],
        processing_time: float,
    ) -> str:
        """ì²­í‚¹ ê²°ê³¼ ì €ì¥"""
        chunk_id = ExperimentNaming.generate_chunk_id(chunker_name, params)

        # ì²­í¬ ë°ì´í„° ì €ì¥
        chunks_file = self.chunk_dir / f"{chunk_id}_chunks.pkl"
        with open(chunks_file, "wb") as f:
            pickle.dump(chunks, f)

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "chunk_id": chunk_id,
            "chunker_name": chunker_name,
            "chunker_params": params,
            "total_chunks": len(chunks),
            "avg_chunk_size": float(np.mean([len(doc.text) for doc in chunks])),
            "chunk_size_variance": float(np.var([len(doc.text) for doc in chunks])),
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "file_size_mb": chunks_file.stat().st_size / (1024 * 1024),
        }

        metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        return chunk_id

    def load_chunking_result(
        self, chunk_id: str
    ) -> Tuple[List[Document], Dict[str, Any]]:
        """ì²­í‚¹ ê²°ê³¼ ë¡œë“œ"""
        chunks_file = self.chunk_dir / f"{chunk_id}_chunks.pkl"
        metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"

        if not chunks_file.exists():
            raise FileNotFoundError(f"ì²­í‚¹ ê²°ê³¼ ì—†ìŒ: {chunk_id}")

        with open(chunks_file, "rb") as f:
            chunks = pickle.load(f)

        metadata = {}
        if metadata_file.exists():
            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)

        return chunks, metadata

    def scan_existing_chunks(self) -> List[str]:
        """ê¸°ì¡´ì— ìƒì„±ëœ ì²­í¬ ID ìŠ¤ìº”"""
        if not self.chunk_dir.exists():
            return []

        chunk_ids = []
        for chunks_file in self.chunk_dir.glob("*_chunks.pkl"):
            chunk_id = chunks_file.stem.replace("_chunks", "")

            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
            metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"
            if metadata_file.exists():
                chunk_ids.append(chunk_id)

        return sorted(chunk_ids)


# ===================================================================
# ì²­í‚¹ ì‹¤í—˜ ì‹¤í–‰ê¸°
# ===================================================================


class ChunkingExperimentRunner:
    """ì²­í‚¹ ì‹¤í—˜ ì „ìš© ì‹¤í–‰ê¸° - Phase 2-1"""

    def __init__(self, output_dir: Optional[str] = None):
        self.output_dir = Path(output_dir or EXPERIMENT_CONFIG["output"]["base_dir"])
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.doc_manager = DocumentManager()
        self.tokenization_manager = TokenizationManager(self.output_dir)
        self.chunking_manager = ChunkingManager(self.output_dir)

        # ìƒíƒœ ì¶”ì 
        self.state = ExperimentState()

    def run_all_chunking_experiments(
        self,
        max_chunks: Optional[int] = None,
        chunker_filter: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """ëª¨ë“  ì²­í‚¹ ì‹¤í—˜ ì‹¤í–‰"""

        log_experiment_start("ì²­í‚¹ ë°ì´í„° ìƒì„±", 0)  # ì¡°í•© ìˆ˜ëŠ” ë‚˜ì¤‘ì— ê³„ì‚°
        self.state.start_time = time.time()
        self.state.current_phase = "ì²­í‚¹ ë°ì´í„° ìƒì„±"

        try:
            # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë”©
            self.state.current_phase = "ë¬¸ì„œ ë¡œë”©"
            logger.info(f"\nğŸ“– === {self.state.current_phase} ===")
            documents = self.doc_manager.load_all_documents()

            if not documents:
                return {"success": False, "error": "ë¬¸ì„œ ì—†ìŒ"}

            self.state.total_documents = len(documents)
            logger.info(f"âœ… ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(documents)}ê°œ")

            # ë‹¨ê³„ 2: í† í°í™” (ì„ íƒì )
            self.state.current_phase = "í† í°í™”"
            logger.info(f"\nğŸ”¤ === {self.state.current_phase} ===")
            tokenization_results = self.tokenization_manager.tokenize_documents(
                documents
            )
            self.state.tokenization_results = len(tokenization_results)

            if tokenization_results:
                logger.info(f"âœ… í† í°í™” ì™„ë£Œ: {len(tokenization_results)}ê°œ ê²°ê³¼")

            # ë‹¨ê³„ 3: ì²­í‚¹ ì‹¤í—˜
            self.state.current_phase = "ì²­í‚¹ ì‹¤í—˜"
            logger.info(f"\nğŸ§© === {self.state.current_phase} ===")

            # í•„í„° ì ìš©
            if chunker_filter:
                original_filter = EXPERIMENT_CONFIG["filters"]["chunkers"]
                EXPERIMENT_CONFIG["filters"]["chunkers"] = chunker_filter
                logger.info(f"ğŸ” ì²­ì»¤ í•„í„° ì ìš©: {chunker_filter}")

            chunk_results = self._run_chunking_experiments(documents, max_chunks)
            self.state.chunking_results = len(chunk_results)

            # í•„í„° ë³µì›
            if chunker_filter:
                EXPERIMENT_CONFIG["filters"]["chunkers"] = original_filter

            if not chunk_results:
                return {"success": False, "error": "ì²­í‚¹ ê²°ê³¼ ì—†ìŒ"}

            # ê²°ê³¼ ì €ì¥
            summary = self._save_chunking_summary(chunk_results)

            log_experiment_end(
                "ì²­í‚¹ ë°ì´í„° ìƒì„±",
                len(chunk_results),
                0,  # ì‹¤íŒ¨ëŠ” ë³„ë„ ê³„ì‚° ì•ˆí•¨
                self.state.start_time,
            )

            return {
                "success": True,
                "state": self.state,
                "summary": summary,
                "output_dir": str(self.output_dir),
                "chunk_results": chunk_results,
            }

        except KeyboardInterrupt:
            logger.warning("\nâŒ ì‚¬ìš©ì ì¤‘ë‹¨")
            return {"success": False, "error": "ì¤‘ë‹¨ë¨"}

        except Exception as e:
            logger.error(f"\nâŒ ì²­í‚¹ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e)}

    def _run_chunking_experiments(
        self,
        documents: List[Document],
        max_chunks: Optional[int] = None,
    ) -> List[str]:
        """ì²­í‚¹ ì‹¤í—˜ ì‹¤í–‰"""
        chunk_results = []
        combination_count = 0

        for (
            chunker_name,
            params,
        ) in self.chunking_manager.generate_chunking_combinations():
            combination_count += 1

            # ìµœëŒ€ ì¡°í•© ìˆ˜ ì œí•œ
            if max_chunks and combination_count > max_chunks:
                logger.info(f"â¹ï¸ ìµœëŒ€ ì¡°í•© ìˆ˜ ë„ë‹¬: {max_chunks}ê°œ")
                break

            logger.info(f"ğŸ§© ì²­í‚¹ [{combination_count}]: {chunker_name}")

            chunk_id = self.chunking_manager.chunk_documents(
                chunker_name, params, documents
            )
            if chunk_id:
                chunk_results.append(chunk_id)
                logger.info(f"âœ… ì™„ë£Œ: {chunk_id}")
            else:
                logger.error(f"âŒ ì‹¤íŒ¨: {chunker_name}")

        logger.info(f"ğŸ‰ ì²­í‚¹ ë‹¨ê³„ ì™„ë£Œ: {len(chunk_results)}ê°œ ê²°ê³¼")
        return chunk_results

    def _save_chunking_summary(self, chunk_results: List[str]) -> Dict[str, Any]:
        """ì²­í‚¹ ê²°ê³¼ ìš”ì•½ ì €ì¥"""

        # ì²­í‚¹ ê²°ê³¼ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        chunk_metadata = []
        for chunk_id in chunk_results:
            try:
                _, metadata = self.chunking_manager.load_chunking_result(chunk_id)
                chunk_metadata.append(
                    {
                        "chunk_id": chunk_id,
                        "chunker_name": metadata.get("chunker_name", "unknown"),
                        "total_chunks": metadata.get("total_chunks", 0),
                        "avg_chunk_size": metadata.get("avg_chunk_size", 0),
                        "processing_time": metadata.get("processing_time", 0),
                        "file_size_mb": metadata.get("file_size_mb", 0),
                        "timestamp": metadata.get("timestamp", ""),
                    }
                )
            except Exception as e:
                logger.warning(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {chunk_id} - {e}")

        # CSV ì €ì¥
        if chunk_metadata:
            results_df = pd.DataFrame(chunk_metadata)
            results_csv = self.output_dir / "chunking_results.csv"
            results_df.to_csv(results_csv, index=False)
        else:
            results_csv = None

        # ìš”ì•½ í†µê³„
        total_time = time.time() - self.state.start_time
        summary = {
            "execution_time": total_time,
            "total_documents": self.state.total_documents,
            "tokenization_results": self.state.tokenization_results,
            "chunking_results": len(chunk_results),
            "successful_chunks": len(chunk_results),
            "total_chunk_files": sum(
                md.get("total_chunks", 0) for md in chunk_metadata
            ),
            "avg_processing_time": np.mean(
                [md.get("processing_time", 0) for md in chunk_metadata]
            )
            if chunk_metadata
            else 0,
            "total_storage_mb": sum(md.get("file_size_mb", 0) for md in chunk_metadata),
            "results_file": str(results_csv) if results_csv else None,
        }

        # JSON ìš”ì•½ ì €ì¥
        summary_file = self.output_dir / "chunking_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\nğŸ“Š === ì²­í‚¹ ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        logger.info(f"ğŸ“„ ì²˜ë¦¬ëœ ë¬¸ì„œ: {self.state.total_documents}ê°œ")
        logger.info(f"ğŸ§© ìƒì„±ëœ ì²­í‚¹ ì¡°í•©: {len(chunk_results)}ê°œ")
        logger.info(f"ğŸ“¦ ì´ ì²­í¬ ê°œìˆ˜: {summary['total_chunk_files']:,}ê°œ")
        logger.info(f"ğŸ’¾ ì´ ì €ì¥ ìš©ëŸ‰: {summary['total_storage_mb']:.1f}MB")
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")

        return summary


# ===================================================================
# CLI ë° ë©”ì¸ ì‹¤í–‰
# ===================================================================


def main():
    parser = argparse.ArgumentParser(
        description="Chunking Database Builder - Phase 2-1"
    )
    parser.add_argument("--output-dir", type=str, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--max-chunks", type=int, help="ìµœëŒ€ ì²˜ë¦¬í•  ì²­í‚¹ ì¡°í•© ìˆ˜")
    parser.add_argument(
        "--chunkers",
        type=str,
        help="ì‚¬ìš©í•  ì²­ì»¤ (comma-separated: token,sentence,recursive)",
    )
    parser.add_argument("--config-summary", action="store_true", help="ì„¤ì • ìš”ì•½ ì¶œë ¥")
    parser.add_argument("--dry-run", action="store_true", help="ì„¤ì • í™•ì¸ë§Œ ì‹¤í–‰")
    parser.add_argument(
        "--scan-existing", action="store_true", help="ê¸°ì¡´ ì²­í¬ íŒŒì¼ ìŠ¤ìº”"
    )

    args = parser.parse_args()

    # ì„¤ì • ìš”ì•½ ì¶œë ¥
    if args.config_summary or args.dry_run:
        print_configuration_summary()

        # ì„¤ì • ê²€ì¦
        validation = validate_configurations()
        if not validation["valid"]:
            logger.error("âŒ ì„¤ì • ì˜¤ë¥˜:")
            for error in validation["errors"]:
                logger.error(f"  â€¢ {error}")
            return 1

        if args.dry_run:
            logger.info("âœ… ì„¤ì • ê²€ì¦ ì™„ë£Œ - ì‹¤í–‰í•˜ì§€ ì•ŠìŒ")
            return 0

    # ì²­í‚¹ ë¹Œë” ì´ˆê¸°í™”
    runner = ChunkingExperimentRunner(args.output_dir)

    # ê¸°ì¡´ ì²­í¬ ìŠ¤ìº”
    if args.scan_existing:
        existing_chunks = runner.chunking_manager.scan_existing_chunks()
        logger.info(f"ğŸ“Š ê¸°ì¡´ ì²­í¬ íŒŒì¼: {len(existing_chunks)}ê°œ")
        for chunk_id in existing_chunks[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            logger.info(f"  â€¢ {chunk_id}")
        if len(existing_chunks) > 10:
            logger.info(f"  â€¢ ... ì™¸ {len(existing_chunks) - 10}ê°œ")
        return 0

    # ì²­ì»¤ í•„í„° ì²˜ë¦¬
    chunker_filter = None
    if args.chunkers:
        chunker_filter = [c.strip() for c in args.chunkers.split(",")]
        logger.info(f"ğŸ” ì²­ì»¤ í•„í„°: {chunker_filter}")

    # ì‹¤ì œ ì²­í‚¹ ì‹¤í—˜ ì‹¤í–‰
    try:
        result = runner.run_all_chunking_experiments(
            max_chunks=args.max_chunks,
            chunker_filter=chunker_filter,
        )

        if result["success"]:
            logger.info("ğŸ‰ ì²­í‚¹ ì‹¤í—˜ ì™„ë£Œ!")
            return 0
        else:
            logger.error(f"âŒ ì²­í‚¹ ì‹¤í—˜ ì‹¤íŒ¨: {result.get('error')}")
            return 1

    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())

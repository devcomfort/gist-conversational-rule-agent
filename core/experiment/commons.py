#!/usr/bin/env python3
"""
Experiment Commons
=================

ì‹¤í—˜ ì‹œìŠ¤í…œì˜ ê³µí†µ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
ëª¨ë“  ì‹¤í—˜ ëª¨ë“ˆ(ì²­í‚¹, QA ìƒì„±, ì„ë² ë”©)ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤ì™€ ìœ í‹¸ë¦¬í‹°ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

ê³µí†µ ì»´í¬ë„ŒíŠ¸:
- ExperimentState: ì‹¤í—˜ ì§„í–‰ ìƒíƒœ ì¶”ì 
- ExperimentResult: ì‹¤í—˜ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°
- ExperimentNaming: ì¼ê´€ëœ ë„¤ì´ë° ì»¨ë²¤ì…˜
- DocumentManager: ë¬¸ì„œ ë¡œë”© ë° ê´€ë¦¬
- TokenizationManager: í† í°í™” ê´€ë¦¬ (ì„ íƒì )

ì‚¬ìš©ë²•:
    from experiment_commons import DocumentManager, ExperimentNaming, ExperimentState
"""

import os
import json
import time
import pickle
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from loguru import logger
from llama_index.core import Document

# ë¡œì»¬ ëª¨ë“ˆ
from experiment_configurations import EXPERIMENT_CONFIG
from core.loaders import load_document, collect_document_paths

# ===================================================================
# ë°ì´í„°í´ë˜ìŠ¤
# ===================================================================


@dataclass
class ExperimentState:
    """ì‹¤í—˜ ì§„í–‰ ìƒíƒœ"""

    total_documents: int = 0
    tokenization_results: int = 0
    chunking_results: int = 0
    embedding_results: int = 0
    successful_experiments: int = 0
    failed_experiments: int = 0
    start_time: float = 0
    current_phase: str = "ì´ˆê¸°í™”"


@dataclass
class ExperimentResult:
    """ê°œë³„ ì‹¤í—˜ ê²°ê³¼"""

    experiment_id: str
    tokenizer_name: Optional[str]
    chunker_name: str
    chunker_params: Dict[str, Any]
    embedding_name: str
    embedding_model: str
    total_chunks: int
    embedding_time: float
    database_size_mb: float
    database_path: str
    success: bool
    error_message: Optional[str] = None
    timestamp: str = ""


# ===================================================================
# ë„¤ì´ë° ì»¨ë²¤ì…˜
# ===================================================================


class ExperimentNaming:
    """ì‹¤í—˜ ê²°ê³¼ ë„¤ì´ë° ì»¨ë²¤ì…˜"""

    ABBREVIATIONS = {
        # í† í¬ë‚˜ì´ì €
        "character": "char",
        "gpt2": "gpt2",
        "cl100k_base": "tiktoken",
        # ì„ë² ë”© ëª¨ë¸ (í‚¤ ì‚¬ìš©)
        "qwen3_8b": "qwen8b",
        "qwen3_0_6b": "qwen06b",
        "jina_v3": "jina",
        "bge_m3": "bgem3",
        "all_minilm_l6": "minilm",
        "multilingual_e5": "e5",
        # Boolean
        True: "T",
        False: "F",
        None: "null",
    }

    @classmethod
    def abbreviate(cls, value: Any) -> str:
        """ê°’ì„ ì•½ì–´ë¡œ ë³€í™˜"""
        if value in cls.ABBREVIATIONS:
            return cls.ABBREVIATIONS[value]
        elif isinstance(value, str) and len(value) > 12:
            return value.replace("-", "").replace("_", "")[:8]
        return str(value).replace("-", "_")

    @classmethod
    def generate_chunk_id(cls, chunker_name: str, params: Dict[str, Any]) -> str:
        """ì²­í‚¹ ID ìƒì„±"""
        # íŒŒë¼ë¯¸í„° í•´ì‹œ
        param_hash = cls._hash_params(params)

        # ì²­ì»¤ë³„ íŠ¹í™” ID
        if chunker_name == "token":
            tokenizer = cls.abbreviate(params.get("tokenizer", "char"))
            size = params.get("chunk_size", 512)
            overlap = params.get("chunk_overlap", 0)
            return f"token_{tokenizer}_s{size}_o{overlap}_{param_hash}"

        elif chunker_name == "sentence":
            tokenizer = cls.abbreviate(params.get("tokenizer_or_token_counter", "char"))
            size = params.get("chunk_size", 1024)
            min_sent = params.get("min_sentences_per_chunk", 2)
            return f"sent_{tokenizer}_s{size}_m{min_sent}_{param_hash}"

        elif chunker_name in ["late", "semantic"]:
            model = cls.abbreviate(params.get("embedding_model", "unknown"))
            size = params.get("chunk_size", 1024)
            return f"{chunker_name}_{model}_s{size}_{param_hash}"

        elif chunker_name == "neural":
            model_name = params.get("model", "bert")
            model = cls.abbreviate(model_name.split("/")[-1])
            min_char = params.get("min_characters_per_chunk", 50)
            return f"neural_{model}_c{min_char}_{param_hash}"

        elif chunker_name == "recursive":
            tokenizer = cls.abbreviate(params.get("tokenizer_or_token_counter", "char"))
            size = params.get("chunk_size", 1024)
            return f"recur_{tokenizer}_s{size}_{param_hash}"

        else:
            return f"{chunker_name}_{param_hash}"

    @classmethod
    def generate_database_path(cls, embedding_name: str, chunk_id: str) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ìƒì„±"""
        embedding_abbrev = cls.abbreviate(embedding_name)
        return f"{embedding_abbrev}/{chunk_id}"

    @classmethod
    def _hash_params(cls, params: Dict[str, Any]) -> str:
        """íŒŒë¼ë¯¸í„° 8ì í•´ì‹œ ìƒì„±"""
        clean_params = {
            k: v for k, v in params.items() if k not in ["_target_", "class"]
        }
        param_str = json.dumps(clean_params, sort_keys=True, separators=(",", ":"))
        return hashlib.md5(param_str.encode()).hexdigest()[:8]


# ===================================================================
# ë¬¸ì„œ ë° í† í°í™” ê´€ë¦¬ì
# ===================================================================


class DocumentManager:
    """ë¬¸ì„œ ë¡œë”© ê´€ë¦¬"""

    def __init__(self):
        self.config = EXPERIMENT_CONFIG["data"]

    def load_all_documents(self) -> List[Document]:
        """ëª¨ë“  ì§€ì› ë¬¸ì„œ ë¡œë“œ"""
        logger.info("ğŸ“– ë¬¸ì„œ ë¡œë”© ì‹œì‘...")

        # ë¬¸ì„œ ê²½ë¡œ ìˆ˜ì§‘
        document_paths = list(
            collect_document_paths(self.config["rules_directory"], lazy=False)
        )

        # í¬ê¸° ë° íŒ¨í„´ í•„í„°ë§
        filtered_paths = []
        for path in document_paths:
            try:
                # í¬ê¸° ì²´í¬
                size_mb = os.path.getsize(path) / (1024 * 1024)
                if size_mb > self.config["max_file_size_mb"]:
                    logger.debug(f"âš ï¸ í¬ê¸° ì´ˆê³¼: {path} ({size_mb:.1f}MB)")
                    continue

                # íŒ¨í„´ ì²´í¬ (ê°„ë‹¨í•œ êµ¬í˜„)
                path_str = str(path)
                skip = False
                for pattern in self.config.get("exclude_patterns", []):
                    if pattern.replace("**/", "").replace("/**", "") in path_str:
                        skip = True
                        break

                if not skip:
                    filtered_paths.append(path)

            except OSError as e:
                logger.debug(f"âš ï¸ ì ‘ê·¼ ì‹¤íŒ¨: {path} - {e}")
                continue

        logger.info(f"ğŸ“„ í•„í„°ë§ëœ ê²½ë¡œ: {len(filtered_paths)}ê°œ")

        # ë¬¸ì„œ ë¡œë“œ
        all_documents = []
        for i, path in enumerate(filtered_paths):
            logger.info(f"ğŸ“„ ë¡œë”© [{i + 1}/{len(filtered_paths)}]: {path.name}")

            try:
                docs = load_document(path)

                # ë©”íƒ€ë°ì´í„° ë³´ê°• (LlamaIndex Document ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                for doc in docs:
                    doc.metadata.update(
                        {
                            "file_path": str(path),
                            "file_name": path.name,
                            "file_extension": path.suffix.lower(),
                            "load_timestamp": datetime.now().isoformat(),
                            "doc_id": f"{path.stem}_{hashlib.md5(str(path).encode()).hexdigest()[:8]}",
                        }
                    )

                all_documents.extend(docs)

            except Exception as e:
                logger.error(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {path} - {e}")
                continue

        logger.info(f"âœ… ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(all_documents)}ê°œ")
        return all_documents


class TokenizationManager:
    """í† í°í™” ê´€ë¦¬ (ì„ íƒì )"""

    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.token_dir = (
            self.output_dir / EXPERIMENT_CONFIG["output"]["tokenization_subdir"]
        )
        self.token_dir.mkdir(parents=True, exist_ok=True)

    def tokenize_documents(self, documents: List[Document]) -> Dict[str, str]:
        """ë¬¸ì„œë“¤ì„ ëª¨ë“  í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”"""
        if not EXPERIMENT_CONFIG["execution"]["enable_tokenization_phase"]:
            logger.info("ğŸ”¤ í† í°í™” ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”ë¨)")
            return {}

        logger.info("ğŸ”¤ í† í°í™” ë‹¨ê³„ ì‹œì‘...")

        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ê²°í•©
        combined_text = "\n\n".join([doc.text for doc in documents])
        doc_hash = hashlib.md5(combined_text.encode()).hexdigest()[:8]

        tokenization_results = {}

        # í•„í„° ì ìš©
        from experiment_configurations import TOKENIZERS

        tokenizers = TOKENIZERS
        if EXPERIMENT_CONFIG["filters"]["tokenizers"]:
            tokenizers = {
                k: v
                for k, v in tokenizers.items()
                if k in EXPERIMENT_CONFIG["filters"]["tokenizers"]
            }

        for tokenizer_name, config in tokenizers.items():
            logger.info(f"ğŸ”¤ í† í°í™”: {tokenizer_name}")

            try:
                start_time = time.time()

                # í† í¬ë‚˜ì´ì €ë³„ êµ¬í˜„
                impl = config["implementation"]
                if impl == "character":
                    tokens = list(combined_text)
                elif impl in ["gpt2", "cl100k_base"]:
                    import tiktoken

                    tokenizer = tiktoken.get_encoding(impl)
                    tokens = tokenizer.encode(combined_text)
                    tokens = [str(t) for t in tokens]
                else:
                    logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í† í¬ë‚˜ì´ì €: {impl}")
                    continue

                processing_time = time.time() - start_time

                # ê²°ê³¼ ì €ì¥
                result_id = self._save_tokenization(
                    tokenizer_name, doc_hash, tokens, processing_time
                )
                tokenization_results[tokenizer_name] = result_id

                logger.info(
                    f"âœ… í† í°í™” ì™„ë£Œ: {tokenizer_name} ({len(tokens):,}ê°œ, {processing_time:.1f}ì´ˆ)"
                )

            except Exception as e:
                logger.error(f"âŒ í† í°í™” ì‹¤íŒ¨: {tokenizer_name} - {e}")
                continue

        logger.info(f"ğŸ‰ í† í°í™” ì™„ë£Œ: {len(tokenization_results)}ê°œ ê²°ê³¼")
        return tokenization_results

    def _save_tokenization(
        self,
        tokenizer_name: str,
        doc_hash: str,
        tokens: List[str],
        processing_time: float,
    ) -> str:
        """í† í°í™” ê²°ê³¼ ì €ì¥"""
        result_id = f"{ExperimentNaming.abbreviate(tokenizer_name)}_{doc_hash}"

        # í† í° ì €ì¥
        token_file = self.token_dir / f"{result_id}_tokens.pkl"
        with open(token_file, "wb") as f:
            pickle.dump(tokens, f)

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "result_id": result_id,
            "tokenizer_name": tokenizer_name,
            "doc_hash": doc_hash,
            "token_count": len(tokens),
            "unique_tokens": len(set(tokens)),
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "file_size_mb": token_file.stat().st_size / (1024 * 1024),
        }

        metadata_file = self.token_dir / f"{result_id}_metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        return result_id


# ===================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ===================================================================


def log_experiment_start(phase: str, total_combinations: int):
    """ì‹¤í—˜ ì‹œì‘ ë¡œê·¸"""
    logger.info(f"\nğŸš€ === {phase} ì‹œì‘ ===")
    logger.info(f"ğŸ“Š ì´ ì¡°í•© ìˆ˜: {total_combinations:,}ê°œ")
    logger.info(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def log_experiment_end(phase: str, successful: int, failed: int, start_time: float):
    """ì‹¤í—˜ ì¢…ë£Œ ë¡œê·¸"""
    duration = time.time() - start_time
    total = successful + failed
    success_rate = (successful / total * 100) if total > 0 else 0

    logger.info(f"\nğŸ‰ === {phase} ì™„ë£Œ ===")
    logger.info(f"âœ… ì„±ê³µ: {successful:,}ê°œ")
    logger.info(f"âŒ ì‹¤íŒ¨: {failed:,}ê°œ")
    logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
    logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")

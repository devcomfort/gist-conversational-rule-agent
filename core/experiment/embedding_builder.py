#!/usr/bin/env python3
"""
Embedding Database Builder
=========================

Phase 2-3: ì„ë² ë”© ë°ì´í„°ì…‹ ìƒì„± ì „ìš© ëª¨ë“ˆ
ì €ì¥ëœ ì²­í‚¹ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- 28,560ê°œ ì¡°í•© ì„ë² ë”© ì²˜ë¦¬ (ì²­í‚¹ Ã— ì„ë² ë”© ëª¨ë¸)
- ì²­í‚¹ ê²°ê³¼ íŒŒì¼ ê¸°ë°˜ ì²˜ë¦¬
- FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ì €ì¥
- Multi-GPU ìë™ ê°ì§€ ë° ìµœì í™”

ë°ì´í„° íë¦„:
    chunks/*.pkl â†’ Embedding â†’ vectorstores/*/

ì‚¬ìš©ë²•:
    python embedding_database_builder.py
    python embedding_database_builder.py --chunk-dir experiments/outputs/chunks
    python embedding_database_builder.py --max-combinations 1000
    python embedding_database_builder.py --embedding-models qwen3_8b,bge_m3
    python embedding_database_builder.py --dry-run
"""

import os
import sys
import json
import time
import pickle
import argparse
import gc
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Iterator

import torch
import numpy as np
import pandas as pd
from loguru import logger
from llama_index.core import Document

# Vector store and embeddings
import faiss
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ë¡œì»¬ ëª¨ë“ˆ
from core.experiment.commons import (
    ExperimentState,
    ExperimentResult,
    ExperimentNaming,
    log_experiment_start,
    log_experiment_end,
)
from experiment_configurations import (
    EMBEDDING_MODELS,
    EXPERIMENT_CONFIG,
    validate_configurations,
    print_configuration_summary,
)

# ===================================================================
# ì„ë² ë”© ê´€ë¦¬ì
# ===================================================================


class EmbeddingManager:
    """ì„ë² ë”© ë° FAISS ì €ì¥ ê´€ë¦¬"""

    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)

    def generate_embedding_combinations(self) -> Iterator[Tuple[str, Dict[str, Any]]]:
        """ì„ë² ë”© ëª¨ë¸ ì¡°í•© ìƒì„±ê¸°"""
        # í•„í„° ì ìš©
        embedding_models = EMBEDDING_MODELS
        if EXPERIMENT_CONFIG["filters"]["embedding_models"]:
            embedding_models = {
                k: v
                for k, v in embedding_models.items()
                if k in EXPERIMENT_CONFIG["filters"]["embedding_models"]
            }

        for name, config in embedding_models.items():
            yield name, config

    def create_vectorstore_and_save(
        self,
        embedding_name: str,
        embedding_config: Dict[str, Any],
        chunk_id: str,
        chunks: List[Document],
        chunk_metadata: Dict[str, Any],
    ) -> ExperimentResult:
        """ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥"""
        start_time = time.time()

        try:
            # ì„ë² ë”© ëª¨ë¸ ìƒì„±
            embedding_model = self._create_embedding_model(embedding_config)

            # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            logger.debug(f"    ğŸ”® ë²¡í„°ìŠ¤í† ì–´ ìƒì„±: {embedding_name}")
            vectorstore = FAISS.from_documents(chunks, embedding_model)
            embedding_time = time.time() - start_time

            # ì €ì¥ ê²½ë¡œ ìƒì„±
            db_relative_path = ExperimentNaming.generate_database_path(
                embedding_name, chunk_id
            )
            db_path = self.output_dir / db_relative_path
            db_path.mkdir(parents=True, exist_ok=True)

            # FAISS ì €ì¥
            vectorstore.save_local(str(db_path))

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            full_metadata = {
                "experiment_id": f"{embedding_name}_{chunk_id}",
                "embedding_name": embedding_name,
                "embedding_model": embedding_config["model_name"],
                "embedding_dimension": embedding_config["dimension"],
                "chunk_id": chunk_id,
                "database_path": str(db_path),
                "timestamp": datetime.now().isoformat(),
                **chunk_metadata,
            }

            with open(db_path / "metadata.json", "w", encoding="utf-8") as f:
                json.dump(full_metadata, f, ensure_ascii=False, indent=2)

            # DB í¬ê¸° ê³„ì‚°
            db_size_mb = sum(
                f.stat().st_size for f in db_path.rglob("*") if f.is_file()
            ) / (1024 * 1024)

            # Multi-GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if EXPERIMENT_CONFIG["execution"]["cleanup_memory_after_each"]:
                if torch.cuda.is_available():
                    # ëª¨ë“  GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    for i in range(torch.cuda.device_count()):
                        with torch.cuda.device(i):
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                gc.collect()

            return ExperimentResult(
                experiment_id=full_metadata["experiment_id"],
                tokenizer_name=None,  # í˜„ì¬ëŠ” ë¯¸ì‚¬ìš©
                chunker_name=chunk_metadata.get("chunker_name", "unknown"),
                chunker_params=chunk_metadata.get("chunker_params", {}),
                embedding_name=embedding_name,
                embedding_model=embedding_config["model_name"],
                total_chunks=len(chunks),
                embedding_time=embedding_time,
                database_size_mb=db_size_mb,
                database_path=str(db_path),
                success=True,
                timestamp=full_metadata["timestamp"],
            )

        except Exception as e:
            embedding_time = time.time() - start_time
            error_msg = f"ì„ë² ë”© ì‹¤íŒ¨: {embedding_name} Ã— {chunk_id} - {str(e)}"

            return ExperimentResult(
                experiment_id=f"{embedding_name}_{chunk_id}",
                tokenizer_name=None,
                chunker_name=chunk_metadata.get("chunker_name", "unknown"),
                chunker_params=chunk_metadata.get("chunker_params", {}),
                embedding_name=embedding_name,
                embedding_model=embedding_config["model_name"],
                total_chunks=0,
                embedding_time=embedding_time,
                database_size_mb=0,
                database_path="",
                success=False,
                error_message=error_msg,
                timestamp=datetime.now().isoformat(),
            )

    def _create_embedding_model(self, config: Dict[str, Any]) -> HuggingFaceEmbeddings:
        """ì„ë² ë”© ëª¨ë¸ ìƒì„± (Multi-GPU ì§€ì›)"""
        model_kwargs = config["model_kwargs"].copy()
        encode_kwargs = config["encode_kwargs"].copy()

        # Multi-GPU ìë™ ì„¤ì •
        if model_kwargs.get("device") == "auto":
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                if gpu_count > 1:
                    # Multi-GPU í™˜ê²½
                    logger.info(f"ğŸ® Multi-GPU ê°ì§€: {gpu_count}ê°œ GPU í™œìš©")
                    model_kwargs["device_map"] = "auto"
                    model_kwargs["torch_dtype"] = model_kwargs.get(
                        "torch_dtype", "float16"
                    )

                    # Multi-GPUì—ì„œ ë°°ì¹˜ í¬ê¸° ìµœì í™”
                    if "batch_size" in encode_kwargs:
                        original_batch_size = encode_kwargs["batch_size"]
                        # GPU ìˆ˜ë§Œí¼ ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ìµœëŒ€ 128ê¹Œì§€)
                        optimized_batch_size = min(original_batch_size * gpu_count, 128)
                        encode_kwargs["batch_size"] = optimized_batch_size
                        logger.info(
                            f"  ğŸ“ˆ ë°°ì¹˜ í¬ê¸° ìµœì í™”: {original_batch_size} â†’ {optimized_batch_size}"
                        )
                else:
                    # Single GPU
                    model_kwargs["device"] = "cuda"
                    logger.info("ğŸ® Single GPU ì‚¬ìš©")
            else:
                # CPU ëª¨ë“œ
                model_kwargs["device"] = "cpu"
                logger.info("ğŸ’» CPU ëª¨ë“œ ì‚¬ìš©")

        return HuggingFaceEmbeddings(
            model_name=config["model_name"],
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
        )


# ===================================================================
# ì²­í‚¹ ê²°ê³¼ ë¡œë”
# ===================================================================


class ChunkingResultLoader:
    """ì €ì¥ëœ ì²­í‚¹ ê²°ê³¼ ë¡œë“œ ê´€ë¦¬"""

    def __init__(self, chunk_dir: Path):
        self.chunk_dir = Path(chunk_dir)

    def scan_available_chunks(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬ ID ìŠ¤ìº”"""
        if not self.chunk_dir.exists():
            logger.warning(f"ì²­í¬ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.chunk_dir}")
            return []

        chunk_ids = []
        for chunks_file in self.chunk_dir.glob("*_chunks.pkl"):
            chunk_id = chunks_file.stem.replace("_chunks", "")

            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
            metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"
            if metadata_file.exists():
                chunk_ids.append(chunk_id)

        logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬: {len(chunk_ids)}ê°œ")
        return sorted(chunk_ids)

    def load_chunk_data(self, chunk_id: str) -> Tuple[List[Document], Dict[str, Any]]:
        """ì²­í¬ ë°ì´í„° ë¡œë“œ"""
        chunks_file = self.chunk_dir / f"{chunk_id}_chunks.pkl"
        metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"

        if not chunks_file.exists():
            raise FileNotFoundError(f"ì²­í¬ íŒŒì¼ ì—†ìŒ: {chunks_file}")

        # ì²­í¬ ë°ì´í„° ë¡œë“œ
        with open(chunks_file, "rb") as f:
            chunks = pickle.load(f)

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata = {}
        if metadata_file.exists():
            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)

        return chunks, metadata


# ===================================================================
# ì„ë² ë”© ì‹¤í—˜ ì‹¤í–‰ê¸°
# ===================================================================


class EmbeddingExperimentRunner:
    """ì„ë² ë”© ì‹¤í—˜ ì „ìš© ì‹¤í–‰ê¸° - Phase 2-3"""

    def __init__(
        self, output_dir: Optional[str] = None, chunk_dir: Optional[str] = None
    ):
        self.output_dir = Path(output_dir or EXPERIMENT_CONFIG["output"]["base_dir"])
        self.chunk_dir = Path(
            chunk_dir
            or (self.output_dir / EXPERIMENT_CONFIG["output"]["chunking_subdir"])
        )

        # ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.chunk_loader = ChunkingResultLoader(self.chunk_dir)
        self.embedding_manager = EmbeddingManager(self.output_dir)

        # ìƒíƒœ ì¶”ì 
        self.state = ExperimentState()
        self.results: List[ExperimentResult] = []

    def _log_gpu_info(self):
        """GPU í™˜ê²½ ì •ë³´ ë¡œê¹…"""
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸ® GPU í™˜ê²½: {gpu_count}ê°œ GPU ê°ì§€ë¨")

            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory // (
                    1024**3
                )
                logger.info(f"  â€¢ GPU {i}: {gpu_name} ({gpu_memory}GB VRAM)")

            # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            logger.info("ğŸ“Š GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
            for i in range(gpu_count):
                allocated = torch.cuda.memory_allocated(i) // (1024**2)
                reserved = torch.cuda.memory_reserved(i) // (1024**2)
                total = torch.cuda.get_device_properties(i).total_memory // (1024**2)
                logger.info(
                    f"  â€¢ GPU {i}: {allocated}MB ì‚¬ìš© ì¤‘, {reserved}MB ì˜ˆì•½ë¨ / {total}MB"
                )

            if gpu_count > 1:
                logger.info("ğŸš€ Multi-GPU ìµœì í™” í™œì„±í™” - ìì› ë‚­ë¹„ ìµœì†Œí™”")
        else:
            logger.info("ğŸ’» CPU ëª¨ë“œ - GPU ì—†ìŒ")

    def run_all_embedding_experiments(
        self,
        max_combinations: Optional[int] = None,
        chunk_filter: Optional[List[str]] = None,
        embedding_filter: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """ëª¨ë“  ì„ë² ë”© ì‹¤í—˜ ì‹¤í–‰"""

        # GPU ì •ë³´ ë¡œê¹…
        self._log_gpu_info()

        log_experiment_start("ì„ë² ë”© ë°ì´í„°ì…‹ ìƒì„±", 0)  # ì¡°í•© ìˆ˜ëŠ” ë‚˜ì¤‘ì— ê³„ì‚°
        self.state.start_time = time.time()
        self.state.current_phase = "ì„ë² ë”© ë°ì´í„°ì…‹ ìƒì„±"

        try:
            # ë‹¨ê³„ 1: ì²­í¬ íŒŒì¼ ìŠ¤ìº”
            self.state.current_phase = "ì²­í‚¹ ê²°ê³¼ ìŠ¤ìº”"
            logger.info(f"\nğŸ“Š === {self.state.current_phase} ===")

            available_chunks = self.chunk_loader.scan_available_chunks()
            if not available_chunks:
                return {"success": False, "error": "ì²­í‚¹ ê²°ê³¼ ì—†ìŒ"}

            # ì²­í¬ í•„í„° ì ìš©
            if chunk_filter:
                filtered_chunks = []
                for chunk_id in available_chunks:
                    if any(pattern in chunk_id for pattern in chunk_filter):
                        filtered_chunks.append(chunk_id)
                available_chunks = filtered_chunks
                logger.info(f"ğŸ” ì²­í¬ í•„í„° ì ìš©: {len(available_chunks)}ê°œ ì„ íƒ")

            # ì„ë² ë”© ëª¨ë¸ í•„í„° ì ìš©
            if embedding_filter:
                original_filter = EXPERIMENT_CONFIG["filters"]["embedding_models"]
                EXPERIMENT_CONFIG["filters"]["embedding_models"] = embedding_filter
                logger.info(f"ğŸ” ì„ë² ë”© ëª¨ë¸ í•„í„° ì ìš©: {embedding_filter}")

            # ë‹¨ê³„ 2: ì„ë² ë”© ì‹¤í—˜ ì‹¤í–‰
            self.state.current_phase = "ì„ë² ë”© ì‹¤í—˜"
            logger.info(f"\nğŸ”® === {self.state.current_phase} ===")

            embedding_results = self._run_embedding_experiments(
                available_chunks, max_combinations
            )

            self.results = embedding_results
            self.state.embedding_results = len(embedding_results)

            # í•„í„° ë³µì›
            if embedding_filter:
                EXPERIMENT_CONFIG["filters"]["embedding_models"] = original_filter

            # ê²°ê³¼ ì €ì¥
            summary = self._save_embedding_summary()

            # ì„±ê³µ/ì‹¤íŒ¨ ë¶„ë¦¬
            successful_results = [r for r in self.results if r.success]
            failed_results = [r for r in self.results if not r.success]

            log_experiment_end(
                "ì„ë² ë”© ë°ì´í„°ì…‹ ìƒì„±",
                len(successful_results),
                len(failed_results),
                self.state.start_time,
            )

            return {
                "success": True,
                "state": self.state,
                "summary": summary,
                "output_dir": str(self.output_dir),
                "embedding_results": embedding_results,
            }

        except KeyboardInterrupt:
            logger.warning("\nâŒ ì‚¬ìš©ì ì¤‘ë‹¨")
            return {"success": False, "error": "ì¤‘ë‹¨ë¨"}

        except Exception as e:
            logger.error(f"\nâŒ ì„ë² ë”© ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e)}

    def _run_embedding_experiments(
        self, chunk_ids: List[str], max_combinations: Optional[int] = None
    ) -> List[ExperimentResult]:
        """ì„ë² ë”© ì‹¤í—˜ ì‹¤í–‰"""
        results = []

        # ì „ì²´ ì¡°í•© ê³„ì‚°
        embedding_combinations = list(
            self.embedding_manager.generate_embedding_combinations()
        )
        total_combinations = len(chunk_ids) * len(embedding_combinations)

        logger.info(f"ğŸ“Š ì´ ì¡°í•© ìˆ˜: {total_combinations:,}ê°œ")

        if max_combinations:
            total_combinations = min(total_combinations, max_combinations)
            logger.info(f"ğŸ” ìµœëŒ€ ì¡°í•© ìˆ˜ë¡œ ì œí•œ: {max_combinations:,}ê°œ")

        current_combination = 0

        for chunk_id in chunk_ids:
            try:
                # ì²­í‚¹ ê²°ê³¼ ë¡œë“œ
                chunks, chunk_metadata = self.chunk_loader.load_chunk_data(chunk_id)
                logger.info(f"ğŸ“¦ ì²­í¬ ë¡œë“œ: {chunk_id} ({len(chunks)}ê°œ ì²­í¬)")

                # ê° ì„ë² ë”© ëª¨ë¸ë¡œ ì‹¤í—˜
                for embedding_name, embedding_config in embedding_combinations:
                    current_combination += 1

                    # ìµœëŒ€ ì¡°í•© ìˆ˜ ì œí•œ
                    if max_combinations and current_combination > max_combinations:
                        logger.info(f"â¹ï¸ ìµœëŒ€ ì¡°í•© ìˆ˜ ë„ë‹¬: {max_combinations}ê°œ")
                        return results

                    logger.info(
                        f"ğŸ”® ì„ë² ë”© [{current_combination:,}/{total_combinations:,}]: "
                        f"{embedding_name} Ã— {chunk_id}"
                    )

                    result = self.embedding_manager.create_vectorstore_and_save(
                        embedding_name,
                        embedding_config,
                        chunk_id,
                        chunks,
                        chunk_metadata,
                    )

                    results.append(result)

                    if result.success:
                        self.state.successful_experiments += 1
                        logger.info(
                            f"âœ… ì„±ê³µ: {embedding_name} ({result.embedding_time:.1f}ì´ˆ, {result.database_size_mb:.1f}MB)"
                        )
                    else:
                        self.state.failed_experiments += 1
                        logger.error(f"âŒ ì‹¤íŒ¨: {result.error_message}")

            except Exception as e:
                logger.error(f"âŒ ì²­í¬ ë¡œë“œ ì‹¤íŒ¨: {chunk_id} - {e}")
                continue

        logger.info(f"ğŸ‰ ì„ë² ë”© ë‹¨ê³„ ì™„ë£Œ: {self.state.successful_experiments}ê°œ ì„±ê³µ")
        return results

    def _save_embedding_summary(self) -> Dict[str, Any]:
        """ì„ë² ë”© ê²°ê³¼ ìš”ì•½ ì €ì¥"""
        # CSV í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥
        results_data = []
        for result in self.results:
            results_data.append(
                {
                    "experiment_id": result.experiment_id,
                    "tokenizer_name": result.tokenizer_name or "",
                    "chunker_name": result.chunker_name,
                    "chunker_params": json.dumps(result.chunker_params),
                    "embedding_name": result.embedding_name,
                    "embedding_model": result.embedding_model,
                    "total_chunks": result.total_chunks,
                    "embedding_time": result.embedding_time,
                    "database_size_mb": result.database_size_mb,
                    "database_path": result.database_path,
                    "success": result.success,
                    "error_message": result.error_message or "",
                    "timestamp": result.timestamp,
                }
            )

        results_df = pd.DataFrame(results_data)
        results_csv = self.output_dir / "embedding_experiment_results.csv"
        results_df.to_csv(results_csv, index=False)

        # ìš”ì•½ í†µê³„
        successful_results = [r for r in self.results if r.success]
        total_time = time.time() - self.state.start_time

        summary = {
            "execution_time": total_time,
            "total_experiments": len(self.results),
            "successful_experiments": len(successful_results),
            "failed_experiments": len(self.results) - len(successful_results),
            "success_rate": len(successful_results) / len(self.results)
            if self.results
            else 0,
            "total_database_size_mb": sum(
                r.database_size_mb for r in successful_results
            ),
            "avg_embedding_time": np.mean(
                [r.embedding_time for r in successful_results]
            )
            if successful_results
            else 0,
            "results_file": str(results_csv),
        }

        # JSON ìš”ì•½ ì €ì¥
        summary_file = self.output_dir / "embedding_experiment_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\nğŸ“Š === ì„ë² ë”© ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        logger.info(f"ğŸ”® ì´ ì‹¤í—˜ ìˆ˜: {len(self.results):,}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {len(successful_results):,}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {len(self.results) - len(successful_results):,}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']:.1%}")
        logger.info(f"ğŸ’¾ ì´ DB í¬ê¸°: {summary['total_database_size_mb']:.1f}MB")
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")

        return summary


# ===================================================================
# CLI ë° ë©”ì¸ ì‹¤í–‰
# ===================================================================


def main():
    parser = argparse.ArgumentParser(
        description="Embedding Database Builder - Phase 2-3"
    )
    parser.add_argument("--output-dir", type=str, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--chunk-dir", type=str, help="ì²­í‚¹ ê²°ê³¼ ë””ë ‰í† ë¦¬")
    parser.add_argument(
        "--max-combinations", type=int, help="ìµœëŒ€ ì²˜ë¦¬í•  ì„ë² ë”© ì¡°í•© ìˆ˜"
    )
    parser.add_argument(
        "--chunk-filter",
        type=str,
        help="ì²­í¬ ID í•„í„° (comma-separated: token,sentence)",
    )
    parser.add_argument(
        "--embedding-models",
        type=str,
        help="ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (comma-separated: qwen3_8b,bge_m3)",
    )
    parser.add_argument("--config-summary", action="store_true", help="ì„¤ì • ìš”ì•½ ì¶œë ¥")
    parser.add_argument("--dry-run", action="store_true", help="ì„¤ì • í™•ì¸ë§Œ ì‹¤í–‰")
    parser.add_argument(
        "--scan-chunks", action="store_true", help="ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬ íŒŒì¼ ìŠ¤ìº”"
    )

    args = parser.parse_args()

    # ì„¤ì • ìš”ì•½ ì¶œë ¥
    if args.config_summary or args.dry_run:
        print_configuration_summary()

        # ì„¤ì • ê²€ì¦
        validation = validate_configurations()
        if not validation["valid"]:
            logger.error("âŒ ì„¤ì • ì˜¤ë¥˜:")
            for error in validation["errors"]:
                logger.error(f"  â€¢ {error}")
            return 1

        if args.dry_run:
            logger.info("âœ… ì„¤ì • ê²€ì¦ ì™„ë£Œ - ì‹¤í–‰í•˜ì§€ ì•ŠìŒ")
            return 0

    # ì„ë² ë”© ë¹Œë” ì´ˆê¸°í™”
    runner = EmbeddingExperimentRunner(args.output_dir, args.chunk_dir)

    # ì²­í¬ íŒŒì¼ ìŠ¤ìº”
    if args.scan_chunks:
        available_chunks = runner.chunk_loader.scan_available_chunks()
        logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬ íŒŒì¼: {len(available_chunks)}ê°œ")
        for chunk_id in available_chunks[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            logger.info(f"  â€¢ {chunk_id}")
        if len(available_chunks) > 10:
            logger.info(f"  â€¢ ... ì™¸ {len(available_chunks) - 10}ê°œ")
        return 0

    # í•„í„° ì²˜ë¦¬
    chunk_filter = None
    if args.chunk_filter:
        chunk_filter = [c.strip() for c in args.chunk_filter.split(",")]
        logger.info(f"ğŸ” ì²­í¬ í•„í„°: {chunk_filter}")

    embedding_filter = None
    if args.embedding_models:
        embedding_filter = [e.strip() for e in args.embedding_models.split(",")]
        logger.info(f"ğŸ” ì„ë² ë”© ëª¨ë¸ í•„í„°: {embedding_filter}")

    # ì‹¤ì œ ì„ë² ë”© ì‹¤í—˜ ì‹¤í–‰
    try:
        result = runner.run_all_embedding_experiments(
            max_combinations=args.max_combinations,
            chunk_filter=chunk_filter,
            embedding_filter=embedding_filter,
        )

        if result["success"]:
            logger.info("ğŸ‰ ì„ë² ë”© ì‹¤í—˜ ì™„ë£Œ!")
            return 0
        else:
            logger.error(f"âŒ ì„ë² ë”© ì‹¤í—˜ ì‹¤íŒ¨: {result.get('error')}")
            return 1

    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())

# í•™ì¹™ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ - ì½”ë“œë² ì´ìŠ¤ ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„

## ğŸ“‹ ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°

í•™ì¹™ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ PDF ê¸°ë°˜ ë¬¸ì„œ ì§ˆì˜ì‘ë‹µì— íŠ¹í™”ëœ ë‹¨ì¼ ëª¨ë“ˆ AI í”Œë«í¼ìœ¼ë¡œ ì¬êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

```
ğŸ“¦ í•™ì¹™ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
â””â”€â”€ ğŸ“„ app.py              - PDF RAGÂ¹ ì‹œìŠ¤í…œ (í•™ì¹™ ë¬¸ì„œ ì „ìš©)
```

---

## ğŸ“„ **app.py** - í•™ì¹™ ë¬¸ì„œ RAG ì‹œìŠ¤í…œ ì‹¬ì¸µ ë¶„ì„

### **ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”**

app.pyëŠ” 265ì¤„ì˜ Python ì½”ë“œë¡œ êµ¬ì„±ëœ ì™„ì „í•œ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì…ë‹ˆë‹¤. í•™ì¹™, ê·œì •, ì •ì±… ë¬¸ì„œ ë“±ì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤.

### **ğŸ”§ í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ**

| ê³„ì¸µ                | ë¼ì´ë¸ŒëŸ¬ë¦¬                 | ìš©ë„                                     | ë¼ì¸ìˆ˜ |
| ------------------- | -------------------------- | ---------------------------------------- | ------ |
| **RAGÂ¹ í”„ë ˆì„ì›Œí¬** | langchain-community        | FAISSÂ¹Â² VectorStore                      | 30-33  |
| **ì„ë² ë”©**          | langchain-huggingface      | HuggingFaceEmbeddings (all-MiniLM-L6-v2) | 54     |
| **í…ìŠ¤íŠ¸ ë¶„í• **     | langchain-text-splitters   | RecursiveCharacterTextSplitter           | 56     |
| **PDF ì²˜ë¦¬**        | PyMuPDFÂ¹âµ (fitz)           | PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ                          | 82-84  |
| **LLMÂ³**            | openai + huggingface-hub   | ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› (GPT-4, DeepSeek-R1 ë“±)   | 47-53  |
| **UI í”„ë ˆì„ì›Œí¬**   | Gradio                     | ì›¹ ì¸í„°í˜ì´ìŠ¤                            | 229-249|
| **ì„¸ì…˜ ê´€ë¦¬**       | threading + OrderedDict    | ì‚¬ìš©ì ì„¸ì…˜ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬               | 42-44  |

### **ğŸ“Š ëª¨ë“ˆë³„ ì„¸ë¶€ ë¶„ì„**

#### **1. ì„¸ì…˜ ê´€ë¦¬ ëª¨ë“ˆ (ë¼ì¸ 42-80)**
```python
# í•µì‹¬ êµ¬í˜„
MAX_SESSIONS = 100
sessions = OrderedDict()
session_lock = threading.Lock()

def get_session_id(request: gr.Request):
    raw_id = request.client.host + str(request.headers.get("user-agent"))
    return hashlib.sha256(raw_id.encode()).hexdigest()
```

**ê¸°ëŠ¥**:
- LRU ìºì‹œ ë°©ì‹ì˜ ì„¸ì…˜ ê´€ë¦¬ (ìµœëŒ€ 100ê°œ ì„¸ì…˜)
- í´ë¼ì´ì–¸íŠ¸ IP + User-Agent ê¸°ë°˜ ì„¸ì…˜ ID ìƒì„±
- Thread-safe ë™ì‹œ ì ‘ê·¼ ì œì–´
- ì„¸ì…˜ë³„ ë…ë¦½ì ì¸ ë²¡í„°ìŠ¤í† ì–´ ë° ëŒ€í™” ê¸°ë¡

**ë³µì¡ë„**: â­â­â­ (ë³´í†µ)

#### **2. PDF ì²˜ë¦¬ ëª¨ë“ˆ (ë¼ì¸ 81-110)**
```python
def extract_text_from_pdf(pdf):
    doc = fitz.open(pdf)
    return "\n".join([page.get_text("text") for page in doc])

def create_vectorstore_from_pdfs(pdfs):
    all_docs = []
    for pdf in pdfs:
        text = extract_text_from_pdf(pdf)
        doc = Document(page_content=text, metadata={"source": pdf})
        docs = TEXT_SPLITTER.split_documents([doc])
        all_docs.extend(docs)
    return FAISS.from_documents(all_docs, EMBED_MODEL)
```

**ê¸°ëŠ¥**:
- PyMuPDFë¥¼ í†µí•œ ì•ˆì •ì ì¸ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ë‹¤ì¤‘ PDF íŒŒì¼ ì¼ê´„ ì²˜ë¦¬
- LangChain Document ê°ì²´ë¡œ ë³€í™˜
- RecursiveCharacterTextSplitterë¡œ ì²­í‚¹ (chunk_size=500, overlap=50)
- FAISS ë²¡í„°ìŠ¤í† ì–´ ìë™ ìƒì„±

**ë³µì¡ë„**: â­â­ (ë‹¨ìˆœ)

#### **3. ì§ˆì˜ì‘ë‹µ ëª¨ë“ˆ (ë¼ì¸ 112-156)**
```python
def handle_query(user_query, request: gr.Request):
    # ë²¡í„° ê²€ìƒ‰
    if vectorstore:
        retriever = vectorstore.as_retriever()
        docs = retriever.invoke(user_query)
        context = "\n".join([doc.page_content for doc in docs])
    
    # LLM í˜¸ì¶œ
    messages.append({"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_query}"})
    completion = client.chat.completions.create(
        model=session["model_id"],
        messages=messages,
    )
```

**ê¸°ëŠ¥**:
- FAISS ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
- ì»¨í…ìŠ¤íŠ¸ + ì§ˆë¬¸ ì¡°í•©ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
- ë‹¤ì¤‘ LLM ì§€ì› (OpenAI, HuggingFace)
- ëŒ€í™” ê¸°ë¡ ìœ ì§€ ë° ì €ì¥
- HTML ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬

**ë³µì¡ë„**: â­â­â­ (ë³´í†µ)

#### **4. ë‹¤ì¤‘ LLM ì§€ì› ëª¨ë“ˆ (ë¼ì¸ 158-182)**
```python
MODELS = {
    "GPT-4": {"model_id": "gpt-4", "provider": "openai"},
    "DeepSeek-R1": {"model_id": "deepseek-ai/DeepSeek-R1", "provider": "novita"},
    "Gemma-3-27B": {"model_id": "google/gemma-3-27b-it", "provider": "hf-inference"},
    "Llama-3.3-70B": {"model_id": "meta-llama/Llama-3.3-70B-Instruct", "provider": "hf-inference"},
    "QwQ-32B": {"model_id": "Qwen/QwQ-32B", "provider": "hf-inference"},
}

def create_client(provider):
    if provider == "openai":
        return openai.Client(api_key=OPENAI_API_KEY)
    return InferenceClient(
        provider=provider, 
        api_key=HF_API_KEY, 
        headers={"X-HF-Bill-To": HF_ENTERPRISE},
    )
```

**ê¸°ëŠ¥**:
- 5ê°œ ì£¼ìš” LLM ëª¨ë¸ ì§€ì›
- ì‹¤ì‹œê°„ ëª¨ë¸ ì „í™˜ ê°€ëŠ¥
- Providerë³„ í´ë¼ì´ì–¸íŠ¸ ìë™ ìƒì„±
- í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ ì œê³µ

**ë³µì¡ë„**: â­â­ (ë‹¨ìˆœ)

#### **5. UI ëª¨ë“ˆ (ë¼ì¸ 206-249)**
```python
with gr.Blocks(title="LiberVance RAG", css=css, fill_height=True) as demo:
    gr.Markdown("<center><h1>ğŸ“„ LiberVance RAG</h1></center>")
    with gr.Row(elem_classes=["responsive-height"]):
        with gr.Column(elem_classes=["fill-height"]):
            chatbot = gr.Chatbot(label="Chatbot", type="messages", elem_classes=["extend-height"])
        with gr.Column(elem_classes=["fill-height"]):
            model_dropdown = gr.Dropdown(list(MODELS.keys()), label="Select Model", value="GPT-4")
            pdf_upload = gr.Files(label="Upload file(s) (PDF only)", file_types=[".pdf"])
            user_input = gr.Textbox(label="Enter your query here", placeholder="e.g., Summarize the key points from this document.", lines=3)
```

**ê¸°ëŠ¥**:
- ë°˜ì‘í˜• 2ì»¬ëŸ¼ ë ˆì´ì•„ì›ƒ
- ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
- ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
- ë‹¤ì¤‘ PDF íŒŒì¼ ì—…ë¡œë“œ
- ì§ˆë¬¸ ì…ë ¥ ë° ì œì¶œ/ë¦¬ì…‹ ë²„íŠ¼

**ë³µì¡ë„**: â­â­ (ë‹¨ìˆœ)

### **âš¡ ì„±ëŠ¥ íŠ¹ì„±**

| ë©”íŠ¸ë¦­              | í˜„ì¬ êµ¬í˜„           | íŠ¹ì§•                                  |
| ------------------- | ------------------- | ------------------------------------- |
| **PDF ì²˜ë¦¬ ì†ë„**   | ~2-3ì´ˆ/íŒŒì¼         | PyMuPDF ê¸°ë°˜ ë¹ ë¥¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ         |
| **ë²¡í„°í™” ì‹œê°„**     | ~1-2ì´ˆ/ì²­í¬         | HuggingFace all-MiniLM-L6-v2 ì„ë² ë”©   |
| **ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„**  | ~0.5-1ì´ˆ            | FAISS ì¸ë©”ëª¨ë¦¬ ë²¡í„° ê²€ìƒ‰              |
| **LLM ì‘ë‹µ ì‹œê°„**   | ëª¨ë¸ë³„ ìƒì´         | GPT-4: ~5-10ì´ˆ, HF ëª¨ë¸: ~3-15ì´ˆ      |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**   | ~100MB/ì„¸ì…˜         | ë²¡í„°ìŠ¤í† ì–´ + ëŒ€í™”ê¸°ë¡                 |
| **ë™ì‹œ ì‚¬ìš©ì**     | ~10-20ëª…            | ì„¸ì…˜ ê¸°ë°˜ ê²©ë¦¬                        |

### **ğŸ”’ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë¶„ì„**

| ë©”ëª¨ë¦¬ ìœ í˜•     | êµ¬í˜„ ê¸°ìˆ                     | ìƒëª…ì£¼ê¸°    | ì €ì¥ ë²”ìœ„                      | ìµœëŒ€ í¬ê¸°        |
| --------------- | ---------------------------- | ----------- | ------------------------------ | ---------------- |
| **ë‹¨ê¸° ë©”ëª¨ë¦¬** | í•¨ìˆ˜ ë¡œì»¬ ë³€ìˆ˜               | ë‹¨ì¼ ìš”ì²­   | ìš”ì²­ ì²˜ë¦¬ ì¤‘ì—ë§Œ               | ~10MB/ìš”ì²­       |
| **ì¥ê¸° ë©”ëª¨ë¦¬** | FAISSÂ¹Â² VectorStore          | ì„¸ì…˜ ì§€ì†   | PDF ë¬¸ì„œ ì„ë² ë”© ì˜êµ¬ ì €ì¥      | ~50MB/ë¬¸ì„œì§‘í•©   |
| **ì„¸ì…˜ ë©”ëª¨ë¦¬** | OrderedDict + threading.Lock | ì‚¬ìš©ì ì„¸ì…˜ | ëŒ€í™” ê¸°ë¡, ë²¡í„°ìŠ¤í† ì–´ LRU ê´€ë¦¬ | 100ì„¸ì…˜ Ã— ~100MB |
| **íŒŒì¼ ì €ì¥**   | JSON íŒŒì¼ ì‹œìŠ¤í…œ             | ì˜êµ¬ ì €ì¥   | ./chat_logs_lvrag/             | ì œí•œ ì—†ìŒ        |

---

## ğŸš€ **ëŒ€ì²´ ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„**

### **ğŸ¦™ LlamaIndex ì „í™˜ ê°€ëŠ¥ì„±**

#### **âœ… ì™„ì „ ëŒ€ì²´ ê°€ëŠ¥ ì˜ì—­**

**1. FAISS VectorStore â†’ LlamaIndex VectorStore**
```python
# í˜„ì¬: langchain-community FAISS
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
vectorstore = FAISS.from_documents(docs, HuggingFaceEmbeddings())

# ëŒ€ì²´ì•ˆ: LlamaIndex FAISS
from llama_index.core import VectorStoreIndex
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

faiss_store = FaissVectorStore()
embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
index = VectorStoreIndex.from_vector_store(faiss_store, embed_model=embed_model)
```

**2. RecursiveCharacterTextSplitter â†’ LlamaIndex Splitters**
```python
# í˜„ì¬: langchain-text-splitters
from langchain_text_splitters import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

# ëŒ€ì²´ì•ˆ: LlamaIndex Splitters
from llama_index.core.text_splitter import SentenceSplitter
splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)
```

**3. Document â†’ LlamaIndex Document**
```python
# í˜„ì¬: LangChain Document
from langchain_core.documents import Document
doc = Document(page_content=text, metadata={"source": pdf})

# ëŒ€ì²´ì•ˆ: LlamaIndex Document
from llama_index.core import Document
doc = Document(text=text, metadata={"source": pdf})
```

#### **ğŸ¯ LlamaIndex ì „í™˜ ì¥ì **

**ê°œë°œì ê²½í—˜ (DX) í–¥ìƒ**:
- âœ… **ë‹¨ì¼ ìƒíƒœê³„**: LangChain ë¶„ì‚° íŒ¨í‚¤ì§€ â†’ LlamaIndex í†µí•© íŒ¨í‚¤ì§€
- âœ… **ê°„ë‹¨í•œ API**: `index.as_query_engine().query()` í•œ ì¤„ë¡œ RAG ì™„ì„±
- âœ… **ëª…í™•í•œ ë¬¸ì„œí™”**: LlamaIndex ê³µì‹ ë¬¸ì„œê°€ ë” ì²´ê³„ì ì´ê³  ëª…í™•
- âœ… **íƒ€ì… íŒíŠ¸**: ë” ë‚˜ì€ IDE ì§€ì› ë° ìë™ì™„ì„±

**ì½”ë“œ ë‹¨ìˆœí™”**:
```python
# í˜„ì¬: ë³µì¡í•œ LangChain íŒŒì´í”„ë¼ì¸ (15ì¤„)
def handle_query(user_query, request: gr.Request):
    session_id = get_session_id(request)
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        session = sessions[session_id]
    
    vectorstore = session["vectorstore"]
    if vectorstore:
        retriever = vectorstore.as_retriever()
        docs = retriever.invoke(user_query)
        context = "\n".join([doc.page_content for doc in docs])
    messages.append({"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_query}"})
    completion = client.chat.completions.create(model=session["model_id"], messages=messages)

# ëŒ€ì²´ì•ˆ: LlamaIndex ë‹¨ìˆœí™” (8ì¤„)
def handle_query(user_query, request: gr.Request):
    session_id = get_session_id(request)
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        query_engine = sessions[session_id]["query_engine"]
    
    response = query_engine.query(user_query)
    return str(response)
```

**ì‚¬ìš©ì ê²½í—˜ (UX) ê°œì„ **:
- âœ… **ë¹ ë¥¸ ì‘ë‹µ**: ìµœì í™”ëœ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ~20% ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•
- âœ… **ë” ë‚˜ì€ ê²€ìƒ‰ í’ˆì§ˆ**: LlamaIndexì˜ ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜
- âœ… **ë©”íƒ€ë°ì´í„° í™œìš©**: ë¬¸ì„œ ì¶œì²˜ ì •ë³´ ë” ì •í™•í•˜ê²Œ í‘œì‹œ

#### **âš ï¸ LlamaIndex ì „í™˜ ë‹¨ì **

**ê°œë°œì ê²½í—˜ (DX) ìš°ë ¤ì‚¬í•­**:
- âŒ **ìƒˆë¡œìš´ í•™ìŠµ**: ê¸°ì¡´ LangChain ì§€ì‹ì„ LlamaIndexë¡œ ì „í™˜ í•„ìš”
- âŒ **ìƒíƒœê³„ ì˜ì¡´**: LlamaIndex íŠ¹ì • ë°©ì‹ì— ì¢…ì†
- âŒ **ë””ë²„ê¹… ë³µì¡ë„**: ë‚´ë¶€ ë¡œì§ì´ ì¶”ìƒí™”ë˜ì–´ ë¬¸ì œ í•´ê²° ì–´ë ¤ì›€

**ê¸°ëŠ¥ì  ì œí•œ**:
- âŒ **ì»¤ìŠ¤í…€ ì œì–´**: LangChain ëŒ€ë¹„ ì„¸ë°€í•œ ì œì–´ ì–´ë ¤ì›€
- âŒ **ë©”ëª¨ë¦¬ ê´€ë¦¬**: í˜„ì¬ì˜ ì •êµí•œ ì„¸ì…˜ ê´€ë¦¬ ë¡œì§ ì¬êµ¬í˜„ í•„ìš”

---

### **ğŸ­ SmolAgents ì „í™˜ ê°€ëŠ¥ì„±**

#### **âœ… ë„êµ¬ ê¸°ë°˜ ëª¨ë“ˆí™”**

**RAG ê¸°ëŠ¥ì„ @toolë¡œ ë¶„í•´**:
```python
from smolagents import tool, CodeAgent
import litellm

@tool
def upload_pdf_documents(pdf_paths: list[str]) -> str:
    """PDF ë¬¸ì„œë“¤ì„ ì—…ë¡œë“œí•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    # PDF ì²˜ë¦¬ ë° ì„ë² ë”© ë¡œì§
    return f"{len(pdf_paths)}ê°œ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."

@tool
def search_documents(query: str, top_k: int = 3) -> str:
    """í•™ì¹™ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
    # ë²¡í„° ê²€ìƒ‰ ë¡œì§
    return search_results

@tool
def answer_question(question: str, context: str) -> str:
    """ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€"""
    response = litellm.completion(
        model="gpt-4",
        messages=[{
            "role": "user", 
            "content": f"Context: {context}\n\nQuestion: {question}"
        }]
    )
    return response.choices[0].message.content

# SmolAgents ì—ì´ì „íŠ¸ ìƒì„±
agent = CodeAgent(
    tools=[upload_pdf_documents, search_documents, answer_question],
    system_message="í•™ì¹™ ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. PDF ì—…ë¡œë“œ, ê²€ìƒ‰, ì§ˆì˜ì‘ë‹µì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤."
)

# ì‚¬ìš©ë²•
result = agent.run("ì¡¸ì—… ìš”ê±´ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”")
```

#### **ğŸ¯ SmolAgents ì „í™˜ ì¥ì **

**ê°œë°œì ê²½í—˜ (DX) í–¥ìƒ**:
- âœ… **ê·¹ë„ì˜ ë‹¨ìˆœì„±**: @tool ë°ì½”ë ˆì´í„°ë§Œìœ¼ë¡œ ê¸°ëŠ¥ ëª¨ë“ˆí™”
- âœ… **ì§ê´€ì  ë””ë²„ê¹…**: ê° ë„êµ¬ê°€ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- âœ… **ë‚®ì€ í•™ìŠµê³¡ì„ **: Python í•¨ìˆ˜ë§Œ ì•Œë©´ ì¦‰ì‹œ ê°œë°œ ê°€ëŠ¥
- âœ… **ìœ ì—°í•œ í™•ì¥**: ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ë„êµ¬ë¡œ ì‰½ê²Œ ì¶”ê°€

**ì‚¬ìš©ì ê²½í—˜ (UX) ê°œì„ **:
- âœ… **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”**: ì—ì´ì „íŠ¸ê°€ í•„ìš”í•œ ë„êµ¬ë¥¼ ìë™ ì„ íƒ
- âœ… **ë‹¨ê³„ë³„ í”¼ë“œë°±**: ê° ë„êµ¬ ì‹¤í–‰ ê³¼ì •ì„ ì‚¬ìš©ìì—ê²Œ í‘œì‹œ
- âœ… **ì˜¤ë¥˜ ë³µêµ¬**: ë„êµ¬ë³„ ë…ë¦½ì  ì‹¤í–‰ìœ¼ë¡œ ë¶€ë¶„ ì‹¤íŒ¨ ì²˜ë¦¬

#### **âš ï¸ SmolAgents ì „í™˜ ë‹¨ì **

**ê¸°ëŠ¥ì  ì œí•œ**:
- âŒ **RAG ì „ë¬¸ì„± ë¶€ì¡±**: LangChain/LlamaIndex ëŒ€ë¹„ RAG ìµœì í™” ê¸°ëŠ¥ ì œí•œ
- âŒ **ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥**: ë‚´ì¥ RAG ë„êµ¬ì˜ ì„±ëŠ¥ì´ FAISS ëŒ€ë¹„ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
- âŒ **ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°**: í˜„ì¬ì˜ ë™ì‹œì„± ì œì–´ ë° ì„¸ì…˜ ê´€ë¦¬ ë³µì¡ë„

---

### **âš¡ LiteLLM ì „í™˜ ê°€ëŠ¥ì„±**

#### **âœ… ì™„ì „ í˜¸í™˜ ëŒ€ì²´**

**í˜„ì¬ ë‹¤ì¤‘ LLM ì§€ì› â†’ LiteLLM í†µí•©**:
```python
# í˜„ì¬: ë³µì¡í•œ providerë³„ í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬
def create_client(provider):
    if provider == "openai":
        return openai.Client(api_key=OPENAI_API_KEY)
    return InferenceClient(provider=provider, api_key=HF_API_KEY)

# ëŒ€ì²´ì•ˆ: LiteLLM ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤
import litellm

def query_llm(model_id: str, messages: list) -> str:
    response = litellm.completion(
        model=model_id,
        messages=messages,
        temperature=0.1
    )
    return response.choices[0].message.content

# ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
models = [
    "gpt-4",                                    # OpenAI
    "claude-3-opus",                           # Anthropic  
    "gemini-pro",                              # Google
    "command-r-plus",                          # Cohere
    "meta-llama/Llama-3.3-70B-Instruct",      # HuggingFace
]
```

#### **ğŸ¯ LiteLLM ì „í™˜ ì¥ì **

**ê°œë°œì ê²½í—˜ (DX) í–¥ìƒ**:
- âœ… **ë‹¨ì¼ API**: ëª¨ë“  LLM ì œê³µìë¥¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ì ‘ê·¼
- âœ… **ìë™ ì¬ì‹œë„**: ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ë‹¤ë¥¸ ì œê³µì ì‹œë„
- âœ… **ë¹„ìš© ì¶”ì **: ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ìë™ ëª¨ë‹ˆí„°ë§
- âœ… **ë¡œë“œ ë°¸ëŸ°ì‹±**: ì—¬ëŸ¬ ì œê³µì ê°„ ìë™ ë¶€í•˜ ë¶„ì‚°

**ì‚¬ìš©ì ê²½í—˜ (UX) ê°œì„ **:
- âœ… **ì•ˆì •ì  ì„œë¹„ìŠ¤**: ë‹¨ì¼ ì œê³µì ì¥ì•  ì‹œì—ë„ ì„œë¹„ìŠ¤ ì§€ì†
- âœ… **ìµœì  ì„±ëŠ¥**: ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ìë™ ë¼ìš°íŒ…
- âœ… **ë” ë§ì€ ëª¨ë¸ ì„ íƒ**: 50+ LLM ëª¨ë¸ ì§€ì›

**ìš´ì˜ íš¨ìœ¨ì„±**:
- âœ… **ë¹„ìš© ìµœì í™”**: ì œê³µìë³„ ê°€ê²© ë¹„êµ ë° ìë™ ì„ íƒ
- âœ… **ì‚¬ìš©ëŸ‰ ë¶„ì„**: ìƒì„¸í•œ API ì‚¬ìš© í†µê³„
- âœ… **ì˜¤ë¥˜ ì²˜ë¦¬**: í†µí•©ëœ ì˜ˆì™¸ ì²˜ë¦¬ ë° ë¡œê¹…

#### **âš ï¸ LiteLLM ì „í™˜ ë‹¨ì **

**ì˜ì¡´ì„± ìš°ë ¤**:
- âŒ **ìƒˆë¡œìš´ ì˜ì¡´ì„±**: LiteLLM ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ì¶”ê°€ ì˜ì¡´ì„±
- âŒ **ì¶”ìƒí™” ì˜¤ë²„í—¤ë“œ**: ì§ì ‘ API í˜¸ì¶œ ëŒ€ë¹„ ì•½ê°„ì˜ ì„±ëŠ¥ ì˜¤ë²„í—¤ë“œ
- âŒ **ì œê³µìë³„ íŠ¹ìˆ˜ ê¸°ëŠ¥**: ê° ì œê³µìì˜ ê³ ìœ  ê¸°ëŠ¥ í™œìš© ì œí•œ

---

## ğŸ”„ **í†µí•© ëŒ€ì²´ ì „ëµ: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•**

### **ğŸ¯ ê¶Œì¥ ëŒ€ì²´ ì•„í‚¤í…ì²˜**

```python
# 1ë‹¨ê³„: LiteLLMìœ¼ë¡œ LLM í†µí•©
import litellm

# 2ë‹¨ê³„: LlamaIndexë¡œ RAG íŒŒì´í”„ë¼ì¸ ëŒ€ì²´
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# 3ë‹¨ê³„: SmolAgentsë¡œ ë„êµ¬ ëª¨ë“ˆí™” (ì„ íƒì )
from smolagents import tool, CodeAgent

class ModernAcademicRAG:
    def __init__(self):
        # LlamaIndex RAG ì„¤ì •
        Settings.embed_model = HuggingFaceEmbedding(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        self.vector_store = FaissVectorStore()
        self.index = VectorStoreIndex.from_vector_store(self.vector_store)
        
    def process_pdfs(self, pdf_paths: list[str]):
        """PDF ì²˜ë¦¬ - LlamaIndex í™œìš©"""
        documents = []
        for pdf_path in pdf_paths:
            text = self._extract_pdf_text(pdf_path)
            doc = Document(text=text, metadata={"source": pdf_path})
            documents.append(doc)
        
        # LlamaIndex ìë™ ì²­í‚¹ ë° ì„ë² ë”©
        self.index = VectorStoreIndex.from_documents(documents)
        
    def query(self, question: str, model: str = "gpt-4") -> str:
        """ì§ˆì˜ì‘ë‹µ - LiteLLM + LlamaIndex"""
        query_engine = self.index.as_query_engine(
            llm=litellm.completion,  # LiteLLM í†µí•©
            response_mode="compact"
        )
        
        response = query_engine.query(question)
        return str(response)
```

### **ğŸ“Š ëŒ€ì²´ íš¨ê³¼ ì˜ˆì¸¡**

| ë©”íŠ¸ë¦­                | í˜„ì¬ êµ¬í˜„      | í•˜ì´ë¸Œë¦¬ë“œ ëŒ€ì²´ | ê°œì„  íš¨ê³¼   |
| --------------------- | -------------- | --------------- | ----------- |
| **ì½”ë“œ ë³µì¡ë„**       | 265ì¤„          | ~180ì¤„          | **-32%**    |
| **ì˜ì¡´ì„± íŒ¨í‚¤ì§€**     | 8ê°œ            | 5ê°œ             | **-38%**    |
| **ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„**    | ~1.0ì´ˆ         | ~0.8ì´ˆ          | **-20%**    |
| **ê°œë°œ ìƒì‚°ì„±**       | ê¸°ì¤€           | +40%            | **+40%**    |
| **ìœ ì§€ë³´ìˆ˜ ìš©ì´ì„±**   | ê¸°ì¤€           | +60%            | **+60%**    |
| **ìƒˆ ê¸°ëŠ¥ ì¶”ê°€ ì‹œê°„** | ê¸°ì¤€           | -50%            | **-50%**    |

---

## ğŸ› ï¸ **ë‹¨ê³„ë³„ ì „í™˜ ê³„íš**

### **Phase 1: LiteLLM í†µí•© (1ì£¼)**
```bash
uv add litellm
# ê¸°ì¡´ ë‹¤ì¤‘ LLM í´ë¼ì´ì–¸íŠ¸ â†’ LiteLLM ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤
```

### **Phase 2: LlamaIndex ì „í™˜ (2ì£¼)**
```bash
uv add llama-index-core llama-index-vector-stores-faiss
uv add llama-index-embeddings-huggingface
# LangChain RAG â†’ LlamaIndex RAG íŒŒì´í”„ë¼ì¸
```

### **Phase 3: í†µí•© í…ŒìŠ¤íŠ¸ ë° ìµœì í™” (1ì£¼)**
```bash
# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ê¸°ëŠ¥ ë™ë“±ì„± ê²€ì¦
# ë¶ˆí•„ìš”í•œ LangChain ì˜ì¡´ì„± ì œê±°
```

---

## ğŸ¯ **ìµœì¢… ê¶Œì¥ì‚¬í•­**

### **âœ… ê¶Œì¥: LlamaIndex + LiteLLM ì¡°í•©**

**ì„ íƒ ì´ìœ **:
1. **âš–ï¸ ìµœì  ê· í˜•**: RAG ì „ë¬¸ì„±(LlamaIndex) + LLM ìœ ì—°ì„±(LiteLLM)
2. **ğŸš€ ë¹ ë¥¸ ì „í™˜**: ê¸°ì¡´ ë¡œì§ê³¼ ë†’ì€ í˜¸í™˜ì„±
3. **ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ**: ê° ì˜ì—­ë³„ ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©
4. **ğŸ”§ ìœ ì§€ë³´ìˆ˜ì„±**: ë‹¨ìˆœí•˜ê³  ëª…í™•í•œ ì½”ë“œ êµ¬ì¡°

**ê¸°ëŒ€ íš¨ê³¼**:
- **ê°œë°œ íš¨ìœ¨ì„±**: 32% ì½”ë“œ ê°ì†Œ, 40% ê°œë°œ ì†ë„ í–¥ìƒ
- **ì‚¬ìš©ì ê²½í—˜**: 20% ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•, ë” ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤
- **ìš´ì˜ íš¨ìœ¨**: 38% ì˜ì¡´ì„± ê°ì†Œ, ë¹„ìš© ìµœì í™”

ì´ ì „ëµì„ í†µí•´ í•™ì¹™ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ **ë‹¨ìˆœí•˜ë©´ì„œë„ ê°•ë ¥í•œ í˜„ëŒ€ì  RAG í”Œë«í¼**ìœ¼ë¡œ ì§„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€âœ¨

---

## ğŸ“š **ìš©ì–´ ë° ê¸°ìˆ  ê°ì£¼**

1. **RAG**: Retrieval-Augmented Generation. ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ ìƒì„± í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•
12. **FAISS**: Facebook AI Similarity Search. Metaì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
15. **PyMuPDF**: Pythonìš© PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬. fitz ëª¨ë“ˆëª…ìœ¼ë¡œ import
3. **LLM**: Large Language Model. GPT, LLaMA, Gemma ë“±ì˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸
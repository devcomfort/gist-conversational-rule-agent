#!/usr/bin/env python3
"""
QA Dataset Generator
===================

ì—°êµ¬ ì‹ ë¢°ì„±ì„ ë³´ì¥í•˜ëŠ” ê°ê´€ì  QA ë°ì´í„°ì…‹ ìƒì„± ì‹œìŠ¤í…œ.
ì²­í‚¹ëœ ë²•ë¥  ë¬¸ì„œë¡œë¶€í„° í‘œì¤€í™”ëœ ì§ˆì˜ì‘ë‹µ ìŒì„ ìƒì„±í•˜ê³  í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
- Mustache í…œí”Œë¦¿ ê¸°ë°˜ ì¼ê´€ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
- ê°ê´€ì  í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
- ì™„ì „í•œ ì¶”ì ì„±ê³¼ ì¬í˜„ì„± ë³´ì¥
- ì—°êµ¬ ëª©ì ì— ìµœì í™”ëœ ë°ì´í„° êµ¬ì¡°

ì‚¬ìš©ë²•:
    python qa_dataset_generator.py
    python qa_dataset_generator.py --chunk-dir experiments/outputs/chunks
    python qa_dataset_generator.py --dry-run
"""

import os
import sys
import json
import time
import pickle
import hashlib
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict

import pystache
import numpy as np
import pandas as pd
from loguru import logger

# LLM í´ë¼ì´ì–¸íŠ¸ (litellm ì‚¬ìš©)
try:
    import litellm
    from litellm import completion
except ImportError:
    logger.error("litellmì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install litellm")
    sys.exit(1)

# ë¡œì»¬ ëª¨ë“ˆ
from experiment_configurations import EXPERIMENT_CONFIG

# ===================================================================
# ë°ì´í„° êµ¬ì¡°
# ===================================================================


@dataclass
class QAPair:
    """ê°œë³„ QA ìŒ ë°ì´í„°"""

    question: str
    answer: str
    qa_type: str  # factual, definitional, procedural, conditional, exception
    difficulty: str  # basic, intermediate, advanced
    answer_source: str
    confidence: str  # high, medium, low


@dataclass
class QADataset:
    """QA ë°ì´í„°ì…‹ ì „ì²´ êµ¬ì¡°"""

    qa_pairs: List[QAPair]
    metadata: Dict[str, Any]


@dataclass
class QAGenerationResult:
    """QA ìƒì„± ê²°ê³¼"""

    chunk_id: str
    success: bool
    qa_dataset: Optional[QADataset] = None
    generation_time: float = 0
    error_message: Optional[str] = None
    quality_score: Optional[float] = None
    quality_grade: Optional[str] = None


@dataclass
class QAEvaluationResult:
    """QA í‰ê°€ ê²°ê³¼"""

    total_score: float
    grade: str  # A, B, C, D, F
    recommendation: str  # accept, revise, reject
    dimension_scores: Dict[str, Any]
    detailed_feedback: Dict[str, Any]
    qa_level_analysis: List[Dict[str, Any]]


# ===================================================================
# í…œí”Œë¦¿ ë Œë”ë§
# ===================================================================


class TemplateRenderer:
    """Mustache í…œí”Œë¦¿ ë Œë”ë§ ê´€ë¦¬"""

    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.system_prompts_dir = self.base_dir / "system_prompts"
        self.templates_dir = self.base_dir / "templates"

        # í…œí”Œë¦¿ ìºì‹œ
        self._template_cache = {}

    def load_template(self, template_path: Path) -> str:
        """í…œí”Œë¦¿ íŒŒì¼ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)"""
        if template_path not in self._template_cache:
            if not template_path.exists():
                raise FileNotFoundError(f"í…œí”Œë¦¿ íŒŒì¼ ì—†ìŒ: {template_path}")

            with open(template_path, "r", encoding="utf-8") as f:
                self._template_cache[template_path] = f.read()

        return self._template_cache[template_path]

    def render_qa_generation_prompt(
        self,
        chunk_text: str,
        chunk_id: str,
        source_document: str,
        chunker_name: Optional[str] = None,
        generation_timestamp: Optional[str] = None,
    ) -> Tuple[str, str]:
        """QA ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ë Œë”ë§ (ì‹œìŠ¤í…œ + ì‚¬ìš©ì)"""

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_template_path = (
            self.system_prompts_dir / "qa_generation_system_prompt.mustache"
        )
        system_prompt = self.load_template(system_template_path)

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë°ì´í„°
        user_data = {
            "chunk_text": chunk_text,
            "chunk_id": chunk_id,
            "source_document": source_document,
            "chunker_name": chunker_name,
            "generation_timestamp": generation_timestamp or datetime.now().isoformat(),
        }

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_template_path = (
            self.templates_dir / "qa_generation_query_template.mustache"
        )
        user_template = self.load_template(user_template_path)
        user_prompt = pystache.render(user_template, user_data)

        return system_prompt, user_prompt

    def render_qa_evaluation_prompt(
        self,
        original_text: str,
        qa_data: str,  # JSON string
        chunk_id: str,
        source_document: str,
        qa_count: int,
        evaluation_timestamp: Optional[str] = None,
    ) -> Tuple[str, str]:
        """QA í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ ë Œë”ë§ (ì‹œìŠ¤í…œ + ì‚¬ìš©ì)"""

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_template_path = (
            self.system_prompts_dir / "qa_evaluation_system_prompt.mustache"
        )
        system_prompt = self.load_template(system_template_path)

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë°ì´í„°
        user_data = {
            "original_text": original_text,
            "qa_data": qa_data,
            "chunk_id": chunk_id,
            "source_document": source_document,
            "qa_count": qa_count,
            "evaluation_timestamp": evaluation_timestamp or datetime.now().isoformat(),
        }

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_template_path = (
            self.templates_dir / "qa_evaluation_query_template.mustache"
        )
        user_template = self.load_template(user_template_path)
        user_prompt = pystache.render(user_template, user_data)

        return system_prompt, user_prompt


# ===================================================================
# LLM í´ë¼ì´ì–¸íŠ¸
# ===================================================================


class LLMClient:
    """LLM API í˜¸ì¶œ ê´€ë¦¬"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {
            "model": "gpt-oss-120b",
            "temperature": 0.1,
            "max_tokens": 2000,
            "timeout": 60,
        }

        # API í‚¤ ì„¤ì • í™•ì¸
        if not os.getenv("OPENAI_API_KEY") and self.config["model"].startswith("gpt-"):
            logger.warning("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    def call_llm(
        self,
        system_prompt: str,
        user_prompt: str,
        response_format: Optional[str] = "json",
    ) -> Tuple[bool, str, Dict[str, Any]]:
        """
        LLM API í˜¸ì¶œ

        Returns:
            (success, response_text, metadata)
        """
        try:
            start_time = time.time()

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]

            # litellmì„ í†µí•œ API í˜¸ì¶œ
            response = completion(
                model=self.config["model"],
                messages=messages,
                temperature=self.config["temperature"],
                max_tokens=self.config["max_tokens"],
                timeout=self.config["timeout"],
            )

            response_time = time.time() - start_time
            response_text = response.choices[0].message.content

            metadata = {
                "model": self.config["model"],
                "response_time": response_time,
                "usage": response.usage.__dict__ if hasattr(response, "usage") else {},
                "timestamp": datetime.now().isoformat(),
            }

            return True, response_text, metadata

        except Exception as e:
            logger.error(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return False, str(e), {}


# ===================================================================
# QA ìƒì„± ê´€ë¦¬ì
# ===================================================================


class QAGenerationManager:
    """QA ë°ì´í„°ì…‹ ìƒì„± ê´€ë¦¬"""

    def __init__(self, output_dir: Path, llm_config: Optional[Dict[str, Any]] = None):
        self.output_dir = Path(output_dir)
        self.qa_dir = self.output_dir / EXPERIMENT_CONFIG["output"].get(
            "qa_subdir", "qa"
        )
        self.qa_dir.mkdir(parents=True, exist_ok=True)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.template_renderer = TemplateRenderer()
        self.llm_client = LLMClient(llm_config)

    def generate_qa_for_chunk(
        self, chunk_text: str, chunk_id: str, chunk_metadata: Dict[str, Any]
    ) -> QAGenerationResult:
        """ê°œë³„ ì²­í¬ì— ëŒ€í•œ QA ìƒì„±"""

        start_time = time.time()

        try:
            logger.info(f"ğŸ¯ QA ìƒì„± ì‹œì‘: {chunk_id}")

            # í”„ë¡¬í”„íŠ¸ ë Œë”ë§
            system_prompt, user_prompt = (
                self.template_renderer.render_qa_generation_prompt(
                    chunk_text=chunk_text,
                    chunk_id=chunk_id,
                    source_document=chunk_metadata.get("source_document", "unknown"),
                    chunker_name=chunk_metadata.get("chunker_name"),
                    generation_timestamp=datetime.now().isoformat(),
                )
            )

            # LLM í˜¸ì¶œ
            success, response, llm_metadata = self.llm_client.call_llm(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                response_format="json",
            )

            if not success:
                return QAGenerationResult(
                    chunk_id=chunk_id,
                    success=False,
                    error_message=f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {response}",
                    generation_time=time.time() - start_time,
                )

            # JSON íŒŒì‹±
            try:
                qa_json = json.loads(response)
                qa_dataset = self._parse_qa_json(qa_json, chunk_id)
            except json.JSONDecodeError as e:
                return QAGenerationResult(
                    chunk_id=chunk_id,
                    success=False,
                    error_message=f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}",
                    generation_time=time.time() - start_time,
                )

            # ê²°ê³¼ ì €ì¥
            result_id = self._save_qa_dataset(qa_dataset, chunk_metadata, llm_metadata)
            generation_time = time.time() - start_time

            logger.info(
                f"âœ… QA ìƒì„± ì™„ë£Œ: {chunk_id} ({len(qa_dataset.qa_pairs)}ê°œ, {generation_time:.1f}ì´ˆ)"
            )

            return QAGenerationResult(
                chunk_id=chunk_id,
                success=True,
                qa_dataset=qa_dataset,
                generation_time=generation_time,
            )

        except Exception as e:
            logger.error(f"âŒ QA ìƒì„± ì‹¤íŒ¨: {chunk_id} - {e}")
            return QAGenerationResult(
                chunk_id=chunk_id,
                success=False,
                error_message=str(e),
                generation_time=time.time() - start_time,
            )

    def _parse_qa_json(self, qa_json: Dict[str, Any], chunk_id: str) -> QADataset:
        """JSON ë°ì´í„°ë¥¼ QADataset ê°ì²´ë¡œ ë³€í™˜"""

        qa_pairs = []
        for qa_data in qa_json.get("qa_pairs", []):
            qa_pair = QAPair(
                question=qa_data.get("question", ""),
                answer=qa_data.get("answer", ""),
                qa_type=qa_data.get("qa_type", "factual"),
                difficulty=qa_data.get("difficulty", "basic"),
                answer_source=qa_data.get("answer_source", ""),
                confidence=qa_data.get("confidence", "medium"),
            )
            qa_pairs.append(qa_pair)

        metadata = qa_json.get("metadata", {})
        metadata["chunk_id"] = chunk_id
        metadata["generation_timestamp"] = datetime.now().isoformat()

        return QADataset(qa_pairs=qa_pairs, metadata=metadata)

    def _save_qa_dataset(
        self,
        qa_dataset: QADataset,
        chunk_metadata: Dict[str, Any],
        llm_metadata: Dict[str, Any],
    ) -> str:
        """QA ë°ì´í„°ì…‹ ì €ì¥"""

        chunk_id = qa_dataset.metadata.get("chunk_id")

        # QA ë°ì´í„° ì €ì¥ (JSON)
        qa_file = self.qa_dir / f"{chunk_id}_qa.json"
        qa_data = {
            "qa_pairs": [asdict(qa) for qa in qa_dataset.qa_pairs],
            "metadata": qa_dataset.metadata,
        }

        with open(qa_file, "w", encoding="utf-8") as f:
            json.dump(qa_data, f, ensure_ascii=False, indent=2)

        # ìƒì„± ë©”íƒ€ë°ì´í„° ì €ì¥
        generation_metadata = {
            "chunk_id": chunk_id,
            "chunk_metadata": chunk_metadata,
            "llm_metadata": llm_metadata,
            "qa_count": len(qa_dataset.qa_pairs),
            "file_size_mb": qa_file.stat().st_size / (1024 * 1024),
            "generation_timestamp": datetime.now().isoformat(),
        }

        metadata_file = self.qa_dir / f"{chunk_id}_generation_metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(generation_metadata, f, ensure_ascii=False, indent=2)

        return chunk_id


# ===================================================================
# QA í’ˆì§ˆ ê²€ì¦ì
# ===================================================================


class QualityValidator:
    """QA í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self, llm_config: Optional[Dict[str, Any]] = None):
        self.template_renderer = TemplateRenderer()
        self.llm_client = LLMClient(llm_config)

    def evaluate_qa_quality(
        self,
        original_text: str,
        qa_dataset: QADataset,
        chunk_id: str,
        source_document: str,
    ) -> QAEvaluationResult:
        """QA í’ˆì§ˆ í‰ê°€ ìˆ˜í–‰"""

        try:
            logger.info(f"ğŸ” í’ˆì§ˆ í‰ê°€ ì‹œì‘: {chunk_id}")

            # QA ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            qa_json_str = json.dumps(
                {
                    "qa_pairs": [asdict(qa) for qa in qa_dataset.qa_pairs],
                    "metadata": qa_dataset.metadata,
                },
                ensure_ascii=False,
                indent=2,
            )

            # í‰ê°€ í”„ë¡¬í”„íŠ¸ ë Œë”ë§
            system_prompt, user_prompt = (
                self.template_renderer.render_qa_evaluation_prompt(
                    original_text=original_text,
                    qa_data=qa_json_str,
                    chunk_id=chunk_id,
                    source_document=source_document,
                    qa_count=len(qa_dataset.qa_pairs),
                    evaluation_timestamp=datetime.now().isoformat(),
                )
            )

            # LLM í˜¸ì¶œ
            success, response, llm_metadata = self.llm_client.call_llm(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                response_format="json",
            )

            if not success:
                logger.error(f"í‰ê°€ LLM í˜¸ì¶œ ì‹¤íŒ¨: {response}")
                return self._create_default_evaluation(chunk_id, error=response)

            # í‰ê°€ ê²°ê³¼ íŒŒì‹±
            try:
                eval_json = json.loads(response)
                evaluation = self._parse_evaluation_json(eval_json)

                logger.info(
                    f"âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ: {chunk_id} (ì ìˆ˜: {evaluation.total_score:.1f}, ë“±ê¸‰: {evaluation.grade})"
                )
                return evaluation

            except json.JSONDecodeError as e:
                logger.error(f"í‰ê°€ ê²°ê³¼ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return self._create_default_evaluation(chunk_id, error=str(e))

        except Exception as e:
            logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {chunk_id} - {e}")
            return self._create_default_evaluation(chunk_id, error=str(e))

    def _parse_evaluation_json(self, eval_json: Dict[str, Any]) -> QAEvaluationResult:
        """í‰ê°€ JSONì„ ê²°ê³¼ ê°ì²´ë¡œ ë³€í™˜"""

        overall = eval_json.get("overall_evaluation", {})

        return QAEvaluationResult(
            total_score=float(overall.get("total_score", 0)),
            grade=overall.get("grade", "F"),
            recommendation=overall.get("recommendation", "reject"),
            dimension_scores=eval_json.get("dimension_scores", {}),
            detailed_feedback=eval_json.get("detailed_feedback", {}),
            qa_level_analysis=eval_json.get("qa_level_analysis", []),
        )

    def _create_default_evaluation(
        self, chunk_id: str, error: str
    ) -> QAEvaluationResult:
        """ê¸°ë³¸/ì—ëŸ¬ í‰ê°€ ê²°ê³¼ ìƒì„±"""

        return QAEvaluationResult(
            total_score=0.0,
            grade="F",
            recommendation="reject",
            dimension_scores={},
            detailed_feedback={"error": error},
            qa_level_analysis=[],
        )


# ===================================================================
# ë©”ì¸ QA ë°ì´í„°ì…‹ ë¹Œë”
# ===================================================================


class QADatasetBuilder:
    """QA ë°ì´í„°ì…‹ ë¹Œë” - ì „ì²´ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬"""

    def __init__(
        self,
        output_dir: Optional[str] = None,
        llm_config: Optional[Dict[str, Any]] = None,
    ):
        self.output_dir = Path(output_dir or EXPERIMENT_CONFIG["output"]["base_dir"])
        self.chunk_dir = self.output_dir / EXPERIMENT_CONFIG["output"].get(
            "chunking_subdir", "chunks"
        )

        # ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.qa_generator = QAGenerationManager(self.output_dir, llm_config)
        self.quality_validator = QualityValidator(llm_config)

        # ê²°ê³¼ ì¶”ì 
        self.results: List[QAGenerationResult] = []

    def scan_available_chunks(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬ ID ìŠ¤ìº”"""

        if not self.chunk_dir.exists():
            logger.warning(f"ì²­í¬ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.chunk_dir}")
            return []

        chunk_ids = []
        for chunks_file in self.chunk_dir.glob("*_chunks.pkl"):
            chunk_id = chunks_file.stem.replace("_chunks", "")

            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
            metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"
            if metadata_file.exists():
                chunk_ids.append(chunk_id)

        logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬: {len(chunk_ids)}ê°œ")
        return sorted(chunk_ids)

    def load_chunk_data(self, chunk_id: str) -> Tuple[List[Any], Dict[str, Any]]:
        """ì²­í¬ ë°ì´í„° ë¡œë“œ"""

        chunks_file = self.chunk_dir / f"{chunk_id}_chunks.pkl"
        metadata_file = self.chunk_dir / f"{chunk_id}_metadata.json"

        if not chunks_file.exists():
            raise FileNotFoundError(f"ì²­í¬ íŒŒì¼ ì—†ìŒ: {chunks_file}")

        # ì²­í¬ ë°ì´í„° ë¡œë“œ
        with open(chunks_file, "rb") as f:
            chunks = pickle.load(f)

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata = {}
        if metadata_file.exists():
            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)

        return chunks, metadata

    def generate_qa_for_all_chunks(
        self,
        chunk_ids: Optional[List[str]] = None,
        max_chunks: Optional[int] = None,
        include_quality_evaluation: bool = True,
    ) -> Dict[str, Any]:
        """ëª¨ë“  ì²­í¬ì— ëŒ€í•œ QA ìƒì„±"""

        logger.info("ğŸš€ QA ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘!")

        # ì²­í¬ ID ê²°ì •
        if chunk_ids is None:
            chunk_ids = self.scan_available_chunks()

        if max_chunks:
            chunk_ids = chunk_ids[:max_chunks]

        logger.info(f"ğŸ“‹ ì²˜ë¦¬í•  ì²­í¬: {len(chunk_ids)}ê°œ")

        # ì²­í¬ë³„ QA ìƒì„±
        successful_results = []
        failed_results = []

        for i, chunk_id in enumerate(chunk_ids, 1):
            logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘ [{i}/{len(chunk_ids)}]: {chunk_id}")

            try:
                # ì²­í¬ ë°ì´í„° ë¡œë“œ
                chunks, chunk_metadata = self.load_chunk_data(chunk_id)

                # ì²­í¬ í…ìŠ¤íŠ¸ ê²°í•©
                chunk_text = "\n\n".join(
                    [
                        chunk.page_content
                        if hasattr(chunk, "page_content")
                        else str(chunk)
                        for chunk in chunks
                    ]
                )

                # QA ìƒì„±
                result = self.qa_generator.generate_qa_for_chunk(
                    chunk_text=chunk_text,
                    chunk_id=chunk_id,
                    chunk_metadata=chunk_metadata,
                )

                # í’ˆì§ˆ í‰ê°€ (ì„ íƒì )
                if result.success and include_quality_evaluation and result.qa_dataset:
                    evaluation = self.quality_validator.evaluate_qa_quality(
                        original_text=chunk_text,
                        qa_dataset=result.qa_dataset,
                        chunk_id=chunk_id,
                        source_document=chunk_metadata.get(
                            "source_document", "unknown"
                        ),
                    )
                    result.quality_score = evaluation.total_score
                    result.quality_grade = evaluation.grade

                self.results.append(result)

                if result.success:
                    successful_results.append(result)
                    logger.info(f"âœ… ì™„ë£Œ: {chunk_id}")
                else:
                    failed_results.append(result)
                    logger.error(f"âŒ ì‹¤íŒ¨: {chunk_id} - {result.error_message}")

            except Exception as e:
                logger.error(f"âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {chunk_id} - {e}")
                failed_results.append(
                    QAGenerationResult(
                        chunk_id=chunk_id, success=False, error_message=str(e)
                    )
                )

        # ê²°ê³¼ ìš”ì•½ ì €ì¥
        summary = self._save_generation_summary(successful_results, failed_results)

        logger.info(f"\nğŸ‰ QA ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
        logger.info(f"âœ… ì„±ê³µ: {len(successful_results)}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {len(failed_results)}ê°œ")
        if include_quality_evaluation and successful_results:
            avg_quality = np.mean(
                [r.quality_score for r in successful_results if r.quality_score]
            )
            logger.info(f"ğŸ“Š í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.1f}ì ")

        return summary

    def _save_generation_summary(
        self,
        successful_results: List[QAGenerationResult],
        failed_results: List[QAGenerationResult],
    ) -> Dict[str, Any]:
        """ìƒì„± ê²°ê³¼ ìš”ì•½ ì €ì¥"""

        # CSV í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥
        results_data = []
        for result in self.results:
            results_data.append(
                {
                    "chunk_id": result.chunk_id,
                    "success": result.success,
                    "qa_count": len(result.qa_dataset.qa_pairs)
                    if result.qa_dataset
                    else 0,
                    "generation_time": result.generation_time,
                    "quality_score": result.quality_score,
                    "quality_grade": result.quality_grade,
                    "error_message": result.error_message or "",
                    "timestamp": datetime.now().isoformat(),
                }
            )

        results_df = pd.DataFrame(results_data)
        results_csv = self.output_dir / "qa_generation_results.csv"
        results_df.to_csv(results_csv, index=False)

        # ìš”ì•½ í†µê³„
        summary = {
            "execution_time": time.time(),
            "total_chunks": len(self.results),
            "successful_generations": len(successful_results),
            "failed_generations": len(failed_results),
            "success_rate": len(successful_results) / len(self.results)
            if self.results
            else 0,
            "total_qa_pairs": sum(
                len(r.qa_dataset.qa_pairs) for r in successful_results if r.qa_dataset
            ),
            "average_quality_score": np.mean(
                [
                    r.quality_score
                    for r in successful_results
                    if r.quality_score is not None
                ]
            )
            if successful_results
            else None,
            "results_file": str(results_csv),
        }

        # JSON ìš”ì•½ ì €ì¥
        summary_file = self.output_dir / "qa_generation_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        return summary


# ===================================================================
# CLI ë° ë©”ì¸ ì‹¤í–‰
# ===================================================================


def main():
    parser = argparse.ArgumentParser(description="QA Dataset Generator")
    parser.add_argument("--chunk-dir", type=str, help="ì²­í¬ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--output-dir", type=str, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--max-chunks", type=int, help="ìµœëŒ€ ì²˜ë¦¬í•  ì²­í¬ ìˆ˜")
    parser.add_argument("--chunk-ids", type=str, nargs="+", help="íŠ¹ì • ì²­í¬ IDë§Œ ì²˜ë¦¬")
    parser.add_argument("--no-evaluation", action="store_true", help="í’ˆì§ˆ í‰ê°€ ìƒëµ")
    parser.add_argument("--dry-run", action="store_true", help="ì„¤ì • í™•ì¸ë§Œ ì‹¤í–‰")
    parser.add_argument(
        "--model", type=str, default="gpt-oss-120b", help="ì‚¬ìš©í•  LLM ëª¨ë¸"
    )

    args = parser.parse_args()

    # LLM ì„¤ì •
    llm_config = {
        "model": args.model,
        "temperature": 0.1,
        "max_tokens": 2000,
        "timeout": 60,
    }

    # QA ë¹Œë” ì´ˆê¸°í™”
    builder = QADatasetBuilder(output_dir=args.output_dir, llm_config=llm_config)

    # ì²­í¬ ë””ë ‰í† ë¦¬ ì—…ë°ì´íŠ¸
    if args.chunk_dir:
        builder.chunk_dir = Path(args.chunk_dir)

    # Dry run
    if args.dry_run:
        chunk_ids = builder.scan_available_chunks()
        logger.info(f"âœ… ì„¤ì • í™•ì¸ ì™„ë£Œ")
        logger.info(f"ğŸ“ ì²­í¬ ë””ë ‰í† ë¦¬: {builder.chunk_dir}")
        logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {builder.output_dir}")
        logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬: {len(chunk_ids)}ê°œ")
        logger.info(f"ğŸ¤– LLM ëª¨ë¸: {llm_config['model']}")
        return 0

    # ì‹¤ì œ QA ìƒì„± ì‹¤í–‰
    try:
        result = builder.generate_qa_for_all_chunks(
            chunk_ids=args.chunk_ids,
            max_chunks=args.max_chunks,
            include_quality_evaluation=not args.no_evaluation,
        )

        if result["success_rate"] > 0.8:
            logger.info("ğŸ‰ QA ìƒì„± ì„±ê³µ!")
            return 0
        else:
            logger.warning(
                f"âš ï¸ QA ìƒì„± ë¶€ë¶„ ì„±ê³µ (ì„±ê³µë¥ : {result['success_rate']:.1%})"
            )
            return 1

    except Exception as e:
        logger.error(f"âŒ QA ìƒì„± ì‹¤íŒ¨: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())

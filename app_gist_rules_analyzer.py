"""
GIST Rules Analyzer - ì§€ìŠ¤íŠ¸ í•™ì¹™ ê·œì • ìë™ ë¶„ì„ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ GISTì˜ ëª¨ë“  í•™ì¹™ê³¼ ê·œì • ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬
ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µì„ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
- í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“  PDF íŒŒì¼ ìë™ ìŠ¤ìº” ë° ì²˜ë¦¬
- ë‹¤ì–‘í•œ rerank ë°©ë²• ë™ì‹œ ë¹„êµ ë¶„ì„
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ ëª¨ë‹ˆí„°ë§
- ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‹¤ì‹œê°„ ì‘ë‹µ
- Markdown í˜•íƒœ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°

ìë™ ì²˜ë¦¬ ê¸°ëŠ¥:
- ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ ëª¨ë“  PDF íŒŒì¼ ìë™ íƒì§€
- ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë™ ë²¡í„°í™” ì²˜ë¦¬
- ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ì²˜ë¦¬ ì™„ë£Œ í›„ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥

ê¸°ìˆ  ìŠ¤íƒ:
- ì„ë² ë”©: sentence-transformers/all-MiniLM-L6-v2
- ë²¡í„° ìŠ¤í† ì–´: FAISS
- í…ìŠ¤íŠ¸ ë¶„í• : RecursiveCharacterTextSplitter
- PDF ì²˜ë¦¬: PyMuPDF (fitz)
- UI: Gradio with custom CSS
"""

import gradio as gr
import os
import json
import time
import hashlib
import html
import threading
import fitz
import openai
import glob
from collections import OrderedDict
from datetime import datetime
from dotenv import load_dotenv
from huggingface_hub import InferenceClient
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from typing import Dict, Generator, List, Optional
import faiss
import numpy as np
from pathlib import Path

# Environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HF_API_KEY = os.getenv("HUGGING_FACE_API_KEY") or os.getenv("HF_TOKEN")
HF_ENTERPRISE = os.getenv("HUGGING_FACE_ENTERPRISE")

# Session variables
MAX_SESSIONS = 100
sessions = OrderedDict()
session_lock = threading.Lock()

# Global shared state
shared_state: Dict = {
    "current_model": "GPT-4",
    "vectorstore": None,
    "pdfs": [],
    "auto_processing_complete": False,
    "processing_status": "ğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (GIST ê·œì •ì§‘ ë¡œë”©)",
    "processed_count": 0,
    "total_count": 0,
    "faiss_comparator": None,
    "performance_logger": None,  # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
}
shared_state_lock = threading.Lock()

# Model setup
MODELS = {
    "GPT-4": {"model_id": "gpt-4", "provider": "openai"},
    "DeepSeek-R1": {"model_id": "deepseek-ai/DeepSeek-R1", "provider": "novita"},
    "Gemma-3-27B": {"model_id": "google/gemma-3-27b-it", "provider": "hf-inference"},
    "Llama-3.3-70B": {
        "model_id": "meta-llama/Llama-3.3-70B-Instruct",
        "provider": "hf-inference",
    },
    "QwQ-32B": {"model_id": "Qwen/QwQ-32B", "provider": "hf-inference"},
}

# Rerank ì„¤ì •
RERANK_OPTIONS = {
    "ì—†ìŒ": {"enabled": False, "model": None, "top_k": 3},
    "Cross-Encoder (ê¸°ë³¸)": {
        "enabled": True,
        "model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "top_k": 3,
    },
    "Cross-Encoder (ê³ ì„±ëŠ¥)": {
        "enabled": True,
        "model": "cross-encoder/ms-marco-MiniLM-L-12-v2",
        "top_k": 3,
    },
    "ë‹¤êµ­ì–´ Cross-Encoder": {
        "enabled": True,
        "model": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
        "top_k": 3,
    },
}

EMBED_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
DIMENSION = 384
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

system_prompt = """You are a GIST Rules and Regulations Expert Assistant. 
You have comprehensive knowledge of all GIST academic rules, regulations, guidelines, and policies.
Always provide accurate, detailed answers based on the provided context.
When answering questions about GIST rules, cite specific regulation numbers and titles when available."""


# --------- (A) PERFORMANCE LOGGING & FAISS INDEX COMPARISON ---------
class PerformanceLogger:
    """ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ê¸°ë¡í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, log_dir: str = "performance_logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.log_file = (
            self.log_dir
            / f"faiss_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        )

    def log_query_performance(self, query: str, results: Dict):
        """ë‹¨ì¼ ì¿¼ë¦¬ì˜ ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë¡œê¹…"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "query_hash": hashlib.md5(query.encode()).hexdigest()[:8],
            "results": results,
        }

        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        print(f"ğŸ“Š ì„±ëŠ¥ ê²°ê³¼ ê¸°ë¡: {self.log_file}")


class FaissIndexComparator:
    """ë‹¤ì–‘í•œ FAISS ì¸ë±ìŠ¤ íƒ€ì…ë³„ ì„±ëŠ¥ ë¹„êµ"""

    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.indexes = {}
        self.performance_logger = PerformanceLogger()

    def create_indexes_from_vectorstore(self, vectorstore: FAISS) -> Dict:
        """ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ë‹¤ì–‘í•œ ì¸ë±ìŠ¤ íƒ€ì… ìƒì„±"""
        print("ğŸ”§ ë‹¤ì–‘í•œ FAISS ì¸ë±ìŠ¤ íƒ€ì… ìƒì„± ì¤‘...")

        # ê¸°ì¡´ ë²¡í„°ì™€ ë¬¸ì„œ ì¶”ì¶œ
        vectors = vectorstore.index.reconstruct_n(0, vectorstore.index.ntotal)

        results = {}

        # 1. IndexFlatL2 (ê¸°ë³¸ê°’ - ì´ë¯¸ ì¡´ì¬)
        start_time = time.time()
        flat_index = faiss.IndexFlatL2(self.dimension)
        flat_index.add(vectors)
        results["IndexFlatL2"] = {
            "creation_time": time.time() - start_time,
            "index": flat_index,
            "memory_usage": flat_index.d * flat_index.ntotal * 4,  # float32
        }

        # 2. IndexIVFFlat (í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ê·¼ì‚¬ ê²€ìƒ‰)
        if vectorstore.index.ntotal > 100:  # ìµœì†Œ ë°ì´í„° í•„ìš”
            start_time = time.time()
            nlist = min(100, int(np.sqrt(vectorstore.index.ntotal)))  # í´ëŸ¬ìŠ¤í„° ìˆ˜
            quantizer = faiss.IndexFlatL2(self.dimension)
            ivf_index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
            ivf_index.train(vectors)
            ivf_index.add(vectors)
            ivf_index.nprobe = min(10, nlist)  # ê²€ìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
            results["IndexIVFFlat"] = {
                "creation_time": time.time() - start_time,
                "index": ivf_index,
                "nlist": nlist,
                "nprobe": ivf_index.nprobe,
                "memory_usage": ivf_index.d * ivf_index.ntotal * 4
                + nlist * ivf_index.d * 4,
            }

        # 3. IndexHNSWFlat (ê·¸ë˜í”„ ê¸°ë°˜ ê³ ì† ê²€ìƒ‰)
        if vectorstore.index.ntotal > 50:
            start_time = time.time()
            hnsw_index = faiss.IndexHNSWFlat(self.dimension, 32)  # M=32
            hnsw_index.hnsw.efConstruction = 200
            hnsw_index.hnsw.efSearch = 64
            hnsw_index.add(vectors)
            results["IndexHNSWFlat"] = {
                "creation_time": time.time() - start_time,
                "index": hnsw_index,
                "M": 32,
                "efConstruction": 200,
                "efSearch": 64,
                "memory_usage": hnsw_index.d * hnsw_index.ntotal * 4
                + hnsw_index.ntotal * 200,  # ì¶”ì •
            }

        # ê²°ê³¼ ì €ì¥
        self.indexes = {name: data["index"] for name, data in results.items()}

        print(f"âœ… {len(results)}ê°œ ì¸ë±ìŠ¤ íƒ€ì… ì¤€ë¹„ ì™„ë£Œ")
        return results

    def compare_search_performance(self, query_vector: np.ndarray, k: int = 3) -> Dict:
        """ëª¨ë“  ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ"""
        results = {}

        for index_name, index in self.indexes.items():
            start_time = time.time()

            try:
                distances, indices = index.search(query_vector.reshape(1, -1), k)
                search_time = time.time() - start_time
                results[index_name] = {
                    "search_time_ms": search_time * 1000,  # ë°€ë¦¬ì´ˆë¡œ ë³€í™˜
                    "distances": distances.tolist(),
                    "indices": indices.tolist(),
                    "success": True,
                }

            except Exception as e:
                results[index_name] = {
                    "search_time_ms": 0,
                    "error": str(e),
                    "success": False,
                }
                print(f"âš ï¸ {index_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        return results


class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œë¥¼ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        self.start_time: Optional[float] = None
        self.first_token_time: Optional[float] = None
        self.end_time: Optional[float] = None
        self.token_count: int = 0
        self.retrieval_time: float = 0.0

    def reset(self):
        self.start_time = None
        self.first_token_time = None
        self.end_time = None
        self.token_count = 0
        self.retrieval_time = 0.0

    def start_query(self):
        self.reset()
        self.start_time = time.time()

    def first_token_received(self):
        if self.first_token_time is None:
            self.first_token_time = time.time()

    def add_token(self, token_text: str = ""):
        self.token_count += 1

    def finish_query(self):
        self.end_time = time.time()

    def get_metrics(self) -> Dict[str, float]:
        if not self.start_time:
            return {}

        metrics: Dict[str, float] = {}
        current_time = self.end_time or time.time()

        # ì „ì²´ ì†Œìš” ì‹œê°„
        metrics["total_time"] = current_time - self.start_time

        # ì²« í† í°ê¹Œì§€ì˜ ì‹œê°„
        if self.first_token_time:
            metrics["time_to_first_token"] = self.first_token_time - self.start_time

        # Tokens per second
        if self.token_count > 0 and self.first_token_time and self.end_time:
            generation_time = self.end_time - self.first_token_time
            if generation_time > 0:
                metrics["tokens_per_second"] = self.token_count / generation_time

        # ê²€ìƒ‰ ì‹œê°„
        metrics["retrieval_time"] = self.retrieval_time

        return metrics


# ê° rerank ëª¨ë“œë³„ ì„±ëŠ¥ ì§€í‘œ ì¶”ì 
performance_trackers = {mode: PerformanceMetrics() for mode in RERANK_OPTIONS.keys()}


def scan_pdf_files() -> List[str]:
    """í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  PDF íŒŒì¼ì„ ìŠ¤ìº”"""
    print("ğŸ” Scanning for PDF files in project directory...")

    # í˜„ì¬ ë””ë ‰í† ë¦¬ì™€ í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = []

    # globì„ ì‚¬ìš©í•´ì„œ ì¬ê·€ì ìœ¼ë¡œ PDF íŒŒì¼ ê²€ìƒ‰
    pdf_patterns = [
        "*.pdf",
        "**/*.pdf",
        "rules/**/*.pdf",
        "documents/**/*.pdf",
        "data/**/*.pdf",
    ]

    for pattern in pdf_patterns:
        found_files = glob.glob(pattern, recursive=True)
        for file in found_files:
            if file not in pdf_files and os.path.exists(file):
                pdf_files.append(file)

    print(f"ğŸ“„ Found {len(pdf_files)} PDF files")

    # íŒŒì¼ë“¤ì„ í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬ (ì‘ì€ íŒŒì¼ë¶€í„° ì²˜ë¦¬)
    pdf_files.sort(key=lambda x: os.path.getsize(x) if os.path.exists(x) else 0)

    # ëŒ€ìš©ëŸ‰ íŒŒì¼ í•„í„°ë§ (50MB ì´ìƒ íŒŒì¼ì€ ê²½ê³ )
    large_files = []
    filtered_files = []

    for pdf_file in pdf_files:
        if os.path.exists(pdf_file):
            file_size = os.path.getsize(pdf_file)
            if file_size > 50 * 1024 * 1024:  # 50MB
                large_files.append((pdf_file, file_size))
            else:
                filtered_files.append(pdf_file)
        else:
            print(f"âš ï¸ File not found during scan: {pdf_file}")

    if large_files:
        print(f"ğŸ“Š Found {len(large_files)} large files (>50MB):")
        for file_path, size in large_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            size_mb = size / (1024 * 1024)
            print(f"   - {os.path.basename(file_path)}: {size_mb:.1f}MB")
        if len(large_files) > 5:
            print(f"   ... and {len(large_files) - 5} more large files")

        # í° íŒŒì¼ë“¤ë„ í¬í•¨í•˜ë˜, ë‚˜ì¤‘ì— ì²˜ë¦¬
        filtered_files.extend([f[0] for f in large_files])

    print(f"ğŸ“„ Total files to process: {len(filtered_files)}")
    return filtered_files


def auto_process_pdfs():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë“  PDF íŒŒì¼ì„ ìë™ ì²˜ë¦¬"""
    print("ğŸš€ Starting automatic PDF processing...")

    try:
        # PDF íŒŒì¼ ìŠ¤ìº” - ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ (ì œí•œ ì—†ìŒ)
        pdf_files = scan_pdf_files()

        print(f"ğŸ“š ì „ì²´ GIST ê·œì •ì§‘ ì²˜ë¦¬: {len(pdf_files)}ê°œ PDF íŒŒì¼")
        print("ğŸ¯ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•´ ëª¨ë“  ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤")

        with shared_state_lock:
            shared_state["total_count"] = len(pdf_files)
            shared_state["processed_count"] = 0
            shared_state["processing_status"] = (
                f"ğŸ“– **ì „ì²´ Document Processing ì‹œì‘**\n\n"
                f"ğŸ“š **ì²˜ë¦¬ ëŒ€ìƒ**: {len(pdf_files)}ê°œ ëª¨ë“  PDF íŒŒì¼\n"
                f"ğŸ¯ ì™„ì „í•œ GIST ê·œì •ì§‘ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘"
            )

        if not pdf_files:
            with shared_state_lock:
                shared_state["processing_status"] = (
                    "âš ï¸ ê·œì •ì§‘ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”"
                )
                shared_state["auto_processing_complete"] = True
            return

        # ë¬¸ì„œ ì²˜ë¦¬
        print("ğŸ“ Processing PDF documents...")
        all_docs = []

        successfully_processed = 0
        failed_files = []

        for i, pdf_file in enumerate(pdf_files):
            try:
                with shared_state_lock:
                    shared_state["processing_status"] = (
                        f"ğŸ“„ {os.path.basename(pdf_file)} ë¬¸ì„œë¥¼ ì½ëŠ” ì¤‘... ({i + 1}/{len(pdf_files)})"
                    )

                print(
                    f"\rğŸ“„ ì²˜ë¦¬ ì¤‘ [{i + 1}/{len(pdf_files)}]: {os.path.basename(pdf_file)}",
                    end="",
                    flush=True,
                )

                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not os.path.exists(pdf_file):
                    print(f"âš ï¸ File not found: {pdf_file}")
                    failed_files.append(f"{os.path.basename(pdf_file)} (íŒŒì¼ ì—†ìŒ)")
                    continue

                # íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
                file_size = os.path.getsize(pdf_file)
                is_large_file = file_size > 10 * 1024 * 1024  # 10MB
                is_very_large_file = file_size > 20 * 1024 * 1024  # 20MB

                # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                doc = fitz.open(pdf_file)
                text_pages = []

                # ğŸ“ˆ íŒŒì¼ í¬ê¸°ë³„ ìµœì í™”ëœ ì²˜ë¦¬
                page_count = doc.page_count
                if is_very_large_file:
                    batch_size = 3  # ë§¤ìš° í° íŒŒì¼: 3í˜ì´ì§€ì”©
                elif is_large_file:
                    batch_size = 8  # í° íŒŒì¼: 8í˜ì´ì§€ì”©
                else:
                    batch_size = min(page_count, 20)  # ì‘ì€ íŒŒì¼: ìµœëŒ€ 20í˜ì´ì§€ì”©

                for batch_start in range(0, page_count, batch_size):
                    batch_end = min(batch_start + batch_size, page_count)
                    batch_pages = []

                    for page_num in range(batch_start, batch_end):
                        try:
                            page = doc[page_num]
                            page_text = page.get_text("text")
                            if page_text.strip():
                                batch_pages.append(page_text)

                            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í˜ì´ì§€ ê°ì²´ ì •ë¦¬
                            del page

                        except Exception as page_error:
                            print(
                                f"âš ï¸ Error reading page {page_num + 1} of {pdf_file}: {page_error}"
                            )
                            continue

                    if batch_pages:
                        text_pages.extend(batch_pages)

                    # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    if is_large_file and batch_end < page_count:
                        progress = (batch_end / page_count) * 100
                        with shared_state_lock:
                            shared_state["processing_status"] = (
                                f"ğŸ“˜ í° ë¬¸ì„œë¥¼ ì°¨ê·¼ì°¨ê·¼ ì½ëŠ” ì¤‘: {os.path.basename(pdf_file)} "
                                f"({i + 1}/{len(pdf_files)}) - ì§„í–‰ë¥ : {progress:.1f}%"
                            )

                doc.close()
                text = "\n".join(text_pages)

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del text_pages

                # ëª…ì‹œì  ë©”ëª¨ë¦¬ ì •ë¦¬ (gc.collect() ì—†ì´)
                if (i + 1) % 20 == 0:
                    print(
                        f"\rğŸ§¹ [{i + 1}/{len(pdf_files)}] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ    "
                    )  # ê³µë°±ìœ¼ë¡œ ì´ì „ í…ìŠ¤íŠ¸ ì§€ì›€

                if text.strip():  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
                    # íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ì²­í¬ í¬ê¸° ì¡°ì •
                    if is_large_file:
                        # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ë” í° ì²­í¬ ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                        dynamic_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=800, chunk_overlap=100
                        )
                    else:
                        # ì¼ë°˜ íŒŒì¼ì€ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
                        dynamic_splitter = TEXT_SPLITTER

                    document = Document(
                        page_content=text,
                        metadata={
                            "source": pdf_file,
                            "filename": os.path.basename(pdf_file),
                            "category": get_document_category(pdf_file),
                            "page_count": page_count,
                            "file_size": file_size,
                            "is_large_file": is_large_file,
                        },
                    )

                    # í…ìŠ¤íŠ¸ ë¶„í• 
                    docs = dynamic_splitter.split_documents([document])
                    all_docs.extend(docs)
                    successfully_processed += 1

                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del text, document

                    file_size_mb = file_size / (1024 * 1024)
                    print(
                        f"\râœ… [{i + 1}/{len(pdf_files)}] {os.path.basename(pdf_file)} ({len(docs)} chunks, {file_size_mb:.1f}MB)"
                    )
                else:
                    print(f"âš ï¸ No text content found in: {pdf_file}")
                    failed_files.append(f"{os.path.basename(pdf_file)} (í…ìŠ¤íŠ¸ ì—†ìŒ)")

                with shared_state_lock:
                    shared_state["processed_count"] = i + 1

            except Exception as e:
                print(f"âŒ Error processing {pdf_file}: {e}")
                failed_files.append(
                    f"{os.path.basename(pdf_file)} (ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:50]}...)"
                )
                continue

        if all_docs:
            # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            total_chunks = len(all_docs)
            print(f"ğŸ”„ Creating vector store from {total_chunks:,} document chunks...")

            # ğŸ“ˆ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ í¬ê¸° ìµœì í™”
            if total_chunks > 30000:
                batch_size = 200  # ì´ˆëŒ€ìš©ëŸ‰: ë§¤ìš° ì‘ì€ ë°°ì¹˜ë¡œ ì•ˆì „í•˜ê²Œ
            elif total_chunks > 20000:
                batch_size = 300  # ëŒ€ìš©ëŸ‰: ì‘ì€ ë°°ì¹˜
            elif total_chunks > 15000:
                batch_size = 400  # ì¤‘ëŒ€ìš©ëŸ‰: ì¤‘ê°„ ë°°ì¹˜
            elif total_chunks > 10000:
                batch_size = 500  # ë§ì€ ì²­í¬: ì¤‘ê°„ ë°°ì¹˜
            elif total_chunks > 5000:
                batch_size = 800  # ì¤‘ê°„ ì²­í¬: í° ë°°ì¹˜
            else:
                batch_size = min(
                    1000, max(200, total_chunks // 10)
                )  # ì ì€ ì²­í¬: ìµœëŒ€ ë°°ì¹˜

            num_batches = (total_chunks + batch_size - 1) // batch_size

            print(
                f"ğŸ“Š Processing in {num_batches} batches of ~{batch_size} chunks each"
            )

            vectorstore: Optional[FAISS] = None
            vectorization_start = time.time()

            for batch_idx in range(num_batches):
                batch_start = batch_idx * batch_size
                batch_end = min(batch_start + batch_size, total_chunks)
                batch_docs = all_docs[batch_start:batch_end]

                batch_start_time = time.time()

                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                progress_percent = (batch_idx / num_batches) * 100
                elapsed_time = time.time() - vectorization_start

                if batch_idx > 0:  # ì²« ë°°ì¹˜ ì´í›„ë¶€í„° ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
                    avg_time_per_batch = elapsed_time / batch_idx
                    remaining_batches = num_batches - batch_idx
                    estimated_remaining = avg_time_per_batch * remaining_batches

                    eta_minutes = int(estimated_remaining // 60)
                    eta_seconds = int(estimated_remaining % 60)

                    if eta_minutes > 0:
                        eta_text = f"ì•½ {eta_minutes}ë¶„ {eta_seconds}ì´ˆ ë‚¨ìŒ"
                    else:
                        eta_text = f"ì•½ {eta_seconds}ì´ˆ ë‚¨ìŒ"

                    # ì²˜ë¦¬ ì†ë„ ê³„ì‚°
                    total_processed_chunks = batch_idx * batch_size
                    chunks_per_second = (
                        total_processed_chunks / elapsed_time if elapsed_time > 0 else 0
                    )
                    speed_text = f"{chunks_per_second:.1f} ì²­í¬/ì´ˆ"
                else:
                    eta_text = "ì˜ˆìƒ ì‹œê°„ ê³„ì‚° ì¤‘..."
                    speed_text = "ì†ë„ ê³„ì‚° ì¤‘..."

                # ì‹œê°ì  ì§„í–‰ë¥  ë°” ìƒì„±
                progress_bar_length = 20
                filled_length = int(progress_percent / 5)  # 5%ì”© í‘œì‹œ
                progress_bar = "â–ˆ" * filled_length + "â–‘" * (
                    progress_bar_length - filled_length
                )

                elapsed_minutes = int(elapsed_time // 60)
                elapsed_seconds = int(elapsed_time % 60)
                elapsed_text = (
                    f"{elapsed_minutes}ë¶„ {elapsed_seconds}ì´ˆ"
                    if elapsed_minutes > 0
                    else f"{elapsed_seconds}ì´ˆ"
                )

                with shared_state_lock:
                    shared_state["processing_status"] = (
                        f"ğŸ§  **Vector Store êµ¬ì¶• ì¤‘** (RAG ì‹œìŠ¤í…œ ì¤€ë¹„)\n\n"
                        f"**ë°°ì¹˜ ì§„í–‰ë¥ **: {batch_idx + 1}/{num_batches} ({progress_percent:.1f}%)\n"
                        f"`{progress_bar}` {progress_percent:.1f}%\n\n"
                        f"**ì²˜ë¦¬ ì¤‘**: {batch_start:,} ~ {batch_end:,} / {total_chunks:,} chunks\n"
                        f"**ì†Œìš” ì‹œê°„**: {elapsed_text}\n"
                        f"**ì²˜ë¦¬ ì†ë„**: {speed_text}\n"
                        f"**ì˜ˆìƒ ì™„ë£Œ**: {eta_text}"
                    )

                try:
                    if vectorstore is None:
                        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                        print(
                            f"\rğŸš€ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”: ë°°ì¹˜ 1/{num_batches} ({len(batch_docs)} chunks)",
                            end="",
                            flush=True,
                        )

                        # ì²« ë²ˆì§¸ ë°°ì¹˜ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¤‘ê°„ ìƒíƒœ ì—…ë°ì´íŠ¸
                        with shared_state_lock:
                            shared_state["processing_status"] = (
                                f"ğŸš€ **FAISS Index ì´ˆê¸°í™” ì¤‘**\n\n"
                                f"**ë‹¨ê³„**: Initial Vector Store ìƒì„±\n"
                                f"**ì²˜ë¦¬ ì¤‘**: {len(batch_docs)}ê°œ chunks â†’ embedding ë³€í™˜\n"
                                f"â³ ì²« ë²ˆì§¸ ë°°ì¹˜ëŠ” ì¸ë±ìŠ¤ ì´ˆê¸°í™”ë¡œ ì‹œê°„ì´ ë” ì†Œìš”ë©ë‹ˆë‹¤"
                            )

                        vectorstore = FAISS.from_documents(batch_docs, EMBED_MODEL)
                    else:
                        # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ë°°ì¹˜ ì¶”ê°€
                        print(
                            f"\râ• ë²¡í„° ìŠ¤í† ì–´ í™•ì¥: ë°°ì¹˜ {batch_idx + 1}/{num_batches} ({len(batch_docs)} chunks)",
                            end="",
                            flush=True,
                        )
                        batch_vectorstore = FAISS.from_documents(
                            batch_docs, EMBED_MODEL
                        )
                        vectorstore.merge_from(batch_vectorstore)
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        del batch_vectorstore

                    batch_time = time.time() - batch_start_time
                    chunks_per_second = (
                        len(batch_docs) / batch_time if batch_time > 0 else 0
                    )
                    print(
                        f"\râœ… ë°°ì¹˜ [{batch_idx + 1}/{num_batches}] ì™„ë£Œ: {len(batch_docs)} chunks in {batch_time:.1f}s ({chunks_per_second:.1f} chunks/s)"
                    )

                    # ë°°ì¹˜ ì™„ë£Œ í›„ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    completed_percent = ((batch_idx + 1) / num_batches) * 100
                    completed_chunks = (batch_idx + 1) * batch_size

                    if batch_idx < num_batches - 1:  # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹Œ ê²½ìš°
                        # ì‹œê°ì  ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
                        progress_bar = "â–ˆ" * int(completed_percent / 5) + "â–‘" * (
                            20 - int(completed_percent / 5)
                        )

                        with shared_state_lock:
                            shared_state["processing_status"] = (
                                f"âœ¨ **Vector Store í™•ì¥ ì¤‘**\n\n"
                                f"**ì™„ë£Œ ë°°ì¹˜**: {batch_idx + 1}/{num_batches} ({completed_percent:.1f}%)\n"
                                f"`{progress_bar}` {completed_percent:.1f}%\n\n"
                                f"âœ… **ì²˜ë¦¬ ì™„ë£Œ**: {min(completed_chunks, total_chunks):,} / {total_chunks:,} chunks\n"
                                f"ğŸš€ **ë²¡í„°í™” ì†ë„**: {chunks_per_second:.1f} chunks/sec"
                            )

                except Exception as e:
                    print(f"âŒ Error in batch {batch_idx + 1}/{num_batches}: {e}")
                    # ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„
                    if len(batch_docs) > 50:
                        print("ğŸ”„ Retrying with smaller batch size...")
                        smaller_batch_size = len(batch_docs) // 2
                        for sub_batch_start in range(
                            0, len(batch_docs), smaller_batch_size
                        ):
                            sub_batch_end = min(
                                sub_batch_start + smaller_batch_size, len(batch_docs)
                            )
                            sub_batch_docs = batch_docs[sub_batch_start:sub_batch_end]

                            try:
                                if vectorstore is None:
                                    vectorstore = FAISS.from_documents(
                                        sub_batch_docs, EMBED_MODEL
                                    )
                                else:
                                    sub_vectorstore = FAISS.from_documents(
                                        sub_batch_docs, EMBED_MODEL
                                    )
                                    vectorstore.merge_from(sub_vectorstore)
                                    del sub_vectorstore
                            except Exception as sub_e:
                                print(f"âŒ Sub-batch failed: {sub_e}")
                                continue
                    else:
                        print(
                            f"âŒ Skipping batch {batch_idx + 1} due to persistent errors"
                        )
                        continue

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del batch_docs

                # 5ê°œ ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬ ì•Œë¦¼ (ëª…ì‹œì  delë§Œ ì‚¬ìš©)
                if (batch_idx + 1) % 5 == 0:
                    print(
                        f"\rğŸ§¹ ë°°ì¹˜ [{batch_idx + 1}] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ",
                        end="",
                        flush=True,
                    )

            total_vectorization_time = time.time() - vectorization_start

            if vectorstore is None:
                print("âŒ Failed to create vector store - all batches failed")
                with shared_state_lock:
                    shared_state["processing_status"] = (
                        "ğŸ˜ **ì¤€ë¹„ ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”**\n\n"
                        "ë¬¸ì„œë“¤ì„ í•™ìŠµí•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
                        "ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆì–´ìš”."
                    )
                    shared_state["auto_processing_complete"] = True
                return

            print(
                f"âœ… Vector store creation complete! Total time: {total_vectorization_time:.1f}s"
            )
            print(
                f"ğŸ“Š Average speed: {total_chunks / total_vectorization_time:.1f} chunks/second"
            )

            # ìƒì„¸í•œ ì²˜ë¦¬ ê²°ê³¼ ìƒì„±
            success_rate = (
                (successfully_processed / len(pdf_files)) * 100 if pdf_files else 0
            )
            failed_count = len(failed_files)

            # ë²¡í„°í™” ì‹œê°„ í¬ë§·íŒ…
            vectorization_minutes = int(total_vectorization_time // 60)
            vectorization_seconds = int(total_vectorization_time % 60)

            if vectorization_minutes > 0:
                vectorization_time_text = (
                    f"{vectorization_minutes}ë¶„ {vectorization_seconds}ì´ˆ"
                )
            else:
                vectorization_time_text = f"{vectorization_seconds}ì´ˆ"

            result_summary = [
                "ğŸ‰ **ì™„ì „í•œ GIST ê·œì •ì§‘ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!**",
                f"ğŸ“š **ì²˜ë¦¬ëœ ë¬¸ì„œ**: {successfully_processed}/{len(pdf_files)}ê°œ ({success_rate:.1f}%)",
                f"ğŸ§  **ìƒì„±ëœ Chunks**: {len(all_docs):,}ê°œ (ì „ì²´ ê·œì •ì§‘ í¬í•¨)",
                f"â±ï¸ **Vector Store ìƒì„± ì‹œê°„**: {vectorization_time_text}",
                "ğŸ” **ëª¨ë“  GIST ê·œì •ì— ëŒ€í•´ 4ê°€ì§€ Rerank ë°©ì‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥**",
            ]

            if failed_count > 0:
                result_summary.append(
                    f"âš ï¸ **ì¼ë¶€ ë¬¸ì œ**: {failed_count}ê°œ íŒŒì¼ì„ ì½ì§€ ëª»í–ˆì–´ìš”"
                )
                if failed_count <= 5:  # ì‹¤íŒ¨í•œ íŒŒì¼ì´ 5ê°œ ì´í•˜ë©´ ëª¨ë‘ í‘œì‹œ
                    result_summary.append(
                        f"**ë¬¸ì œ íŒŒì¼**: {', '.join(failed_files[:5])}"
                    )
                else:  # ë§ìœ¼ë©´ ì¼ë¶€ë§Œ í‘œì‹œ
                    result_summary.append(
                        f"**ë¬¸ì œ íŒŒì¼** (ì¼ë¶€): {', '.join(failed_files[:3])}... ì™¸ {failed_count - 3}ê°œ"
                    )

            final_status = "\n".join(result_summary)

            with shared_state_lock:
                shared_state["vectorstore"] = vectorstore
                shared_state["pdfs"] = pdf_files

                # âœ¨ FAISS ì¸ë±ìŠ¤ ë¹„êµê¸° & ì„±ëŠ¥ ë¡œê±° ì´ˆê¸°í™”
                print("ğŸ”§ FAISS ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
                shared_state["performance_logger"] = PerformanceLogger()
                shared_state["faiss_comparator"] = FaissIndexComparator(
                    dimension=DIMENSION
                )

                # ë‹¤ì–‘í•œ FAISS ì¸ë±ìŠ¤ íƒ€ì… ìƒì„±
                try:
                    index_creation_results = shared_state[
                        "faiss_comparator"
                    ].create_indexes_from_vectorstore(vectorstore)
                    print(
                        f"âœ… ì„±ëŠ¥ ë¹„êµ ì¤€ë¹„ ì™„ë£Œ: {len(index_creation_results)}ê°œ ì¸ë±ìŠ¤ íƒ€ì…"
                    )
                except Exception as e:
                    print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    shared_state["faiss_comparator"] = None

                shared_state["processing_status"] = final_status
                shared_state["auto_processing_complete"] = True

            print("âœ… Auto-processing complete!")
            print(
                f"   Successfully processed: {successfully_processed}/{len(pdf_files)} files"
            )
            print(f"   Generated chunks: {len(all_docs)}")
            if failed_count > 0:
                print(f"   Failed files: {failed_count}")
                for failed_file in failed_files[:3]:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê·¸ì— ì¶œë ¥
                    print(f"     - {failed_file}")
                if failed_count > 3:
                    print(f"     ... and {failed_count - 3} more")
        else:
            with shared_state_lock:
                shared_state["processing_status"] = (
                    "ğŸ˜” ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ì–´ìš”. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”"
                )
                shared_state["auto_processing_complete"] = True

    except Exception as e:
        print(f"âŒ Auto-processing failed: {e}")
        with shared_state_lock:
            shared_state["processing_status"] = f"ğŸ˜ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}"
            shared_state["auto_processing_complete"] = True


def get_document_category(file_path: str) -> str:
    """íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
    path_lower = file_path.lower()

    if "í•™ì¹™" in path_lower or "ê·œì •" in path_lower:
        return "í•™ì¹™Â·ê·œì •"
    elif "ì§€ì¹¨" in path_lower:
        return "ì§€ì¹¨Â·ê¸°ì¤€"
    elif "ìš´ì˜" in path_lower:
        return "ìš´ì˜Â·ê´€ë¦¬"
    elif "ì—°êµ¬" in path_lower:
        return "ì—°êµ¬Â·í•™ìˆ "
    elif "í•™ìƒ" in path_lower:
        return "í•™ìƒÂ·êµìœ¡"
    else:
        return "ê¸°íƒ€"


def get_processing_status():
    """í˜„ì¬ ì²˜ë¦¬ ìƒíƒœë¥¼ ë°˜í™˜"""
    with shared_state_lock:
        if shared_state["auto_processing_complete"]:
            return shared_state["processing_status"]
        else:
            processed = shared_state["processed_count"]
            total = shared_state["total_count"]
            status = shared_state["processing_status"]
            if total > 0:
                progress_percent = processed / total * 100
                progress_bar = "â–ˆ" * int(progress_percent / 5) + "â–‘" * (
                    20 - int(progress_percent / 5)
                )
                progress_text = f"{processed}/{total} ({progress_percent:.1f}%)"
                return f"{status}\n\nğŸ“ˆ **ì§„í–‰ë¥ **: {progress_text}\n`{progress_bar}`"
            return status


def get_processing_status_with_complete_check():
    """ì²˜ë¦¬ ìƒíƒœì™€ ì™„ë£Œ ì—¬ë¶€ë¥¼ í•¨ê»˜ ë°˜í™˜"""
    with shared_state_lock:
        is_complete = shared_state["auto_processing_complete"]
        status = get_processing_status()
        return status, is_complete


# --------- (A) SESSION SETUP ---------
def get_session_id(request: gr.Request):
    raw_id = request.client.host + str(request.headers.get("user-agent"))
    return hashlib.sha256(raw_id.encode()).hexdigest()


def init_session(session_id: str, rerank_method="ì—†ìŒ"):
    with shared_state_lock:
        current_model = str(shared_state["current_model"])

    if len(sessions) >= MAX_SESSIONS:
        evicted_id, _ = sessions.popitem(last=False)
        print(f"ğŸ§¹ Removed LRU session: {evicted_id[:8]}...")

    model_info = MODELS[current_model]
    sessions[session_id] = {
        "history": {
            mode: [{"role": "system", "content": system_prompt}]
            for mode in RERANK_OPTIONS.keys()
        },
        "model_id": model_info["model_id"],
        "client": create_client(model_info["provider"]),
        "rerank_method": rerank_method,
    }


# --------- (B) DOCUMENT TEXT EXTRACTION ---------
def extract_text_from_pdf(pdf):
    doc = fitz.open(pdf)
    return "\n".join([page.get_text("text") for page in doc])


def create_vectorstore_from_pdfs(pdfs):
    all_docs = []
    for pdf in pdfs:
        text = extract_text_from_pdf(pdf)
        doc = Document(
            page_content=text,
            metadata={
                "source": pdf,
                "filename": os.path.basename(pdf) if hasattr(pdf, "name") else str(pdf),
                "category": "ì—…ë¡œë“œëœ ë¬¸ì„œ",
            },
        )
        docs = TEXT_SPLITTER.split_documents([doc])
        all_docs.extend(docs)
    return FAISS.from_documents(all_docs, EMBED_MODEL)


def create_retriever(vectorstore, rerank_method="ì—†ìŒ"):
    """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ retriever ìƒì„± - rerank ì˜µì…˜ ì§€ì›"""
    if not vectorstore:
        return None

    rerank_config = RERANK_OPTIONS.get(rerank_method, RERANK_OPTIONS["ì—†ìŒ"])

    if not rerank_config["enabled"]:
        # ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©
        return vectorstore.as_retriever(
            search_kwargs={"k": 5}
        )  # GIST ê·œì •ì´ ë§ìœ¼ë¯€ë¡œ ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰

    try:
        # Rerank ê¸°ëŠ¥ í™œì„±í™”
        top_k = rerank_config.get("top_k", 3)
        if not isinstance(top_k, int) or top_k <= 0:
            top_k = 3
        initial_k = max(15, top_k * 5)  # ë” ë§ì€ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼
        base_retriever = vectorstore.as_retriever(search_kwargs={"k": initial_k})

        # Cross-encoder ëª¨ë¸ ì„¤ì •
        cross_encoder = HuggingFaceCrossEncoder(model_name=rerank_config["model"])
        compressor = CrossEncoderReranker(model=cross_encoder)

        # ContextualCompressionRetrieverë¡œ ë˜í•‘
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor, base_retriever=base_retriever
        )

        # Top-K ì œí•œì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
        class TopKLimitedRetriever:
            def __init__(self, retriever, top_k):
                self.retriever = retriever
                self.top_k = top_k

            def invoke(self, query, **kwargs):
                docs = self.retriever.invoke(query, **kwargs)
                return docs[: self.top_k]

        limited_retriever = TopKLimitedRetriever(compression_retriever, top_k)
        return limited_retriever

    except Exception as e:
        print(f"âŒ Reranker setup failed for {rerank_method}: {e}")
        return vectorstore.as_retriever(search_kwargs={"k": 5})


def handle_additional_pdf_upload(pdfs, request: gr.Request):
    """ì¶”ê°€ PDF íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬"""
    if not pdfs:
        return get_processing_status()

    start_time = time.time()
    session_id = get_session_id(request)

    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )

        sessions.move_to_end(session_id)

    # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì™€ ë³‘í•©
    with shared_state_lock:
        existing_vectorstore = shared_state["vectorstore"]
        existing_pdfs = shared_state["pdfs"].copy()

    try:
        print("ğŸ“„ Processing additional PDF(s)...")
        new_vectorstore = create_vectorstore_from_pdfs(pdfs)

        if existing_vectorstore and new_vectorstore:
            # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì™€ ë³‘í•©
            print("ğŸ”„ Merging with existing documents...")
            existing_vectorstore.merge_from(new_vectorstore)
            merged_vectorstore = existing_vectorstore
            all_pdfs = existing_pdfs + list(pdfs)
        else:
            merged_vectorstore = new_vectorstore or existing_vectorstore
            all_pdfs = list(pdfs) if not existing_vectorstore else existing_pdfs

        with shared_state_lock:
            shared_state["vectorstore"] = merged_vectorstore
            shared_state["pdfs"] = all_pdfs

        end_time = time.time()
        elapsed_time = end_time - start_time
        print(f"âœ… Added {len(pdfs)} PDFs in {elapsed_time:.2f} seconds")

        return f"âœ… {len(pdfs)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ! ({elapsed_time:.2f}ì´ˆ) | ì´ {len(all_pdfs)}ê°œ ë¬¸ì„œ"

    except Exception as e:
        print(f"âŒ Additional PDF upload failed: {e}")
        return f"ğŸ˜ íŒŒì¼ ì¶”ê°€ì— ì‹¤íŒ¨í–ˆì–´ìš”: {str(e)}"


# --------- (C) PRIMARY CHAT FUNCTION ---------
def handle_query_for_rerank(
    user_query: str, rerank_method: str, request: gr.Request
) -> Generator:
    """íŠ¹ì • rerank ë°©ë²•ìœ¼ë¡œ ì¿¼ë¦¬ ì²˜ë¦¬"""
    session_id = get_session_id(request)

    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        session = sessions[session_id]
        sessions.move_to_end(session_id)

    # ì„±ëŠ¥ ì¶”ì  ì‹œì‘
    metrics = performance_trackers[rerank_method]
    metrics.start_query()

    # íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    history = session["history"][rerank_method]
    messages = history.copy()
    client = session["client"]

    # í˜„ì¬ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    with shared_state_lock:
        current_model = str(shared_state["current_model"])
        vectorstore = shared_state["vectorstore"]
        faiss_comparator = shared_state.get("faiss_comparator")
        performance_logger = shared_state.get("performance_logger")

    model_info = MODELS[current_model]

    # Extract relevant text data from PDFs
    context = ""
    faiss_performance_results = {}

    if vectorstore:
        print(f"ğŸ” [{rerank_method}] Retrieving relevant GIST rules and regulations...")
        retrieval_start = time.time()

        retriever = create_retriever(vectorstore, rerank_method)
        if retriever:
            docs = retriever.invoke(user_query)
            # ë¬¸ì„œ ì†ŒìŠ¤ ì •ë³´ í¬í•¨
            context_parts: List[str] = []
            for doc in docs:
                source_info = doc.metadata.get(
                    "filename", doc.metadata.get("source", "")
                )
                category = doc.metadata.get("category", "")
                context_parts.append(f"[{category}] {source_info}:\n{doc.page_content}")
            context = "\n\n".join(context_parts)

        retrieval_end = time.time()
        metrics.retrieval_time = retrieval_end - retrieval_start
        print(
            f"ğŸ“Š [{rerank_method}] Retrieved {len(docs) if 'docs' in locals() else 0} documents in {metrics.retrieval_time:.2f}s"
        )

        # ğŸš€ FAISS ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
        if faiss_comparator and performance_logger:
            print(f"ğŸ”¬ [{rerank_method}] FAISS ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¹„êµ ì‹œì‘...")
            try:
                # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
                query_vector = EMBED_MODEL.embed_query(user_query)
                query_vector_np = np.array(query_vector, dtype=np.float32)

                # ëª¨ë“  ì¸ë±ìŠ¤ì—ì„œ ì„±ëŠ¥ ë¹„êµ
                faiss_performance_results = faiss_comparator.compare_search_performance(
                    query_vector_np, k=3
                )

                # ì„±ëŠ¥ ê²°ê³¼ ë¡œê¹…
                performance_logger.log_query_performance(
                    user_query,
                    {
                        "rerank_method": rerank_method,
                        "faiss_performance": faiss_performance_results,
                        "retrieval_time": metrics.retrieval_time,
                    },
                )

                # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
                print(f"ğŸ“Š [{rerank_method}] FAISS ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
                for index_name, result in faiss_performance_results.items():
                    if result.get("success"):
                        print(f"   â€¢ {index_name}: {result['search_time_ms']:.2f}ms")
                    else:
                        print(
                            f"   â€¢ {index_name}: ì‹¤íŒ¨ - {result.get('error', 'Unknown error')}"
                        )

            except Exception as e:
                print(f"âš ï¸ [{rerank_method}] FAISS ì„±ëŠ¥ ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
                faiss_performance_results = {"error": str(e)}

    messages.append(
        {
            "role": "user",
            "content": f"Context (GIST Rules & Regulations):\n{context}\n\nQuestion: {user_query}",
        }
    )

    # Add user message to history first
    history.append({"role": "user", "content": user_query})

    # Create initial assistant message placeholder
    history.append({"role": "assistant", "content": ""})

    # Yield initial state with user query
    yield history, format_metrics(metrics.get_metrics(), rerank_method)

    # Invoke client with user query using streaming
    print(f"ğŸ’¬ [{rerank_method}] Inquiring LLM with streaming...")

    try:
        if model_info["provider"] == "openai":
            # OpenAI ìŠ¤íŠ¸ë¦¬ë°
            completion = client.chat.completions.create(
                model=model_info["model_id"], messages=messages, stream=True
            )

            bot_response = ""
            for chunk in completion:
                if chunk.choices[0].delta.content:
                    if not metrics.first_token_time:
                        metrics.first_token_received()

                    chunk_content = chunk.choices[0].delta.content
                    bot_response += chunk_content
                    metrics.add_token(chunk_content)

                    # Update the last message (assistant's response)
                    history[-1]["content"] = html.escape(bot_response)
                    yield history, format_metrics(metrics.get_metrics(), rerank_method)

        else:
            # HuggingFace Inference Client ìŠ¤íŠ¸ë¦¬ë°
            completion = client.chat.completions.create(
                model=model_info["model_id"], messages=messages, stream=True
            )

            bot_response = ""
            for chunk in completion:
                content = None
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and chunk.choices[0].delta.content
                ):
                    content = chunk.choices[0].delta.content
                elif hasattr(chunk, "token"):
                    content = (
                        chunk.token.text
                        if hasattr(chunk.token, "text")
                        else str(chunk.token)
                    )

                if content:
                    if not metrics.first_token_time:
                        metrics.first_token_received()

                    bot_response += content
                    metrics.add_token(content)

                    # Update the last message (assistant's response)
                    history[-1]["content"] = html.escape(bot_response)
                    yield history, format_metrics(metrics.get_metrics(), rerank_method)

    except Exception as e:
        print(f"âŒ [{rerank_method}] Streaming error: {e}")
        print("Falling back to non-streaming mode...")

        # Fallback to non-streaming
        try:
            completion = client.chat.completions.create(
                model=model_info["model_id"],
                messages=messages,
            )
            bot_response = completion.choices[0].message.content
            metrics.add_token(bot_response)
            history[-1]["content"] = html.escape(bot_response)
            yield history, format_metrics(metrics.get_metrics(), rerank_method)
        except Exception as fallback_error:
            print(f"âŒ [{rerank_method}] Fallback error: {fallback_error}")
            error_message = f"[{rerank_method}] ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            history[-1]["content"] = error_message
            yield history, format_metrics(metrics.get_metrics(), rerank_method)
            return

    # ì™„ë£Œ ì‹œê°„ ê¸°ë¡
    metrics.finish_query()

    # Save final history
    save_history(history, session_id, rerank_method)

    final_metrics = metrics.get_metrics()
    print(
        f"âœ… [{rerank_method}] Query completed in {final_metrics.get('total_time', 0):.2f}s"
    )
    yield history, format_metrics(final_metrics, rerank_method)


def format_metrics(metrics: Dict[str, float], rerank_method: str) -> str:
    """ì„±ëŠ¥ ì§€í‘œë¥¼ í¬ë§·ëœ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if not metrics:
        return f"**{rerank_method}** - ì¸¡ì • ì¤‘..."

    # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
    try:
        lines = [f"**{rerank_method}**"]

        if "time_to_first_token" in metrics:
            lines.append(f"- **ì²« í† í°**: {metrics['time_to_first_token']:.2f}ì´ˆ")

        if "total_time" in metrics:
            lines.append(f"- **ì´ ì‹œê°„**: {metrics['total_time']:.2f}ì´ˆ")

        if "tokens_per_second" in metrics:
            lines.append(f"- **ì†ë„**: {metrics['tokens_per_second']:.1f} tokens/s")

        if "retrieval_time" in metrics:
            lines.append(f"- **ê²€ìƒ‰ ì‹œê°„**: {metrics['retrieval_time']:.2f}ì´ˆ")

        result = "\n".join(lines)
        # ë¬¸ìì—´ íƒ€ì… í™•ì¸ ë° ë³´ì¥
        return str(result) if result else f"**{rerank_method}** - ë°ì´í„° ì—†ìŒ"

    except Exception as e:
        print(f"âŒ format_metrics ì˜¤ë¥˜: {e}")
        return f"**{rerank_method}** - í˜•ì‹ ì˜¤ë¥˜"


# --------- (D) ADDITIONAL FUNCTIONS ---------
def create_client(provider):
    """ëª¨ë¸ ì •ë³´ì— ë”°ë¼ InferenceClient ê°ì²´ ìƒì„±"""
    if provider == "openai":
        return openai.Client(api_key=OPENAI_API_KEY)

    headers = {}
    if HF_ENTERPRISE:
        headers["X-HF-Bill-To"] = HF_ENTERPRISE

    return InferenceClient(
        provider=provider,
        api_key=HF_API_KEY,
        headers=headers if headers else None,
    )


def change_model(model_name, request: gr.Request):
    """ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ ëª¨ë¸ ë³€ê²½ - ëª¨ë“  ì„¸ì…˜ì— ë™ê¸°í™”"""
    session_id = get_session_id(request)

    # ê³µìœ  ìƒíƒœ ì—…ë°ì´íŠ¸
    with shared_state_lock:
        shared_state["current_model"] = model_name

    # í˜„ì¬ ì„¸ì…˜ ì—…ë°ì´íŠ¸
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        else:
            model_info = MODELS[model_name]
            sessions[session_id]["model_id"] = model_info["model_id"]
            sessions[session_id]["client"] = create_client(model_info["provider"])
            sessions.move_to_end(session_id)

    print(f"ğŸ”„ Model changed to: {model_name}")
    return f"âœ… {model_name} ì¤€ë¹„ì™„ë£Œ"


def save_history(history, session_id, rerank_method):
    """ëŒ€í™” ê¸°ë¡(history)ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    folder = "./chat_logs_gist_analyzer"
    os.makedirs(folder, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(folder, f"{timestamp}_{session_id}_{rerank_method}.json")

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def reset_session(request: gr.Request):
    """ëŒ€í™” ë° íŒŒì¼ ì—…ë¡œë“œ ë‚´ì—­ ì‚­ì œ"""
    session_id = get_session_id(request)

    with session_lock:
        init_session(session_id)
        sessions.move_to_end(session_id)
        print(f"â™»ï¸ Session {session_id[:8]}... reset.")

    # ì„±ëŠ¥ ì§€í‘œë„ ë¦¬ì…‹
    for tracker in performance_trackers.values():
        tracker.reset()

    # ëª¨ë“  ì±„íŒ…ì°½ì„ ë¹ˆ ìƒíƒœë¡œ ë¦¬ì…‹
    empty_histories = [[] for _ in RERANK_OPTIONS.keys()]
    empty_metrics = [format_metrics({}, method) for method in RERANK_OPTIONS.keys()]

    return "", *empty_histories, *empty_metrics


def copy_as_markdown(history, rerank_method):
    """ì±„íŒ… ë‚´ì—­ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not history:
        return "ë³µì‚¬í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

    markdown_content = [f"# GIST Rules Analyzer - {rerank_method} ê²€ìƒ‰ ê²°ê³¼\n"]

    for i, message in enumerate(history):
        if message["role"] == "system":
            continue

        role = "â“ ì§ˆë¬¸" if message["role"] == "user" else "ğŸ’¡ ë‹µë³€"
        content = (
            html.unescape(message["content"])
            if isinstance(message["content"], str)
            else str(message["content"])
        )

        markdown_content.append(f"## {role}\n")
        markdown_content.append(f"{content}\n")

    result = "\n".join(markdown_content)
    print(f"ğŸ“‹ Markdown content prepared for {rerank_method} ({len(result)} chars)")
    return result


def handle_multi_query(user_query, request: gr.Request):
    """ëª¨ë“  rerank ëª¨ë“œì—ì„œ ë™ì‹œì— ì¿¼ë¦¬ ì‹¤í–‰"""
    if not user_query.strip():
        return [[] for _ in RERANK_OPTIONS.keys()] + [
            format_metrics({}, method) for method in RERANK_OPTIONS.keys()
        ]

    print(f"ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {user_query[:50]}...")

    # ëª¨ë“  rerank ëª¨ë“œì— ëŒ€í•´ ì œë„ˆë ˆì´í„° ìƒì„±
    generators = {
        method: handle_query_for_rerank(user_query, method, request)
        for method in RERANK_OPTIONS.keys()
    }

    # í˜„ì¬ ìƒíƒœ ì¶”ì 
    current_states = {
        method: ([], format_metrics({}, method)) for method in RERANK_OPTIONS.keys()
    }
    active_generators = set(RERANK_OPTIONS.keys())

    while active_generators:
        updated_methods = set()

        for method in list(active_generators):
            try:
                history, metrics = next(generators[method])
                current_states[method] = (history, metrics)
                updated_methods.add(method)
            except StopIteration:
                active_generators.remove(method)
                print(f"âœ… {method} completed")

        if updated_methods or len(active_generators) == 0:
            # ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
            results = []
            for method in RERANK_OPTIONS.keys():
                history, metrics = current_states[method]
                results.extend([history, metrics])

            yield results

    print("âœ… ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ì™„ë£Œ!")


# --------- (E) Gradio UI ---------
css = """
div {
    flex-wrap: nowrap !important;
}
.responsive-height {
    height: 100vh !important;
    padding-bottom: 20px !important;
}
.fill-height {
    height: 100% !important;
    flex-wrap: nowrap !important;
}
.extend-height {
    min-height: 300px !important;
    flex: 1 !important;
    overflow: auto !important;
}
.metrics-box {
    background: linear-gradient(135deg, #1a237e 0%, #283593 100%) !important;
    border: 2px solid #3f51b5 !important;
    border-radius: 12px !important;
    padding: 16px !important;
    margin: 12px 0 !important;
    color: #ffffff !important;
    font-weight: 500 !important;
    box-shadow: 0 4px 12px rgba(63, 81, 181, 0.3) !important;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif !important;
}
.metrics-box h2 {
    color: #e3f2fd !important;
    font-size: 1.1em !important;
    margin-bottom: 8px !important;
    font-weight: 600 !important;
    text-shadow: 0 1px 2px rgba(0,0,0,0.3) !important;
}
.metrics-box p {
    color: #ffffff !important;
    margin: 4px 0 !important;
    font-size: 0.95em !important;
    line-height: 1.4 !important;
}
.metrics-box strong {
    color: #e8eaf6 !important;
    font-weight: 600 !important;
}
.metrics-box ul li {
    color: #ffffff !important;
    margin: 4px 0 !important;
}
.status-box {
    background: linear-gradient(135deg, #e8f5e8 0%, #f0f8f0 100%) !important;
    border: 2px solid #4caf50 !important;
    border-radius: 12px !important;
    padding: 16px !important;
    margin: 12px 0 !important;
    color: #2e7d32 !important;
    font-weight: 500 !important;
    box-shadow: 0 2px 8px rgba(76, 175, 80, 0.1) !important;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif !important;
}
.status-box h2, .status-box h3 {
    color: #1b5e20 !important;
    margin-bottom: 8px !important;
}
.status-box strong {
    color: #1b5e20 !important;
}
.status-box code {
    background: rgba(76, 175, 80, 0.1) !important;
    padding: 2px 6px !important;
    border-radius: 4px !important;
    font-family: 'Courier New', monospace !important;
    color: #2e7d32 !important;
}
.progress-container {
    background: #f5f5f5 !important;
    border-radius: 8px !important;
    padding: 8px !important;
    margin: 8px 0 !important;
}
.auto-refresh-controls {
    background: rgba(33, 150, 243, 0.05) !important;
    border-radius: 8px !important;
    padding: 8px !important;
    margin: 4px 0 !important;
}
footer {
    display: none !important;
}
"""

# ìë™ PDF ì²˜ë¦¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
auto_processing_thread = threading.Thread(target=auto_process_pdfs, daemon=True)
auto_processing_thread.start()

with gr.Blocks(title="GIST Rules Analyzer", css=css, fill_height=True) as demo:
    gr.Markdown(
        "<center><h1>ğŸ“š GIST Rules Analyzer</h1><p><strong>í•™ìˆ ìš© RAG ì‹œìŠ¤í…œ</strong> | ê´‘ì£¼ê³¼í•™ê¸°ìˆ ì› í•™ì¹™Â·ê·œì • ê²€ìƒ‰ ì—°êµ¬</p></center>"
    )

    # ì²˜ë¦¬ ìƒíƒœ í‘œì‹œ
    with gr.Row():
        processing_status = gr.Markdown(
            value=get_processing_status(), elem_classes=["status-box"]
        )

    # ìƒíƒœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
    def update_status():
        return get_processing_status()

    # ì²˜ë¦¬ ì™„ë£Œ í™•ì¸ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
    def check_and_update_status():
        status, is_complete = get_processing_status_with_complete_check()
        return (
            status,
            gr.update(visible=not is_complete),  # ìë™ ìƒˆë¡œê³ ì¹¨ ì²´í¬ë°•ìŠ¤
            gr.update(interactive=is_complete),  # ì „ì†¡ ë²„íŠ¼ë§Œ ì œì–´
        )

    # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ ì¶”ê°€
    with gr.Row(elem_classes=["auto-refresh-controls"]):
        refresh_btn = gr.Button("ğŸ”„ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸", size="sm")
        auto_refresh_checkbox = gr.Checkbox(
            label="ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (3ì´ˆ ê°„ê²©)", value=True, visible=True
        )

    # ìë™ ìƒˆë¡œê³ ì¹¨ì„ ìœ„í•œ ìˆ¨ê²¨ì§„ ë²„íŠ¼
    auto_refresh_trigger = gr.Button(visible=False)

    # ìƒˆë¡œê³ ì¹¨ ì´ë²¤íŠ¸ëŠ” ë‚˜ì¤‘ì— ì •ì˜

    # JavaScriptë¡œ ìë™ ìƒˆë¡œê³ ì¹¨ êµ¬í˜„
    demo.load(
        None,
        None,
        None,
        js="""
        function autoRefresh() {
            if (document.querySelector('input[aria-label="ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (3ì´ˆ ê°„ê²©)"]').checked) {
                setTimeout(() => {
                    document.querySelector('button[style*="display: none"]').click();
                    autoRefresh();
                }, 3000);
            } else {
                setTimeout(autoRefresh, 1000);
            }
        }
        setTimeout(autoRefresh, 1000);
        """,
    )

    with gr.Row():
        with gr.Column(scale=2):
            # ê³µí†µ ì»¨íŠ¸ë¡¤
            with gr.Row():
                model_dropdown = gr.Dropdown(
                    list(MODELS.keys()),
                    label="ğŸ§  LLM ëª¨ë¸ ì„ íƒ",
                    value="GPT-4",
                    scale=2,
                )
                model_status = gr.Textbox(
                    label="ëª¨ë¸ ìƒíƒœ",
                    value="âœ… GPT-4 ì¤€ë¹„ì™„ë£Œ",
                    interactive=False,
                    scale=1,
                )

            additional_pdf_upload = gr.Files(
                label="ğŸ“„ ì¶”ê°€ ë¬¸ì„œ ì—…ë¡œë“œ (Vector Store í™•ì¥)", file_types=[".pdf"]
            )

            pdf_status = gr.Textbox(
                label="ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ", value=get_processing_status(), interactive=False
            )

            user_input = gr.Textbox(
                label="ğŸ” ì§ˆì˜ë¬¸ ì…ë ¥ (Query Input)",
                placeholder="ì˜ˆ: êµìˆ˜ë‹˜ì´ ë°•ì‚¬ê³¼ì • í•™ìƒì„ ì§€ë„í•  ìˆ˜ ìˆëŠ” ê¸°ê°„ì€ ì–¸ì œê¹Œì§€ì¸ê°€ìš”?",
                info="4ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ ë™ì‹œ í…ŒìŠ¤íŠ¸ë©ë‹ˆë‹¤",
                lines=3,
                interactive=True,  # í•­ìƒ í™œì„±í™” - ë¯¸ë¦¬ ì…ë ¥ ê°€ëŠ¥
            )

            with gr.Row():
                submit_btn = gr.Button(
                    "ğŸš€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", variant="primary", scale=2, interactive=False
                )
                reset_btn = gr.Button("ğŸ”„ ì´ˆê¸°í™”", variant="secondary", scale=1)

        with gr.Column(scale=3):
            # Rerank ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ğŸ” ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ (Baseline)")
                    chatbot_none = gr.Chatbot(
                        label="Vector Search Only", type="messages", height=350
                    )
                    metrics_none = gr.Markdown(
                        value=format_metrics({}, "ì—†ìŒ"), elem_classes=["metrics-box"]
                    )
                    copy_btn_none = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")
                    copy_output_none = gr.Textbox(
                        label="Markdown í˜•íƒœ (Ctrl+A, Ctrl+C)", lines=2, visible=False
                    )

                with gr.Column():
                    gr.Markdown("### ğŸ¯ Cross-Encoder Rerank (Basic)")
                    chatbot_basic = gr.Chatbot(
                        label="ms-marco-MiniLM-L-6-v2", type="messages", height=350
                    )
                    metrics_basic = gr.Markdown(
                        value=format_metrics({}, "Cross-Encoder (ê¸°ë³¸)"),
                        elem_classes=["metrics-box"],
                    )
                    copy_btn_basic = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")
                    copy_output_basic = gr.Textbox(
                        label="Markdown í˜•íƒœ (Ctrl+A, Ctrl+C)", lines=2, visible=False
                    )

            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ğŸš€ Cross-Encoder Rerank (Advanced)")
                    chatbot_advanced = gr.Chatbot(
                        label="ms-marco-MiniLM-L-12-v2", type="messages", height=350
                    )
                    metrics_advanced = gr.Markdown(
                        value=format_metrics({}, "Cross-Encoder (ê³ ì„±ëŠ¥)"),
                        elem_classes=["metrics-box"],
                    )
                    copy_btn_advanced = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")
                    copy_output_advanced = gr.Textbox(
                        label="Markdown í˜•íƒœ (Ctrl+A, Ctrl+C)", lines=2, visible=False
                    )

                with gr.Column():
                    gr.Markdown("### ğŸŒ Multilingual Cross-Encoder")
                    chatbot_multilang = gr.Chatbot(
                        label="mmarco-mMiniLMv2-L12-H384-v1",
                        type="messages",
                        height=350,
                    )
                    metrics_multilang = gr.Markdown(
                        value=format_metrics({}, "ë‹¤êµ­ì–´ Cross-Encoder"),
                        elem_classes=["metrics-box"],
                    )
                    copy_btn_multilang = gr.Button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬", size="sm")
                    copy_output_multilang = gr.Textbox(
                        label="Markdown í˜•íƒœ (Ctrl+A, Ctrl+C)", lines=2, visible=False
                    )

    # ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì„¤ì •
    additional_pdf_upload.change(
        fn=handle_additional_pdf_upload,
        inputs=[additional_pdf_upload],
        outputs=[pdf_status],
    )

    model_dropdown.change(
        fn=change_model, inputs=[model_dropdown], outputs=[model_status]
    )

    # ìƒíƒœ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì¶”ê°€
    refresh_btn.click(
        fn=check_and_update_status,
        outputs=[processing_status, auto_refresh_checkbox, submit_btn],
    )
    auto_refresh_trigger.click(
        fn=check_and_update_status,
        outputs=[processing_status, auto_refresh_checkbox, submit_btn],
    )

    # ì´ˆê¸° ë¡œë“œ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
    demo.load(
        fn=check_and_update_status,
        outputs=[processing_status, auto_refresh_checkbox, submit_btn],
    )

    # ë©€í‹° ì¿¼ë¦¬ ì²˜ë¦¬
    submit_btn.click(
        fn=handle_multi_query,
        inputs=[user_input],
        outputs=[
            chatbot_none,
            metrics_none,
            chatbot_basic,
            metrics_basic,
            chatbot_advanced,
            metrics_advanced,
            chatbot_multilang,
            metrics_multilang,
        ],
    )

    user_input.submit(
        fn=handle_multi_query,
        inputs=[user_input],
        outputs=[
            chatbot_none,
            metrics_none,
            chatbot_basic,
            metrics_basic,
            chatbot_advanced,
            metrics_advanced,
            chatbot_multilang,
            metrics_multilang,
        ],
    )

    # ë¦¬ì…‹ ê¸°ëŠ¥
    reset_btn.click(
        fn=reset_session,
        inputs=[],
        outputs=[
            user_input,
            chatbot_none,
            chatbot_basic,
            chatbot_advanced,
            chatbot_multilang,
            metrics_none,
            metrics_basic,
            metrics_advanced,
            metrics_multilang,
        ],
    )

    # ë³µì‚¬ ë²„íŠ¼ ì´ë²¤íŠ¸
    copy_btn_none.click(
        fn=lambda hist: (copy_as_markdown(hist, "ì—†ìŒ"), gr.update(visible=True)),
        inputs=[chatbot_none],
        outputs=[copy_output_none, copy_output_none],
    )

    copy_btn_basic.click(
        fn=lambda hist: (
            copy_as_markdown(hist, "Cross-Encoder (ê¸°ë³¸)"),
            gr.update(visible=True),
        ),
        inputs=[chatbot_basic],
        outputs=[copy_output_basic, copy_output_basic],
    )

    copy_btn_advanced.click(
        fn=lambda hist: (
            copy_as_markdown(hist, "Cross-Encoder (ê³ ì„±ëŠ¥)"),
            gr.update(visible=True),
        ),
        inputs=[chatbot_advanced],
        outputs=[copy_output_advanced, copy_output_advanced],
    )

    copy_btn_multilang.click(
        fn=lambda hist: (
            copy_as_markdown(hist, "ë‹¤êµ­ì–´ Cross-Encoder"),
            gr.update(visible=True),
        ),
        inputs=[chatbot_multilang],
        outputs=[copy_output_multilang, copy_output_multilang],
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - CLI entry point"""
    demo.launch(share=True, favicon_path="")


def main_dev():
    """ê°œë°œìš© ì‹¤í–‰ í•¨ìˆ˜ - ë¡œì»¬ ì„œë²„ë§Œ"""
    demo.launch(share=False, server_name="localhost", server_port=7860)


def main_prod():
    """í”„ë¡œë•ì…˜ ì‹¤í–‰ í•¨ìˆ˜ - ì™¸ë¶€ ì ‘ê·¼ ê°€ëŠ¥"""
    import os

    port = int(os.getenv("PORT", 7860))
    demo.launch(share=False, server_name="0.0.0.0", server_port=port)


if __name__ == "__main__":
    print("ğŸš€ Starting GIST Rules Analyzer...")
    print("ğŸ“„ Auto-processing PDF files in background...")
    main_dev()

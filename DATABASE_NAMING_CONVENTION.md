# ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë„¤ì´ë° ì»¨ë²¤ì…˜

## ğŸ¯ ì„¤ê³„ ì›ì¹™

1. **ì™„ì „í•œ ì¶”ì ì„±**: íŒŒì¼ëª…ë§Œìœ¼ë¡œ ëª¨ë“  ì‹¤í—˜ ì¡°ê±´ ì‹ë³„ ê°€ëŠ¥
2. **ê³„ì¸µì  êµ¬ì¡°**: ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ì£¼ìš” ë¶„ë¥˜, íŒŒì¼ëª…ìœ¼ë¡œ ì„¸ë¶€ ì‚¬í•­
3. **ê°€ë…ì„±ê³¼ ê°„ê²°ì„±**: ê¸¸ì§€ë§Œ ì˜ë¯¸ìˆëŠ” êµ¬ì¡°
4. **ìë™í™” ì¹œí™”ì **: í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ íŒŒì‹± ë° ë¶„ì„ ìš©ì´

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
experiments/outputs/
â”œâ”€â”€ chunking_results/                    # ì²­í‚¹ ê²°ê³¼ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ token_gpt2-512-100_a1b2c3d4/
â”‚   â”œâ”€â”€ semantic_qwen3-0.7-1024-3_e5f6g7h8/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ embeddings/                         # ì„ë² ë”©ë³„ ë¶„ë¥˜
â”‚   â”œâ”€â”€ qwen3_8b/                       # ì„ë² ë”© ëª¨ë¸ëª…
â”‚   â”‚   â”œâ”€â”€ token_gpt2-512-100_a1b2c3d4/
â”‚   â”‚   â”œâ”€â”€ semantic_minilm-0.7-1024-3_e5f6g7h8/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ jina_v3/
â”‚   â”œâ”€â”€ bge_m3/
â”‚   â””â”€â”€ ...
â””â”€â”€ analysis/                          # ë¶„ì„ ê²°ê³¼
    â”œâ”€â”€ parameter_impact_analysis.json
    â”œâ”€â”€ optimal_combinations.csv
    â””â”€â”€ benchmark_results/
```

## ğŸ—ï¸ ë„¤ì´ë° íŒ¨í„´

### ê¸°ë³¸ êµ¬ì¡°
```
{CHUNKER_TYPE}_{KEY_PARAMS}_{PARAM_HASH}
```

### ì²­í‚¹ ê²°ê³¼ íŒŒì¼ëª…
```
{chunker_type}_{key_parameters}_{8char_hash}/
â”œâ”€â”€ chunks.pkl                    # ì²­í‚¹ëœ ë¬¸ì„œë“¤
â”œâ”€â”€ metadata.json                 # ì „ì²´ íŒŒë¼ë¯¸í„° ë©”íƒ€ë°ì´í„°
â””â”€â”€ stats.json                    # ì²­í‚¹ í†µê³„
```

### ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ëª…
```
{embedding_model}_{chunker_type}_{key_parameters}_{8char_hash}/
â”œâ”€â”€ index.faiss                   # FAISS ì¸ë±ìŠ¤
â”œâ”€â”€ index.pkl                     # FAISS ë©”íƒ€ë°ì´í„°  
â”œâ”€â”€ embedding_metadata.json       # ì„ë² ë”© ì„¤ì •
â””â”€â”€ experiment_info.json          # ì‹¤í—˜ ì „ì²´ ì •ë³´
```

## ğŸ“ ì²­ì»¤ë³„ í•µì‹¬ íŒŒë¼ë¯¸í„° ì •ì˜

### 1. Token Chunker
**íŒŒë¼ë¯¸í„°**: `tokenizer-chunksize-overlap`
```bash
# ì˜ˆì‹œë“¤
token_gpt2-512-100_a1b2c3d4/          # GPT2, 512 í† í°, 100 ì¤‘ì²©
token_tiktoken-1024-0_e5f6g7h8/        # Tiktoken, 1024 í† í°, ì¤‘ì²© ì—†ìŒ
token_char-256-50_d4c3b2a1/            # Character, 256 í† í°, 50 ì¤‘ì²©
```

### 2. Sentence Chunker  
**íŒŒë¼ë¯¸í„°**: `tokenizer-chunksize-minSent-approx`
```bash
# ì˜ˆì‹œë“¤
sentence_gpt2-1024-2-F_b2c3d4e5/       # GPT2, 1024, ìµœì†Œ2ë¬¸ì¥, ì •í™•ì²˜ë¦¬
sentence_char-512-1-T_f6g7h8i9/        # Char, 512, ìµœì†Œ1ë¬¸ì¥, ê·¼ì‚¬ì²˜ë¦¬
sentence_tiktoken-2048-3-F_a9b8c7d6/   # Tiktoken, 2048, ìµœì†Œ3ë¬¸ì¥, ì •í™•ì²˜ë¦¬
```

### 3. Late Chunker
**íŒŒë¼ë¯¸í„°**: `embeddingmodel-chunksize-minchar`
```bash  
# ì˜ˆì‹œë“¤
late_minilm-1024-50_c3d4e5f6/          # MiniLM, 1024 í¬ê¸°, ìµœì†Œ50ì
late_bge-2048-100_g7h8i9j0/            # BGE, 2048 í¬ê¸°, ìµœì†Œ100ì
late_e5-512-24_k1l2m3n4/               # E5, 512 í¬ê¸°, ìµœì†Œ24ì
```

### 4. Neural Chunker
**íŒŒë¼ë¯¸í„°**: `model-minchar-stride`
```bash
# ì˜ˆì‹œë“¤  
neural_bert-50-null_d4e5f6g7/          # BERT, ìµœì†Œ50ì, ìŠ¤íŠ¸ë¼ì´ë“œì—†ìŒ
neural_distilbert-24-64_h8i9j0k1/      # DistilBERT, ìµœì†Œ24ì, 64ìŠ¤íŠ¸ë¼ì´ë“œ
neural_deberta-100-128_l2m3n4o5/       # DeBERTa, ìµœì†Œ100ì, 128ìŠ¤íŠ¸ë¼ì´ë“œ
```

### 5. Recursive Chunker
**íŒŒë¼ë¯¸í„°**: `tokenizer-chunksize-minchar`
```bash
# ì˜ˆì‹œë“¤
recursive_char-1024-50_e5f6g7h8/       # Character, 1024, ìµœì†Œ50ì
recursive_gpt2-2048-100_i9j0k1l2/      # GPT2, 2048, ìµœì†Œ100ì  
recursive_tiktoken-512-24_m3n4o5p6/    # Tiktoken, 512, ìµœì†Œ24ì
```

### 6. Semantic Chunker
**íŒŒë¼ë¯¸í„°**: `embeddingmodel-threshold-window-chunksize`
```bash
# ì˜ˆì‹œë“¤
semantic_minilm-0.7-3-1024_f6g7h8i9/  # MiniLM, 0.7ì„ê³„ê°’, 3ìœˆë„ìš°, 1024í¬ê¸°
semantic_bge-0.8-5-2048_j0k1l2m3/      # BGE, 0.8ì„ê³„ê°’, 5ìœˆë„ìš°, 2048í¬ê¸°
semantic_e5-0.6-1-512_n4o5p6q7/        # E5, 0.6ì„ê³„ê°’, 1ìœˆë„ìš°, 512í¬ê¸°
```

## ğŸ” íŒŒë¼ë¯¸í„° í•´ì‹œ ìƒì„± ê·œì¹™

### í•´ì‹œ ìƒì„± ë¡œì§
```python
import hashlib
import json

def generate_param_hash(chunker_params: dict) -> str:
    """íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ë¥¼ 8ì í•´ì‹œë¡œ ë³€í™˜"""
    # 1. _target_ ì œê±°
    clean_params = {k: v for k, v in chunker_params.items() if k != "_target_"}
    
    # 2. ì •ë ¬ëœ JSONìœ¼ë¡œ ì§ë ¬í™”  
    param_string = json.dumps(clean_params, sort_keys=True, separators=(',', ':'))
    
    # 3. MD5 í•´ì‹œì˜ ì• 8ìë¦¬ ì‚¬ìš©
    return hashlib.md5(param_string.encode()).hexdigest()[:8]

# ì˜ˆì‹œ
params = {
    "tokenizer": "gpt2", 
    "chunk_size": 512, 
    "chunk_overlap": 100
}
hash_value = generate_param_hash(params)  # "a1b2c3d4"
```

### ì•½ì–´ ë§¤í•‘ í…Œì´ë¸”
```python
ABBREVIATION_MAP = {
    # í† í¬ë‚˜ì´ì €
    "character": "char",
    "cl100k_base": "tiktoken", 
    "gpt2": "gpt2",
    "p50k_base": "p50k",
    "r50k_base": "r50k",
    
    # ì„ë² ë”© ëª¨ë¸ (Late/Semanticìš©)
    "sentence-transformers/all-MiniLM-L6-v2": "minilm",
    "BAAI/bge-small-en-v1.5": "bge",
    "intfloat/multilingual-e5-small": "e5",
    "Qwen/Qwen3-Embedding-0.6B": "qwen3",
    "jinaai/jina-embeddings-v3": "jina",
    
    # ì‹ ê²½ë§ ëª¨ë¸ (Neuralìš©)  
    "bert-base-uncased": "bert",
    "distilbert-base-uncased": "distilbert",
    "microsoft/deberta-v3-base": "deberta",
    "roberta-base": "roberta",
    
    # Boolean ê°’
    True: "T",
    False: "F",
    None: "null"
}
```

## ğŸ“Š ì‹¤ì œ íŒŒì¼ëª… ì˜ˆì‹œ

### ì „ì²´ ì‹¤í—˜ ê²°ê³¼ êµ¬ì¡°
```
experiments/outputs/
â”œâ”€â”€ chunking_results/
â”‚   â”œâ”€â”€ token_gpt2-512-100_a1b2c3d4/
â”‚   â”‚   â”œâ”€â”€ chunks.pkl
â”‚   â”‚   â”œâ”€â”€ metadata.json
â”‚   â”‚   â””â”€â”€ stats.json
â”‚   â”œâ”€â”€ semantic_minilm-0.7-3-1024_f6g7h8i9/
â”‚   â”‚   â”œâ”€â”€ chunks.pkl  
â”‚   â”‚   â”œâ”€â”€ metadata.json
â”‚   â”‚   â””â”€â”€ stats.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ qwen3_8b/                              # ì„ë² ë”© ëª¨ë¸ë³„ ë¶„ë¥˜
â”‚   â”œâ”€â”€ token_gpt2-512-100_a1b2c3d4/       # ì²­í‚¹ ê²°ê³¼ì™€ ë§¤ì¹­
â”‚   â”‚   â”œâ”€â”€ index.faiss
â”‚   â”‚   â”œâ”€â”€ index.pkl
â”‚   â”‚   â””â”€â”€ embedding_metadata.json
â”‚   â”œâ”€â”€ semantic_minilm-0.7-3-1024_f6g7h8i9/
â”‚   â”‚   â”œâ”€â”€ index.faiss
â”‚   â”‚   â”œâ”€â”€ index.pkl  
â”‚   â”‚   â””â”€â”€ embedding_metadata.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ jina_v3/
â”‚   â”œâ”€â”€ token_gpt2-512-100_a1b2c3d4/
â”‚   â”œâ”€â”€ sentence_char-1024-2-F_b2c3d4e5/
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ analysis/
    â”œâ”€â”€ experiment_results.csv             # ì „ì²´ ê²°ê³¼ ë§¤íŠ¸ë¦­ìŠ¤
    â”œâ”€â”€ naming_convention_map.json          # íŒŒì¼ëª…â†”ì‹¤í—˜ì¡°ê±´ ë§¤í•‘  
    â””â”€â”€ performance_by_naming_pattern.json  # ë„¤ì´ë° íŒ¨í„´ë³„ ì„±ëŠ¥ ë¶„ì„
```

## ğŸ› ï¸ êµ¬í˜„ í´ë˜ìŠ¤

```python
class DatabaseNamingConvention:
    """ë°ì´í„°ë² ì´ìŠ¤ ë„¤ì´ë° ì»¨ë²¤ì…˜ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.abbreviations = ABBREVIATION_MAP
    
    def generate_chunker_identifier(self, chunker_type: str, 
                                   params: Dict[str, Any]) -> str:
        """ì²­ì»¤ íƒ€ì…ê³¼ íŒŒë¼ë¯¸í„°ë¡œ ì‹ë³„ì ìƒì„±"""
        
        if chunker_type == "token":
            return self._token_identifier(params)
        elif chunker_type == "sentence":
            return self._sentence_identifier(params)
        elif chunker_type == "late":
            return self._late_identifier(params)
        elif chunker_type == "neural":
            return self._neural_identifier(params)
        elif chunker_type == "recursive":
            return self._recursive_identifier(params)
        elif chunker_type == "semantic":
            return self._semantic_identifier(params)
            
    def _token_identifier(self, params: Dict[str, Any]) -> str:
        tokenizer = self._abbreviate(params.get("tokenizer", "char"))
        chunk_size = params.get("chunk_size", 512)
        overlap = params.get("chunk_overlap", 0)
        param_hash = self._generate_hash(params)
        
        return f"token_{tokenizer}-{chunk_size}-{overlap}_{param_hash}"
    
    def _semantic_identifier(self, params: Dict[str, Any]) -> str:
        model = self._abbreviate(params.get("embedding_model", "minilm"))
        threshold = params.get("threshold", 0.7)
        window = params.get("similarity_window", 3)
        chunk_size = params.get("chunk_size", 1024)
        param_hash = self._generate_hash(params)
        
        return f"semantic_{model}-{threshold}-{window}-{chunk_size}_{param_hash}"
    
    def generate_database_path(self, embedding_model: str, 
                              chunker_identifier: str) -> Path:
        """ìµœì¢… ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ìƒì„±"""
        embedding_abbrev = self._abbreviate(embedding_model)
        return Path(f"experiments/outputs/{embedding_abbrev}/{chunker_identifier}")
    
    def parse_identifier(self, identifier: str) -> Dict[str, Any]:
        """ì‹ë³„ìë¥¼ íŒŒì‹±í•˜ì—¬ íŒŒë¼ë¯¸í„° ë³µì›"""
        # ì—­ë°©í–¥ íŒŒì‹± ë¡œì§ êµ¬í˜„
        pass
```

## ğŸ“ˆ ë„¤ì´ë° ì»¨ë²¤ì…˜ ë¶„ì„ ë„êµ¬

### íŒ¨í„´ ë¶„ì„
```python
def analyze_naming_patterns(database_paths: List[Path]) -> Dict[str, Any]:
    """ë„¤ì´ë° íŒ¨í„´ë³„ ì„±ëŠ¥ ë¶„ì„"""
    
    pattern_analysis = {
        "by_chunker_type": defaultdict(list),
        "by_embedding_model": defaultdict(list),
        "by_key_parameters": defaultdict(list),
        "hash_collisions": [],
        "naming_efficiency": {}
    }
    
    for path in database_paths:
        # íŒŒì¼ëª… íŒŒì‹± í›„ ë¶„ì„
        pass
    
    return pattern_analysis
```

## ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­

### 1. 2ë‹¨ê³„ ê³„ì¸µ êµ¬ì¡°
- **Level 1**: ì„ë² ë”© ëª¨ë¸ë³„ ë¶„ë¥˜ (`qwen3_8b/`, `jina_v3/`)
- **Level 2**: ì²­ì»¤ ì‹ë³„ì (`token_gpt2-512-100_a1b2c3d4/`)

### 2. ë©”íƒ€ë°ì´í„° í’ë¶€í™”
- `experiment_info.json`: ì „ì²´ ì‹¤í—˜ ì¡°ê±´
- `naming_convention_map.json`: íŒŒì¼ëª…â†”íŒŒë¼ë¯¸í„° ë§¤í•‘

### 3. ìë™ ê²€ì¦ ì‹œìŠ¤í…œ
- ì¤‘ë³µ í•´ì‹œ ì²´í¬
- íŒŒì¼ëª… ìœ íš¨ì„± ê²€ì¦  
- ì—­ë°©í–¥ íŒŒì‹± í…ŒìŠ¤íŠ¸

ì´ ë„¤ì´ë° ì»¨ë²¤ì…˜ìœ¼ë¡œ **5,288ê°œ ì¡°í•©**ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ , íŒŒì¼ëª…ë§Œìœ¼ë¡œë„ ì‹¤í—˜ ì¡°ê±´ì„ ì™„ë²½íˆ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

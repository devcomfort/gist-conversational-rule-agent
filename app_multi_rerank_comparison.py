"""
LiberVance RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ - Enhanced Version

ì´ ëª¨ë“ˆì€ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì˜ í–¥ìƒëœ ë²„ì „ì…ë‹ˆë‹¤.
ê¸°ì¡´ ê¸°ëŠ¥ì— ì¶”ê°€ë¡œ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- ì„±ëŠ¥ ì§€í‘œ ëª¨ë‹ˆí„°ë§ (ì²« í† í° ì‹œê°„, ì™„ë£Œ ì‹œê°„, TPS)
- ëª¨ë“  rerank ëª¨ë“œ ë™ì‹œ ë¹„êµ í…ŒìŠ¤íŠ¸
- ëª¨ë¸ ì„ íƒ ë™ê¸°í™”
- Markdown í˜•íƒœ ì‘ë‹µ ë³µì‚¬ ê¸°ëŠ¥

ì£¼ìš” ê¸°ëŠ¥:
- PDF ë¬¸ì„œ ìë™ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
- FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ í†µí•œ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
- ë‹¤ì–‘í•œ LLM ëª¨ë¸ ì§€ì› (GPT-4, DeepSeek-R1, Gemma-3, LLaMA-3.3, QwQ-32B)
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- ë‹¤ì¤‘ rerank ëª¨ë“œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ì§€í‘œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ì„¸ì…˜ë³„ ë…ë¦½ì ì¸ ë¬¸ì„œ ê´€ë¦¬
- ëŒ€í™” ê¸°ë¡ ì €ì¥ ë° ê´€ë¦¬

ê¸°ìˆ  ìŠ¤íƒ:
- ì„ë² ë”©: sentence-transformers/all-MiniLM-L6-v2
- ë²¡í„° ìŠ¤í† ì–´: FAISS
- í…ìŠ¤íŠ¸ ë¶„í• : RecursiveCharacterTextSplitter
- PDF ì²˜ë¦¬: PyMuPDF (fitz)
"""

import gradio as gr
import os
import json
import time
import hashlib
import html
import threading
import fitz
import openai
from collections import OrderedDict
from datetime import datetime
from dotenv import load_dotenv
from huggingface_hub import InferenceClient
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from typing import Dict, Generator, Tuple, List, Optional

# Environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HF_API_KEY = os.getenv("HUGGING_FACE_API_KEY") or os.getenv("HF_TOKEN")
HF_ENTERPRISE = os.getenv("HUGGING_FACE_ENTERPRISE")

# Session variables
MAX_SESSIONS = 100
sessions = OrderedDict()
session_lock = threading.Lock()

# Global shared state
shared_state: Dict = {"current_model": "GPT-4", "vectorstore": None, "pdfs": []}
shared_state_lock = threading.Lock()

# Model setup
MODELS = {
    "GPT-4": {"model_id": "gpt-4", "provider": "openai"},
    "DeepSeek-R1": {"model_id": "deepseek-ai/DeepSeek-R1", "provider": "novita"},
    "Gemma-3-27B": {"model_id": "google/gemma-3-27b-it", "provider": "hf-inference"},
    "Llama-3.3-70B": {
        "model_id": "meta-llama/Llama-3.3-70B-Instruct",
        "provider": "hf-inference",
    },
    "QwQ-32B": {"model_id": "Qwen/QwQ-32B", "provider": "hf-inference"},
}

# Rerank ì„¤ì •
RERANK_OPTIONS = {
    "ì—†ìŒ": {"enabled": False, "model": None, "top_k": 3},
    "Cross-Encoder (ê¸°ë³¸)": {
        "enabled": True,
        "model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "top_k": 3,
    },
    "Cross-Encoder (ê³ ì„±ëŠ¥)": {
        "enabled": True,
        "model": "cross-encoder/ms-marco-MiniLM-L-12-v2",
        "top_k": 3,
    },
    "ë‹¤êµ­ì–´ Cross-Encoder": {
        "enabled": True,
        "model": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
        "top_k": 3,
    },
}

EMBED_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
DIMENSION = 384
TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

system_prompt = "You are a helpful assistant that can answer questions based on the context when provided."


class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œë¥¼ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        self.start_time: Optional[float] = None
        self.first_token_time: Optional[float] = None
        self.end_time: Optional[float] = None
        self.token_count: int = 0
        self.retrieval_time: float = 0.0

    def reset(self):
        self.start_time = None
        self.first_token_time = None
        self.end_time = None
        self.token_count = 0
        self.retrieval_time = 0.0

    def start_query(self):
        self.reset()
        self.start_time = time.time()

    def first_token_received(self):
        if self.first_token_time is None:
            self.first_token_time = time.time()

    def add_token(self, token_text: str = ""):
        self.token_count += 1

    def finish_query(self):
        self.end_time = time.time()

    def get_metrics(self) -> Dict[str, float]:
        if not self.start_time:
            return {}

        metrics: Dict[str, float] = {}
        current_time = self.end_time or time.time()

        # ì „ì²´ ì†Œìš” ì‹œê°„
        metrics["total_time"] = current_time - self.start_time

        # ì²« í† í°ê¹Œì§€ì˜ ì‹œê°„
        if self.first_token_time:
            metrics["time_to_first_token"] = self.first_token_time - self.start_time

        # Tokens per second
        if self.token_count > 0 and self.first_token_time and self.end_time:
            generation_time = self.end_time - self.first_token_time
            if generation_time > 0:
                metrics["tokens_per_second"] = self.token_count / generation_time

        # ê²€ìƒ‰ ì‹œê°„
        metrics["retrieval_time"] = self.retrieval_time

        return metrics


# ê° rerank ëª¨ë“œë³„ ì„±ëŠ¥ ì§€í‘œ ì¶”ì 
performance_trackers = {mode: PerformanceMetrics() for mode in RERANK_OPTIONS.keys()}


# --------- (A) SESSION SETUP ---------
def get_session_id(request: gr.Request):
    raw_id = request.client.host + str(request.headers.get("user-agent"))
    return hashlib.sha256(raw_id.encode()).hexdigest()


def init_session(session_id: str, rerank_method="ì—†ìŒ"):
    with shared_state_lock:
        current_model = str(shared_state["current_model"])

    if len(sessions) >= MAX_SESSIONS:
        evicted_id, _ = sessions.popitem(last=False)
        print(f"ğŸ§¹ Removed LRU session: {evicted_id[:8]}...")

    model_info = MODELS[current_model]
    sessions[session_id] = {
        "history": {
            mode: [{"role": "system", "content": system_prompt}]
            for mode in RERANK_OPTIONS.keys()
        },
        "model_id": model_info["model_id"],
        "client": create_client(model_info["provider"]),
        "rerank_method": rerank_method,
    }


# --------- (B) DOCUMENT TEXT EXTRACTION ---------
def extract_text_from_pdf(pdf):
    doc = fitz.open(pdf)
    return "\n".join([page.get_text("text") for page in doc])


def create_vectorstore_from_pdfs(pdfs):
    all_docs = []
    for pdf in pdfs:
        text = extract_text_from_pdf(pdf)
        doc = Document(page_content=text, metadata={"source": pdf})
        docs = TEXT_SPLITTER.split_documents([doc])
        all_docs.extend(docs)
    return FAISS.from_documents(all_docs, EMBED_MODEL)


def create_retriever(vectorstore, rerank_method="ì—†ìŒ"):
    """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ retriever ìƒì„± - rerank ì˜µì…˜ ì§€ì›"""
    if not vectorstore:
        return None

    rerank_config = RERANK_OPTIONS.get(rerank_method, RERANK_OPTIONS["ì—†ìŒ"])

    if not rerank_config["enabled"]:
        # ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©
        return vectorstore.as_retriever(search_kwargs={"k": 3})

    try:
        # Rerank ê¸°ëŠ¥ í™œì„±í™”
        top_k = rerank_config.get("top_k", 3)
        if not isinstance(top_k, int) or top_k <= 0:
            top_k = 3
        initial_k = max(10, top_k * 3)  # ìµœì¢… ê²°ê³¼ì˜ 3ë°° ì •ë„ ê²€ìƒ‰
        base_retriever = vectorstore.as_retriever(search_kwargs={"k": initial_k})

        # Cross-encoder ëª¨ë¸ ì„¤ì •
        cross_encoder = HuggingFaceCrossEncoder(model_name=rerank_config["model"])
        compressor = CrossEncoderReranker(model=cross_encoder)

        # ContextualCompressionRetrieverë¡œ ë˜í•‘
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor, base_retriever=base_retriever
        )

        # Top-K ì œí•œì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
        class TopKLimitedRetriever:
            def __init__(self, retriever, top_k):
                self.retriever = retriever
                self.top_k = top_k

            def invoke(self, query, **kwargs):
                docs = self.retriever.invoke(query, **kwargs)
                return docs[: self.top_k]

        limited_retriever = TopKLimitedRetriever(compression_retriever, top_k)
        return limited_retriever

    except Exception as e:
        print(f"âŒ Reranker setup failed for {rerank_method}: {e}")
        return vectorstore.as_retriever(search_kwargs={"k": 3})


def handle_pdf_upload(pdfs, request: gr.Request):
    start_time = time.time()
    session_id = get_session_id(request)

    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
            print(
                f"âœ… New session created: {session_id[:8]}... | Total sessions: {len(sessions)}"
            )

        sessions.move_to_end(session_id)

    # Update shared vectorstore
    with shared_state_lock:
        print("Processing PDF(s)...")
        vectorstore = create_vectorstore_from_pdfs(pdfs) if pdfs else None
        shared_state["vectorstore"] = vectorstore
        shared_state["pdfs"] = pdfs

    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Processed {len(pdfs)} PDFs in {elapsed_time:.2f} seconds")

    return f"âœ… {len(pdfs)}ê°œ PDF íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)"


# --------- (C) PRIMARY CHAT FUNCTION ---------
def handle_query_for_rerank(
    user_query: str, rerank_method: str, request: gr.Request
) -> Generator:
    """íŠ¹ì • rerank ë°©ë²•ìœ¼ë¡œ ì¿¼ë¦¬ ì²˜ë¦¬"""
    session_id = get_session_id(request)

    # Ensure session-safe access
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        session = sessions[session_id]
        sessions.move_to_end(session_id)

    # ì„±ëŠ¥ ì¶”ì  ì‹œì‘
    metrics = performance_trackers[rerank_method]
    metrics.start_query()

    # íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    history = session["history"][rerank_method]
    messages = history.copy()
    client = session["client"]

    # í˜„ì¬ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    with shared_state_lock:
        current_model = str(shared_state["current_model"])
        vectorstore = shared_state["vectorstore"]

    model_info = MODELS[current_model]

    # Extract relevant text data from PDFs
    context = ""
    if vectorstore:
        print(f"ğŸ” [{rerank_method}] Retrieving relevant data...")
        retrieval_start = time.time()

        retriever = create_retriever(vectorstore, rerank_method)
        if retriever:
            docs = retriever.invoke(user_query)
            context = "\n".join([doc.page_content for doc in docs])

        retrieval_end = time.time()
        metrics.retrieval_time = retrieval_end - retrieval_start
        print(
            f"ğŸ“Š [{rerank_method}] Retrieved {len(docs) if 'docs' in locals() else 0} documents in {metrics.retrieval_time:.2f}s"
        )

    messages.append(
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_query}"}
    )

    # Add user message to history first
    history.append({"role": "user", "content": user_query})

    # Create initial assistant message placeholder
    history.append({"role": "assistant", "content": ""})

    # Yield initial state with user query
    yield history, format_metrics(metrics.get_metrics(), rerank_method)

    # Invoke client with user query using streaming
    print(f"ğŸ’¬ [{rerank_method}] Inquiring LLM with streaming...")

    try:
        if model_info["provider"] == "openai":
            # OpenAI ìŠ¤íŠ¸ë¦¬ë°
            completion = client.chat.completions.create(
                model=model_info["model_id"], messages=messages, stream=True
            )

            bot_response = ""
            for chunk in completion:
                if chunk.choices[0].delta.content:
                    if not metrics.first_token_time:
                        metrics.first_token_received()

                    chunk_content = chunk.choices[0].delta.content
                    bot_response += chunk_content
                    metrics.add_token(chunk_content)

                    # Update the last message (assistant's response)
                    history[-1]["content"] = html.escape(bot_response)
                    yield history, format_metrics(metrics.get_metrics(), rerank_method)

        else:
            # HuggingFace Inference Client ìŠ¤íŠ¸ë¦¬ë°
            completion = client.chat.completions.create(
                model=model_info["model_id"], messages=messages, stream=True
            )

            bot_response = ""
            for chunk in completion:
                content = None
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and chunk.choices[0].delta.content
                ):
                    content = chunk.choices[0].delta.content
                elif hasattr(chunk, "token"):
                    content = (
                        chunk.token.text
                        if hasattr(chunk.token, "text")
                        else str(chunk.token)
                    )

                if content:
                    if not metrics.first_token_time:
                        metrics.first_token_received()

                    bot_response += content
                    metrics.add_token(content)

                    # Update the last message (assistant's response)
                    history[-1]["content"] = html.escape(bot_response)
                    yield history, format_metrics(metrics.get_metrics(), rerank_method)

    except Exception as e:
        print(f"âŒ [{rerank_method}] Streaming error: {e}")
        print("Falling back to non-streaming mode...")

        # Fallback to non-streaming
        try:
            completion = client.chat.completions.create(
                model=model_info["model_id"],
                messages=messages,
            )
            bot_response = completion.choices[0].message.content
            metrics.add_token(bot_response)
            history[-1]["content"] = html.escape(bot_response)
            yield history, format_metrics(metrics.get_metrics(), rerank_method)
        except Exception as fallback_error:
            print(f"âŒ [{rerank_method}] Fallback error: {fallback_error}")
            error_message = f"[{rerank_method}] ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            history[-1]["content"] = error_message
            yield history, format_metrics(metrics.get_metrics(), rerank_method)
            return

    # ì™„ë£Œ ì‹œê°„ ê¸°ë¡
    metrics.finish_query()

    # Save final history
    save_history(history, session_id, rerank_method)

    final_metrics = metrics.get_metrics()
    print(
        f"âœ… [{rerank_method}] Query completed in {final_metrics.get('total_time', 0):.2f}s"
    )
    yield history, format_metrics(final_metrics, rerank_method)


def format_metrics(metrics: Dict[str, float], rerank_method: str) -> str:
    """ì„±ëŠ¥ ì§€í‘œë¥¼ í¬ë§·ëœ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if not metrics:
        return f"**{rerank_method}** - ì¸¡ì • ì¤‘..."

    lines = [f"## ğŸ“Š {rerank_method} ì„±ëŠ¥ ì§€í‘œ"]

    if "time_to_first_token" in metrics:
        lines.append(f"â±ï¸ **ì²« í† í°ê¹Œì§€**: {metrics['time_to_first_token']:.2f}ì´ˆ")

    if "total_time" in metrics:
        lines.append(f"ğŸ **ì´ ì†Œìš” ì‹œê°„**: {metrics['total_time']:.2f}ì´ˆ")

    if "tokens_per_second" in metrics:
        lines.append(f"ğŸš€ **ì†ë„**: {metrics['tokens_per_second']:.1f} tokens/sec")

    if "retrieval_time" in metrics:
        lines.append(f"ğŸ” **ê²€ìƒ‰ ì‹œê°„**: {metrics['retrieval_time']:.2f}ì´ˆ")

    return "\n".join(lines)


# --------- (D) ADDITIONAL FUNCTIONS ---------
def create_client(provider):
    """ëª¨ë¸ ì •ë³´ì— ë”°ë¼ InferenceClient ê°ì²´ ìƒì„±"""
    if provider == "openai":
        return openai.Client(api_key=OPENAI_API_KEY)

    headers = {}
    if HF_ENTERPRISE:
        headers["X-HF-Bill-To"] = HF_ENTERPRISE

    return InferenceClient(
        provider=provider,
        api_key=HF_API_KEY,
        headers=headers if headers else None,
    )


def change_model(model_name, request: gr.Request):
    """ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ ëª¨ë¸ ë³€ê²½ - ëª¨ë“  ì„¸ì…˜ì— ë™ê¸°í™”"""
    session_id = get_session_id(request)

    # ê³µìœ  ìƒíƒœ ì—…ë°ì´íŠ¸
    with shared_state_lock:
        shared_state["current_model"] = model_name

    # í˜„ì¬ ì„¸ì…˜ ì—…ë°ì´íŠ¸
    with session_lock:
        if session_id not in sessions:
            init_session(session_id)
        else:
            model_info = MODELS[model_name]
            sessions[session_id]["model_id"] = model_info["model_id"]
            sessions[session_id]["client"] = create_client(model_info["provider"])
            sessions.move_to_end(session_id)

    print(f"ğŸ”„ Model changed to: {model_name}")
    return f"âœ… ëª¨ë¸ì´ {model_name}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤"


def save_history(history, session_id, rerank_method):
    """ëŒ€í™” ê¸°ë¡(history)ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    folder = "./chat_logs_lvrag_enhanced"
    os.makedirs(folder, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(folder, f"{timestamp}_{session_id}_{rerank_method}.json")

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def reset_session(request: gr.Request):
    """ëŒ€í™” ë° íŒŒì¼ ì—…ë¡œë“œ ë‚´ì—­ ì‚­ì œ"""
    session_id = get_session_id(request)

    with session_lock:
        init_session(session_id)
        sessions.move_to_end(session_id)
        print(f"â™»ï¸ Session {session_id[:8]}... reset.")

    # ì„±ëŠ¥ ì§€í‘œë„ ë¦¬ì…‹
    for tracker in performance_trackers.values():
        tracker.reset()

    # ëª¨ë“  ì±„íŒ…ì°½ì„ ë¹ˆ ìƒíƒœë¡œ ë¦¬ì…‹
    empty_histories = [[] for _ in RERANK_OPTIONS.keys()]
    empty_metrics = [format_metrics({}, method) for method in RERANK_OPTIONS.keys()]

    return "", *empty_histories, *empty_metrics


def copy_as_markdown(history, rerank_method):
    """ì±„íŒ… ë‚´ì—­ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not history:
        return "ë³µì‚¬í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

    markdown_content = [f"# {rerank_method} ì±„íŒ… ê¸°ë¡\n"]

    for i, message in enumerate(history):
        if message["role"] == "system":
            continue

        role = "ì‚¬ìš©ì" if message["role"] == "user" else "AI ì–´ì‹œìŠ¤í„´íŠ¸"
        content = (
            html.unescape(message["content"])
            if isinstance(message["content"], str)
            else str(message["content"])
        )

        markdown_content.append(f"## {role}\n")
        markdown_content.append(f"{content}\n")

    result = "\n".join(markdown_content)
    print(f"ğŸ“‹ Markdown content prepared for {rerank_method} ({len(result)} chars)")
    return result


# --------- (E) Multi-Chat Interface ---------
def create_rerank_interface():
    """ê° rerank ëª¨ë“œë³„ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    interfaces = {}

    for rerank_method in RERANK_OPTIONS.keys():
        with gr.Column():
            gr.Markdown(f"### ğŸ”„ {rerank_method}")

            chatbot = gr.Chatbot(
                label=f"{rerank_method} ì±„íŒ…", type="messages", height=400
            )

            metrics_display = gr.Markdown(
                value=format_metrics({}, rerank_method), label="ì„±ëŠ¥ ì§€í‘œ"
            )

            copy_btn = gr.Button(f"ğŸ“‹ {rerank_method} Markdown ë³µì‚¬", size="sm")
            copy_output = gr.Textbox(
                label="Markdown ë‚´ìš© (Ctrl+A, Ctrl+Cë¡œ ë³µì‚¬)", lines=3, visible=False
            )

            # ë³µì‚¬ ë²„íŠ¼ ì´ë²¤íŠ¸
            copy_btn.click(
                fn=lambda hist, method=rerank_method: (
                    copy_as_markdown(hist, method),
                    gr.update(visible=True),
                ),
                inputs=[chatbot],
                outputs=[copy_output, copy_output],
            )

            interfaces[rerank_method] = {
                "chatbot": chatbot,
                "metrics": metrics_display,
                "copy_btn": copy_btn,
                "copy_output": copy_output,
            }

    return interfaces


def handle_multi_query(user_query, request: gr.Request):
    """ëª¨ë“  rerank ëª¨ë“œì—ì„œ ë™ì‹œì— ì¿¼ë¦¬ ì‹¤í–‰"""
    if not user_query.strip():
        return [[] for _ in RERANK_OPTIONS.keys()] + [
            format_metrics({}, method) for method in RERANK_OPTIONS.keys()
        ]

    print(f"ğŸš€ Starting multi-query test with: {user_query[:50]}...")

    # ëª¨ë“  rerank ëª¨ë“œì— ëŒ€í•´ ì œë„ˆë ˆì´í„° ìƒì„±
    generators = {
        method: handle_query_for_rerank(user_query, method, request)
        for method in RERANK_OPTIONS.keys()
    }

    # í˜„ì¬ ìƒíƒœ ì¶”ì 
    current_states = {
        method: ([], format_metrics({}, method)) for method in RERANK_OPTIONS.keys()
    }
    active_generators = set(RERANK_OPTIONS.keys())

    while active_generators:
        updated_methods = set()

        for method in list(active_generators):
            try:
                history, metrics = next(generators[method])
                current_states[method] = (history, metrics)
                updated_methods.add(method)
            except StopIteration:
                active_generators.remove(method)
                print(f"âœ… {method} completed")

        if updated_methods or len(active_generators) == 0:
            # ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
            results = []
            for method in RERANK_OPTIONS.keys():
                history, metrics = current_states[method]
                results.extend([history, metrics])

            yield results

    print("ğŸ‰ All rerank modes completed!")


# --------- (F) Gradio UI ---------
css = """
div {
    flex-wrap: nowrap !important;
}
.responsive-height {
    height: 100vh !important;
    padding-bottom: 20px !important;
}
.fill-height {
    height: 100% !important;
    flex-wrap: nowrap !important;
}
.extend-height {
    min-height: 300px !important;
    flex: 1 !important;
    overflow: auto !important;
}
.metrics-box {
    background: linear-gradient(135deg, #e3f2fd 0%, #f8f9fa 100%) !important;
    border: 2px solid #2196f3 !important;
    border-radius: 12px !important;
    padding: 16px !important;
    margin: 12px 0 !important;
    color: #1565c0 !important;
    font-weight: 500 !important;
    box-shadow: 0 2px 8px rgba(33, 150, 243, 0.1) !important;
}
.metrics-box h2 {
    color: #0d47a1 !important;
    font-size: 1.1em !important;
    margin-bottom: 8px !important;
    font-weight: 600 !important;
}
.metrics-box p {
    color: #1565c0 !important;
    margin: 4px 0 !important;
    font-size: 0.95em !important;
    line-height: 1.4 !important;
}
footer {
    display: none !important;
}
"""

with gr.Blocks(title="LiberVance RAG Enhanced", css=css, fill_height=True) as demo:
    gr.Markdown(
        "<center><h1>ğŸ“„ LiberVance RAG Enhanced - Multi-Rerank Comparison</h1></center>"
    )

    with gr.Row():
        with gr.Column(scale=2):
            # ê³µí†µ ì»¨íŠ¸ë¡¤
            with gr.Row():
                model_dropdown = gr.Dropdown(
                    list(MODELS.keys()),
                    label="ğŸ¤– Model Selection (ëª¨ë“  ì±„íŒ…ì°½ ë™ê¸°í™”)",
                    value="GPT-4",
                    scale=2,
                )
                model_status = gr.Textbox(
                    label="ëª¨ë¸ ìƒíƒœ",
                    value="âœ… GPT-4 ì¤€ë¹„ë¨",
                    interactive=False,
                    scale=1,
                )

            pdf_upload = gr.Files(
                label="ğŸ“ Upload PDF files (ëª¨ë“  ì±„íŒ…ì°½ ê³µìœ )", file_types=[".pdf"]
            )

            pdf_status = gr.Textbox(
                label="íŒŒì¼ ìƒíƒœ", value="ğŸ“ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", interactive=False
            )

            user_input = gr.Textbox(
                label="ğŸ” Query (ëª¨ë“  Rerank ëª¨ë“œì—ì„œ ë™ì‹œ í…ŒìŠ¤íŠ¸)",
                placeholder="ì˜ˆ: ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
                lines=3,
            )

            with gr.Row():
                submit_btn = gr.Button("ğŸš€ Submit to All", variant="primary", scale=2)
                reset_btn = gr.Button("â™»ï¸ Reset All", variant="secondary", scale=1)

        with gr.Column(scale=3):
            # Rerank ëª¨ë“œë³„ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ğŸ”„ ì—†ìŒ")
                    chatbot_none = gr.Chatbot(label="ì—†ìŒ", type="messages", height=350)
                    metrics_none = gr.Markdown(
                        value=format_metrics({}, "ì—†ìŒ"), elem_classes=["metrics-box"]
                    )
                    copy_btn_none = gr.Button("ğŸ“‹ Markdown ë³µì‚¬", size="sm")
                    copy_output_none = gr.Textbox(
                        label="Markdown (Ctrl+A, Ctrl+C)", lines=2, visible=False
                    )

                with gr.Column():
                    gr.Markdown("### ğŸ”„ Cross-Encoder (ê¸°ë³¸)")
                    chatbot_basic = gr.Chatbot(
                        label="Cross-Encoder (ê¸°ë³¸)", type="messages", height=350
                    )
                    metrics_basic = gr.Markdown(
                        value=format_metrics({}, "Cross-Encoder (ê¸°ë³¸)"),
                        elem_classes=["metrics-box"],
                    )
                    copy_btn_basic = gr.Button("ğŸ“‹ Markdown ë³µì‚¬", size="sm")
                    copy_output_basic = gr.Textbox(
                        label="Markdown (Ctrl+A, Ctrl+C)", lines=2, visible=False
                    )

            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ğŸ”„ Cross-Encoder (ê³ ì„±ëŠ¥)")
                    chatbot_advanced = gr.Chatbot(
                        label="Cross-Encoder (ê³ ì„±ëŠ¥)", type="messages", height=350
                    )
                    metrics_advanced = gr.Markdown(
                        value=format_metrics({}, "Cross-Encoder (ê³ ì„±ëŠ¥)"),
                        elem_classes=["metrics-box"],
                    )
                    copy_btn_advanced = gr.Button("ğŸ“‹ Markdown ë³µì‚¬", size="sm")
                    copy_output_advanced = gr.Textbox(
                        label="Markdown (Ctrl+A, Ctrl+C)", lines=2, visible=False
                    )

                with gr.Column():
                    gr.Markdown("### ğŸ”„ ë‹¤êµ­ì–´ Cross-Encoder")
                    chatbot_multilang = gr.Chatbot(
                        label="ë‹¤êµ­ì–´ Cross-Encoder", type="messages", height=350
                    )
                    metrics_multilang = gr.Markdown(
                        value=format_metrics({}, "ë‹¤êµ­ì–´ Cross-Encoder"),
                        elem_classes=["metrics-box"],
                    )
                    copy_btn_multilang = gr.Button("ğŸ“‹ Markdown ë³µì‚¬", size="sm")
                    copy_output_multilang = gr.Textbox(
                        label="Markdown (Ctrl+A, Ctrl+C)", lines=2, visible=False
                    )

    # ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì„¤ì •
    pdf_upload.change(fn=handle_pdf_upload, inputs=[pdf_upload], outputs=[pdf_status])

    model_dropdown.change(
        fn=change_model, inputs=[model_dropdown], outputs=[model_status]
    )

    # ë©€í‹° ì¿¼ë¦¬ ì²˜ë¦¬
    submit_btn.click(
        fn=handle_multi_query,
        inputs=[user_input],
        outputs=[
            chatbot_none,
            metrics_none,
            chatbot_basic,
            metrics_basic,
            chatbot_advanced,
            metrics_advanced,
            chatbot_multilang,
            metrics_multilang,
        ],
    )

    user_input.submit(
        fn=handle_multi_query,
        inputs=[user_input],
        outputs=[
            chatbot_none,
            metrics_none,
            chatbot_basic,
            metrics_basic,
            chatbot_advanced,
            metrics_advanced,
            chatbot_multilang,
            metrics_multilang,
        ],
    )

    # ë¦¬ì…‹ ê¸°ëŠ¥
    reset_btn.click(
        fn=reset_session,
        inputs=[],
        outputs=[
            user_input,
            chatbot_none,
            chatbot_basic,
            chatbot_advanced,
            chatbot_multilang,
            metrics_none,
            metrics_basic,
            metrics_advanced,
            metrics_multilang,
        ],
    )

    # ë³µì‚¬ ë²„íŠ¼ ì´ë²¤íŠ¸
    copy_btn_none.click(
        fn=lambda hist: (copy_as_markdown(hist, "ì—†ìŒ"), gr.update(visible=True)),
        inputs=[chatbot_none],
        outputs=[copy_output_none, copy_output_none],
    )

    copy_btn_basic.click(
        fn=lambda hist: (
            copy_as_markdown(hist, "Cross-Encoder (ê¸°ë³¸)"),
            gr.update(visible=True),
        ),
        inputs=[chatbot_basic],
        outputs=[copy_output_basic, copy_output_basic],
    )

    copy_btn_advanced.click(
        fn=lambda hist: (
            copy_as_markdown(hist, "Cross-Encoder (ê³ ì„±ëŠ¥)"),
            gr.update(visible=True),
        ),
        inputs=[chatbot_advanced],
        outputs=[copy_output_advanced, copy_output_advanced],
    )

    copy_btn_multilang.click(
        fn=lambda hist: (
            copy_as_markdown(hist, "ë‹¤êµ­ì–´ Cross-Encoder"),
            gr.update(visible=True),
        ),
        inputs=[chatbot_multilang],
        outputs=[copy_output_multilang, copy_output_multilang],
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - CLI entry point"""
    demo.launch(share=True, favicon_path="")


def main_dev():
    """ê°œë°œìš© ì‹¤í–‰ í•¨ìˆ˜ - ë¡œì»¬ ì„œë²„ë§Œ"""
    demo.launch(share=False, server_name="localhost", server_port=7860)


def main_prod():
    """í”„ë¡œë•ì…˜ ì‹¤í–‰ í•¨ìˆ˜ - ì™¸ë¶€ ì ‘ê·¼ ê°€ëŠ¥"""
    import os

    port = int(os.getenv("PORT", 7860))
    demo.launch(share=False, server_name="0.0.0.0", server_port=port)


if __name__ == "__main__":
    main_dev()

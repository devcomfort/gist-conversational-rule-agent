#!/usr/bin/env python3
"""
ì‹¤í—˜ ì„¤ì • í†µí•© ê´€ë¦¬
==================

ëª¨ë“  í† í¬ë‚˜ì´ì €, ì²­ì»¤, ì„ë² ë”© ëª¨ë¸ ì„¤ì •ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.
ì‹¤í—˜ ì‹¤í–‰ê¸°ëŠ” ì´ ì„¤ì •ë“¤ì„ ìˆœíšŒí•˜ë©° ëª¨ë“  ì¡°í•©ì„ ì‹¤í—˜í•©ë‹ˆë‹¤.

êµ¬ì¡°:
- TOKENIZERS: í† í¬ë‚˜ì´ì € ì„¤ì • ë° íŒŒë¼ë¯¸í„°
- CHUNKERS: ì²­ì»¤ í´ë˜ìŠ¤ì™€ íŒŒë¼ë¯¸í„° ì¡°í•©
- EMBEDDING_MODELS: ì„ë² ë”© ëª¨ë¸ê³¼ ì„¤ì •
"""

from chonkie import (
    TokenChunker,
    SentenceChunker,
    LateChunker,
    NeuralChunker,
    RecursiveChunker,
    SemanticChunker,
)

# ===================================================================
# í† í¬ë‚˜ì´ì € ì„¤ì •
# ===================================================================

TOKENIZERS = {
    "character": {
        "name": "character",
        "description": "ë¬¸ì ê¸°ë°˜ í† í¬ë‚˜ì´ì € (ê¸°ë³¸)",
        "implementation": "character",  # ì‹¤ì œ êµ¬í˜„ì—ì„œ ì‚¬ìš©í•  í‚¤
    },
    "gpt2": {
        "name": "gpt2",
        "description": "GPT-2 BPE í† í¬ë‚˜ì´ì €",
        "implementation": "gpt2",
        "requires_tiktoken": True,
    },
    "tiktoken": {
        "name": "cl100k_base",
        "description": "GPT-4 Tiktoken í† í¬ë‚˜ì´ì € (ìµœì‹ )",
        "implementation": "cl100k_base",
        "requires_tiktoken": True,
    },
}

# ===================================================================
# ì„ë² ë”© ëª¨ë¸ ì„¤ì • (embedding_models.yaml â†’ ì½”ë“œ)
# ===================================================================

EMBEDDING_MODELS = {
    "qwen3_8b": {
        "model_name": "Qwen/Qwen3-Embedding-8B",
        "dimension": 1024,
        "mteb_rank": 2,
        "db_name": "faiss_qwen3_embedding_8b",
        "description": "Qwen3 Embedding 8B - MTEB 2ìœ„ (ìµœê³  ì„±ëŠ¥)",
        "model_kwargs": {
            "device": "auto",  # ìë™ CUDA/CPU ê°ì§€
            "trust_remote_code": True,
            "torch_dtype": "float16",
        },
        "encode_kwargs": {
            "normalize_embeddings": True,
            "batch_size": 16,
        },
    },
    "qwen3_0_6b": {
        "model_name": "Qwen/Qwen3-Embedding-0.6B",
        "dimension": 1024,
        "mteb_rank": 4,
        "db_name": "faiss_qwen3_embedding_0_6b",
        "description": "Qwen3 Embedding 0.6B - MTEB 4ìœ„ (íš¨ìœ¨ì  ê³ ì„±ëŠ¥)",
        "model_kwargs": {
            "device": "auto",
            "trust_remote_code": True,
            "torch_dtype": "float16",
        },
        "encode_kwargs": {"normalize_embeddings": True, "batch_size": 32},
    },
    "jina_v3": {
        "model_name": "jinaai/jina-embeddings-v3",
        "dimension": 1024,
        "mteb_rank": 22,
        "db_name": "faiss_jina_embeddings_v3",
        "description": "Jina Embeddings v3 - MTEB 22ìœ„ (ê· í˜•ì¡íŒ ì„±ëŠ¥)",
        "model_kwargs": {"device": "auto", "trust_remote_code": True},
        "encode_kwargs": {"normalize_embeddings": True, "batch_size": 32},
    },
    "bge_m3": {
        "model_name": "BAAI/bge-m3",
        "dimension": 1024,
        "mteb_rank": 23,
        "db_name": "faiss_bge_m3",
        "description": "BGE-M3 - MTEB 23ìœ„ (ë‹¤êµ­ì–´ ì§€ì›)",
        "model_kwargs": {"device": "auto"},
        "encode_kwargs": {"normalize_embeddings": True, "batch_size": 32},
    },
    "all_minilm_l6": {
        "model_name": "sentence-transformers/all-MiniLM-L6-v2",
        "dimension": 384,
        "mteb_rank": 117,
        "db_name": "faiss_all_minilm_l6_v2",
        "description": "All-MiniLM-L6-v2 - MTEB 117ìœ„ (ê²½ëŸ‰ ë² ì´ìŠ¤ë¼ì¸)",
        "model_kwargs": {"device": "auto"},
        "encode_kwargs": {
            "normalize_embeddings": True,
            "batch_size": 64,  # ê²½ëŸ‰ ëª¨ë¸ í° ë°°ì¹˜
        },
    },
    "multilingual_e5": {
        "model_name": "intfloat/multilingual-e5-small",
        "dimension": 384,
        "mteb_rank": 45,
        "db_name": "faiss_multilingual_e5_small",
        "description": "Multilingual E5 Small - MTEB 45ìœ„ (ë‹¤êµ­ì–´ íŠ¹í™”)",
        "model_kwargs": {"device": "auto"},
        "encode_kwargs": {"normalize_embeddings": True, "batch_size": 64},
    },
}

# ===================================================================
# ì²­ì»¤ ì„¤ì • (chunker_params.yaml â†’ ì½”ë“œ)
# ===================================================================

CHUNKERS = {
    "token": {
        "class": TokenChunker,
        "description": "í† í° ê¸°ë°˜ ì²­ì»¤ - ê³ ì • í† í° ìˆ˜ë¡œ ë¶„í•  (ë²•ì ë¬¸ì„œ/ê¸°ìˆ ë¬¸ì„œ ì í•©)",
        "parameters": {
            "base": {
                "chunk_overlap": 0,  # ê¸°ë³¸ ê²¹ì¹¨ ì—†ìŒ
            },
            "variations": {
                "tokenizer": ["character", "gpt2", "cl100k_base"],
                "chunk_size": [128, 256, 512, 1024, 2048],  # ë‹¨ë¬¸ì—ì„œ ì¥ë¬¸ê¹Œì§€
                "chunk_overlap": [0, 64, 128, 256],  # 2ì˜ ë°°ìˆ˜, ë¬¸ë§¥ ë³´ì¡´ìš©
            },
        },
    },
    "sentence": {
        "class": SentenceChunker,
        "description": "ë¬¸ì¥ ê¸°ë°˜ ì²­ì»¤ - ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  (ì¼ë°˜ ë¬¸ì„œ ì í•©)",
        "parameters": {
            "base": {
                "chunk_overlap": 0,
                "approximate": False,
                "delim": [". ", "! ", "? ", "\n"],
                "include_delim": "prev",
            },
            "variations": {
                "tokenizer_or_token_counter": ["character", "gpt2", "cl100k_base"],
                "chunk_size": [256, 512, 1024, 2048],  # ë‹¤ì–‘í•œ ë¬¸ì„œ ê¸¸ì´ ëŒ€ì‘
                "min_sentences_per_chunk": [1, 2, 4, 8],  # 2ì˜ ë°°ìˆ˜, ë‹¨ë¬¸~ì¥ë¬¸
                "min_characters_per_sentence": [32, 64, 128],  # 2ì˜ ë°°ìˆ˜, ì‹¤ìš©ì  ë²”ìœ„
            },
        },
    },
    "late": {
        "class": LateChunker,
        "description": "ì§€ì—° ì²­ì»¤ - ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ì  ë¶„í•  (ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´)",
        "parameters": {
            "base": {},
            "variations": {
                "embedding_model": "ALL_EMBEDDINGS",
                "chunk_size": [256, 512, 1024, 2048, 4096],  # ê´‘ë²”ìœ„ ì‹¤í—˜
                "min_characters_per_chunk": [64, 128, 256, 512],  # 2ì˜ ë°°ìˆ˜, ì‹¤ìš©ì„±
            },
        },
    },
    "neural": {
        "class": NeuralChunker,
        "description": "ì‹ ê²½ë§ ì²­ì»¤ - ë”¥ëŸ¬ë‹ ëª¨ë¸ ê¸°ë°˜ ì§€ëŠ¥ì  ë¶„í• ",
        "parameters": {
            "base": {
                "device_map": "auto",  # GPU/CPU ìë™ ì„ íƒ
            },
            "variations": {
                "model": [
                    "mirth/chonky_distilbert_base_uncased_1",
                    "bert-base-uncased",
                    "distilbert-base-uncased",
                    "bert-base-multilingual-cased",
                    "roberta-base",
                ],
                "min_characters_per_chunk": [64, 128, 256, 512],  # 2ì˜ ë°°ìˆ˜
                "stride": [None, 0.1, 0.2, 0.5],  # ê²¹ì¹¨ ë¹„ìœ¨ ë‹¤ì–‘í™”
            },
        },
    },
    "recursive": {
        "class": RecursiveChunker,
        "description": "ì¬ê·€ ì²­ì»¤ - ê³„ì¸µì  ë¶„í• ë¡œ êµ¬ì¡° ë³´ì¡´ (ê¸°ìˆ ë¬¸ì„œ ì í•©)",
        "parameters": {
            "base": {},
            "variations": {
                "tokenizer_or_token_counter": ["character", "gpt2", "cl100k_base"],
                "chunk_size": [256, 512, 1024, 2048],
                "min_characters_per_chunk": [64, 128, 256],  # 2ì˜ ë°°ìˆ˜
            },
        },
    },
    "semantic": {
        "class": SemanticChunker,
        "description": "ì˜ë¯¸ì  ì²­ì»¤ - ì˜ë¯¸ ìœ ì‚¬ì„± ê¸°ë°˜ ë™ì  ë¶„í•  (ì£¼ì œë³„ ë¶„í• )",
        "parameters": {
            "base": {
                "delim": [". ", "! ", "? ", "\n"],
                "include_delim": "prev",
                "skip_window": 0,  # ê³ ê¸‰ íŒŒë¼ë¯¸í„° ë³µì›
                "filter_window": 5,
                "filter_polyorder": 3,
                "filter_tolerance": 0.2,
            },
            "variations": {
                "embedding_model": "ALL_EMBEDDINGS",
                "threshold": [0.6, 0.7, 0.8, 0.9],  # 0.5 ì œê±° (ë„ˆë¬´ ê´€ëŒ€í•¨)
                "similarity_window": [2, 4, 8],  # 2ì˜ ë°°ìˆ˜, ë¬¸ë§¥ ì°½ í¬ê¸°
                "chunk_size": [256, 512, 1024, 2048, 4096],  # ê´‘ë²”ìœ„ ì‹¤í—˜
                "min_sentences_per_chunk": [1, 2, 4],  # 2ì˜ ë°°ìˆ˜ë¡œ í™•ì¥
                "min_characters_per_sentence": [32, 64, 128, 256],  # 2ì˜ ë°°ìˆ˜
            },
        },
    },
}

# ===================================================================
# ê¸°ë³¸ ì‹¤í—˜ ì„¤ì •
# ===================================================================

EXPERIMENT_CONFIG = {
    "data": {
        "rules_directory": "rules",
        "supported_extensions": [".pdf", ".hwp", ".docx", ".pptx"],
        "max_file_size_mb": 50,
        "exclude_patterns": ["**/.*", "**/__pycache__/**"],
    },
    "output": {
        "base_dir": "experiments/outputs",
        "tokenization_subdir": "tokenization_results",
        "chunking_subdir": "chunking_results",
        "embedding_subdir": "",  # ë£¨íŠ¸ì— ì„ë² ë”©ë³„ í´ë”
    },
    "execution": {
        "enable_tokenization_phase": True,  # í† í°í™” ë‹¨ê³„ í™œì„±í™” ì—¬ë¶€
        "save_intermediate_results": True,  # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì—¬ë¶€
        "cleanup_memory_after_each": True,  # ê° ì‹¤í—˜ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
    },
    "storage": {
        # ê¸°ë³¸ ì €ì¥ ì„¤ì •ë§Œ
    },
    "filters": {
        # ì‹¤í—˜ ë²”ìœ„ ì œí•œ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
        "tokenizers": None,  # None = ì „ì²´, ë˜ëŠ” ["character", "gpt2"]
        "chunkers": None,  # None = ì „ì²´, ë˜ëŠ” ["token", "sentence"]
        "embedding_models": None,  # None = ì „ì²´, ë˜ëŠ” ["all_minilm_l6"]
    },
    "performance": {
        # ì œí•œ ì—†ìŒ - ì‹œìŠ¤í…œ ìµœëŒ€ ì„±ëŠ¥ í™œìš©
    },
}

# ===================================================================
# ì¡°í•© ê³„ì‚° í•¨ìˆ˜ë“¤
# ===================================================================


def count_tokenizer_combinations() -> int:
    """í† í¬ë‚˜ì´ì € ì¡°í•© ìˆ˜ ê³„ì‚°"""
    return len(TOKENIZERS)


def count_chunker_combinations() -> int:
    """ì²­ì»¤ íŒŒë¼ë¯¸í„° ì¡°í•© ìˆ˜ ê³„ì‚°"""
    total = 0

    for chunker_name, config in CHUNKERS.items():
        variations = config["parameters"]["variations"]

        # ê° íŒŒë¼ë¯¸í„°ë³„ ì¡°í•© ìˆ˜ ê³„ì‚°
        combo_count = 1
        for param_name, param_values in variations.items():
            if param_name == "embedding_model" and param_values == "ALL_EMBEDDINGS":
                # ì„ë² ë”© ëª¨ë¸ ìˆ˜ë§Œí¼ ê³±í•¨
                combo_count *= len(EMBEDDING_MODELS)
            elif isinstance(param_values, list):
                combo_count *= len(param_values)

        total += combo_count

    return total


def count_embedding_combinations() -> int:
    """ì„ë² ë”© ëª¨ë¸ ìˆ˜"""
    return len(EMBEDDING_MODELS)


def calculate_total_experiments() -> dict:
    """ì „ì²´ ì‹¤í—˜ ì¡°í•© ìˆ˜ ê³„ì‚°"""
    tokenizer_count = count_tokenizer_combinations()
    chunker_count = count_chunker_combinations()
    embedding_count = count_embedding_combinations()

    # ì‹¤ì œë¡œëŠ” ì²­í‚¹ ê²°ê³¼ Ã— ì„ë² ë”© ëª¨ë¸ ì¡°í•©
    # (í† í¬ë‚˜ì´ì €ëŠ” ì„ íƒì  ë‹¨ê³„)
    total_experiments = chunker_count * embedding_count

    return {
        "tokenizers": tokenizer_count,
        "chunker_combinations": chunker_count,
        "embedding_models": embedding_count,
        "total_experiments": total_experiments,
    }


# ===================================================================
# ì„¤ì • ê²€ì¦ í•¨ìˆ˜ë“¤
# ===================================================================


def validate_configurations() -> dict:
    """ì„¤ì • ê²€ì¦ ë° ìš”ì•½"""
    validation_results = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "summary": {},
    }

    # í† í¬ë‚˜ì´ì € ê²€ì¦
    tokenizer_names = list(TOKENIZERS.keys())
    validation_results["summary"]["tokenizers"] = tokenizer_names

    # ì²­ì»¤ ê²€ì¦
    chunker_names = list(CHUNKERS.keys())
    validation_results["summary"]["chunkers"] = chunker_names

    # ì„ë² ë”© ëª¨ë¸ ê²€ì¦
    embedding_names = list(EMBEDDING_MODELS.keys())
    validation_results["summary"]["embedding_models"] = embedding_names

    # ì¡°í•© ìˆ˜ ê³„ì‚°
    experiment_stats = calculate_total_experiments()
    validation_results["summary"]["experiment_statistics"] = experiment_stats

    # ëŒ€í˜• ëª¨ë¸ ê²½ê³  (ë°°ì¹˜ í¬ê¸° ê¸°ë°˜)
    large_models = [
        name
        for name, config in EMBEDDING_MODELS.items()
        if config.get("encode_kwargs", {}).get("batch_size", 32) <= 16
    ]

    if large_models:
        validation_results["warnings"].append(
            f"ëŒ€í˜• ëª¨ë¸ ê°ì§€: {large_models} (ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ GPU ë©”ëª¨ë¦¬ ì ˆì•½ ì¤‘)"
        )

    # ì‹¤í—˜ ìˆ˜ ê²½ê³ 
    if experiment_stats["total_experiments"] > 5000:
        validation_results["warnings"].append(
            f"ëŒ€ê·œëª¨ ì‹¤í—˜: {experiment_stats['total_experiments']}ê°œ ì‹¤í—˜ ì˜ˆìƒ (ì¶©ë¶„í•œ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ í•„ìš”)"
        )

    return validation_results


# ===================================================================
# ì„¤ì • í‘œì‹œ í•¨ìˆ˜
# ===================================================================


def print_configuration_summary():
    """ì„¤ì • ìš”ì•½ ì¶œë ¥"""
    print("=" * 80)
    print("ğŸ”§ ì‹¤í—˜ ì„¤ì • ìš”ì•½")
    print("=" * 80)

    # í† í¬ë‚˜ì´ì €
    print(f"\nğŸ”¤ í† í¬ë‚˜ì´ì € ({len(TOKENIZERS)}ê°œ):")
    for name, config in TOKENIZERS.items():
        print(f"  â€¢ {name}: {config['description']}")

    # ì„ë² ë”© ëª¨ë¸
    print(f"\nğŸ¤– ì„ë² ë”© ëª¨ë¸ ({len(EMBEDDING_MODELS)}ê°œ):")
    for name, config in EMBEDDING_MODELS.items():
        print(
            f"  â€¢ {name}: {config['description']} (dim={config['dimension']}, rank={config['mteb_rank']}ìœ„)"
        )

    # ì²­ì»¤
    print(f"\nğŸ§© ì²­ì»¤ ({len(CHUNKERS)}ê°œ):")
    for name, config in CHUNKERS.items():
        variations = config["parameters"]["variations"]
        param_count = 1
        for param_values in variations.values():
            if isinstance(param_values, list):
                param_count *= len(param_values)
            elif param_values == "ALL_EMBEDDINGS":
                param_count *= len(EMBEDDING_MODELS)
        print(f"  â€¢ {name}: {config['description']} (~{param_count}ê°œ ì¡°í•©)")

    # ì‹¤í—˜ í†µê³„
    stats = calculate_total_experiments()
    print(f"\nğŸ“Š ì‹¤í—˜ í†µê³„:")
    print(f"  â€¢ ì²­í‚¹ ì¡°í•©: {stats['chunker_combinations']:,}ê°œ")
    print(f"  â€¢ ì„ë² ë”© ëª¨ë¸: {stats['embedding_models']}ê°œ")
    print(f"  â€¢ ì´ ì‹¤í—˜: {stats['total_experiments']:,}ê°œ")

    # ê²€ì¦ ê²°ê³¼
    validation = validate_configurations()
    if validation["warnings"]:
        print(f"\nâš ï¸ ì£¼ì˜ì‚¬í•­:")
        for warning in validation["warnings"]:
            print(f"  â€¢ {warning}")

    print("=" * 80)


# ===================================================================
# í•™ìˆ  ì—°êµ¬ìš© ì¶”ê°€ í•¨ìˆ˜ë“¤
# ===================================================================


def generate_experiment_metadata():
    """ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ìƒì„±"""
    import datetime
    import platform

    try:
        import torch

        torch_available = True
    except ImportError:
        torch_available = False

    metadata = {
        "experiment_info": {
            "timestamp": datetime.datetime.now().isoformat(),
            "total_experiments": calculate_total_experiments()["total_experiments"],
            "mode": "unrestricted",
        },
        "system_info": {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "torch_version": torch.__version__ if torch_available else "N/A",
            "cuda_available": torch.cuda.is_available() if torch_available else False,
        },
    }

    return metadata


def estimate_storage_requirements():
    """ìŠ¤í† ë¦¬ì§€ ìš”êµ¬ëŸ‰ ê³„ì‚°"""
    stats = calculate_total_experiments()

    # ê¸°ë³¸ ì¶”ì •ì¹˜ (ì œì•½ ì—†ìŒ)
    base_size_mb = 17

    total_size_mb = base_size_mb * stats["total_experiments"]
    total_size_gb = total_size_mb / 1024

    return {
        "total_experiments": stats["total_experiments"],
        "size_per_experiment_mb": base_size_mb,
        "total_size_gb": round(total_size_gb, 2),
    }


def print_experiment_summary():
    """ì‹¤í—˜ ì„¤ì • ìš”ì•½"""
    print("\n" + "=" * 80)
    print("ğŸ”§ ì‹¤í—˜ ì„¤ì • ìš”ì•½ (ì œì•½ ì—†ìŒ)")
    print("=" * 80)

    # ê¸°ë³¸ êµ¬ì„± ìš”ì•½
    print_configuration_summary()

    print(f"\nâœ… ì‹¤í—˜ ì›ì¹™:")
    print(f"  â€¢ ëª¨ë“  ì‹¤í—˜ì´ ì œì•½ ì—†ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤")
    print(f"  â€¢ ì‹œìŠ¤í…œ ìµœëŒ€ ì„±ëŠ¥ì„ í™œìš©í•©ë‹ˆë‹¤")
    print(f"  â€¢ ëª¨ë“  ë°ì´í„°ê°€ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³´ì¡´ë©ë‹ˆë‹¤")


if __name__ == "__main__":
    # ì‹¤í—˜ ì„¤ì • ìš”ì•½ ì¶œë ¥
    print_experiment_summary()

    # ê²€ì¦ ì‹¤í–‰
    validation = validate_configurations()

    if validation["valid"]:
        print("âœ… ëª¨ë“  ì„¤ì •ì´ ìœ íš¨í•©ë‹ˆë‹¤!")
    else:
        print("âŒ ì„¤ì • ì˜¤ë¥˜:")
        for error in validation["errors"]:
            print(f"  â€¢ {error}")
